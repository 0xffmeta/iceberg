{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Apache Iceberg is an open table format for huge analytic datasets. Iceberg adds tables to compute engines including Spark, Trino, PrestoDB, Flink and Hive using a high-performance table format that works just like a SQL table. User experience \u00b6 Iceberg avoids unpleasant surprises. Schema evolution works and won\u2019t inadvertently un-delete data. Users don\u2019t need to know about partitioning to get fast queries. Schema evolution supports add, drop, update, or rename, and has no side-effects Hidden partitioning prevents user mistakes that cause silently incorrect results or extremely slow queries Partition layout evolution can update the layout of a table as data volume or query patterns change Time travel enables reproducible queries that use exactly the same table snapshot, or lets users easily examine changes Version rollback allows users to quickly correct problems by resetting tables to a good state Reliability and performance \u00b6 Iceberg was built for huge tables. Iceberg is used in production where a single table can contain tens of petabytes of data and even these huge tables can be read without a distributed SQL engine. Scan planning is fast \u2013 a distributed SQL engine isn\u2019t needed to read a table or find files Advanced filtering \u2013 data files are pruned with partition and column-level stats, using table metadata Iceberg was designed to solve correctness problems in eventually-consistent cloud object stores. Works with any cloud store and reduces NN congestion when in HDFS, by avoiding listing and renames Serializable isolation \u2013 table changes are atomic and readers never see partial or uncommitted changes Multiple concurrent writers use optimistic concurrency and will retry to ensure that compatible updates succeed, even when writes conflict Open standard \u00b6 Iceberg has been designed and developed to be an open community standard with a specification to ensure compatibility across languages and implementations. Apache Iceberg is open source , and is developed at the Apache Software Foundation .","title":"Home"},{"location":"#user-experience","text":"Iceberg avoids unpleasant surprises. Schema evolution works and won\u2019t inadvertently un-delete data. Users don\u2019t need to know about partitioning to get fast queries. Schema evolution supports add, drop, update, or rename, and has no side-effects Hidden partitioning prevents user mistakes that cause silently incorrect results or extremely slow queries Partition layout evolution can update the layout of a table as data volume or query patterns change Time travel enables reproducible queries that use exactly the same table snapshot, or lets users easily examine changes Version rollback allows users to quickly correct problems by resetting tables to a good state","title":"User experience"},{"location":"#reliability-and-performance","text":"Iceberg was built for huge tables. Iceberg is used in production where a single table can contain tens of petabytes of data and even these huge tables can be read without a distributed SQL engine. Scan planning is fast \u2013 a distributed SQL engine isn\u2019t needed to read a table or find files Advanced filtering \u2013 data files are pruned with partition and column-level stats, using table metadata Iceberg was designed to solve correctness problems in eventually-consistent cloud object stores. Works with any cloud store and reduces NN congestion when in HDFS, by avoiding listing and renames Serializable isolation \u2013 table changes are atomic and readers never see partial or uncommitted changes Multiple concurrent writers use optimistic concurrency and will retry to ensure that compatible updates succeed, even when writes conflict","title":"Reliability and performance"},{"location":"#open-standard","text":"Iceberg has been designed and developed to be an open community standard with a specification to ensure compatibility across languages and implementations. Apache Iceberg is open source , and is developed at the Apache Software Foundation .","title":"Open standard"},{"location":"api/","text":"Iceberg Java API \u00b6 Tables \u00b6 The main purpose of the Iceberg API is to manage table metadata, like schema, partition spec, metadata, and data files that store table data. Table metadata and operations are accessed through the Table interface. This interface will return table information. Table metadata \u00b6 The Table interface provides access to the table metadata: schema returns the current table schema spec returns the current table partition spec properties returns a map of key-value properties currentSnapshot returns the current table snapshot snapshots returns all valid snapshots for the table snapshot(id) returns a specific snapshot by ID location returns the table\u2019s base location Tables also provide refresh to update the table to the latest version, and expose helpers: io returns the FileIO used to read and write table files locationProvider returns a LocationProvider used to create paths for data and metadata files Scanning \u00b6 File level \u00b6 Iceberg table scans start by creating a TableScan object with newScan . TableScan scan = table.newScan(); To configure a scan, call filter and select on the TableScan to get a new TableScan with those changes. TableScan filteredScan = scan.filter(Expressions.equal(\"id\", 5)) Calls to configuration methods create a new TableScan so that each TableScan is immutable and won\u2019t change unexpectedly if shared across threads. When a scan is configured, planFiles , planTasks , and schema are used to return files, tasks, and the read projection. TableScan scan = table.newScan() .filter(Expressions.equal(\"id\", 5)) .select(\"id\", \"data\"); Schema projection = scan.schema(); Iterable<CombinedScanTask> tasks = scan.planTasks(); Use asOfTime or useSnapshot to configure the table snapshot for time travel queries. Row level \u00b6 Iceberg table scans start by creating a ScanBuilder object with IcebergGenerics.read . ScanBuilder scanBuilder = IcebergGenerics.read(table) To configure a scan, call where and select on the ScanBuilder to get a new ScanBuilder with those changes. scanBuilder.where(Expressions.equal(\"id\", 5)) When a scan is configured, call method build to execute scan. build return CloseableIterable<Record> CloseableIterable<Record> result = IcebergGenerics.read(table) .where(Expressions.lessThan(\"id\", 5)) .build(); where Record is Iceberg record for iceberg-data module org.apache.iceberg.data.Record . Update operations \u00b6 Table also exposes operations that update the table. These operations use a builder pattern, PendingUpdate , that commits when PendingUpdate#commit is called. For example, updating the table schema is done by calling updateSchema , adding updates to the builder, and finally calling commit to commit the pending changes to the table: table.updateSchema() .addColumn(\"count\", Types.LongType.get()) .commit(); Available operations to update a table are: updateSchema \u2013 update the table schema updateProperties \u2013 update table properties updateLocation \u2013 update the table\u2019s base location newAppend \u2013 used to append data files newFastAppend \u2013 used to append data files, will not compact metadata newOverwrite \u2013 used to append data files and remove files that are overwritten newDelete \u2013 used to delete data files newRewrite \u2013 used to rewrite data files; will replace existing files with new versions newTransaction \u2013 create a new table-level transaction rewriteManifests \u2013 rewrite manifest data by clustering files, for faster scan planning rollback \u2013 rollback the table state to a specific snapshot Transactions \u00b6 Transactions are used to commit multiple table changes in a single atomic operation. A transaction is used to create individual operations using factory methods, like newAppend , just like working with a Table . Operations created by a transaction are committed as a group when commitTransaction is called. For example, deleting and appending a file in the same transaction: Transaction t = table.newTransaction(); // commit operations to the transaction t.newDelete().deleteFromRowFilter(filter).commit(); t.newAppend().appendFile(data).commit(); // commit all the changes to the table t.commitTransaction(); Types \u00b6 Iceberg data types are located in the org.apache.iceberg.types package . Primitives \u00b6 Primitive type instances are available from static methods in each type class. Types without parameters use get , and types like decimal use factory methods: Types.IntegerType.get() // int Types.DoubleType.get() // double Types.DecimalType.of(9, 2) // decimal(9, 2) Nested types \u00b6 Structs, maps, and lists are created using factory methods in type classes. Like struct fields, map keys or values and list elements are tracked as nested fields. Nested fields track field IDs and nullability. Struct fields are created using NestedField.optional or NestedField.required . Map value and list element nullability is set in the map and list factory methods. // struct<1 id: int, 2 data: optional string> StructType struct = Struct.of( Types.NestedField.required(1, \"id\", Types.IntegerType.get()), Types.NestedField.optional(2, \"data\", Types.StringType.get()) ) // map<1 key: int, 2 value: optional string> MapType map = MapType.ofOptional( 1, Types.IntegerType.get(), 2, Types.StringType.get() ) // array<1 element: int> ListType list = ListType.ofRequired(1, IntegerType.get()); Expressions \u00b6 Iceberg\u2019s expressions are used to configure table scans. To create expressions, use the factory methods in Expressions . Supported predicate expressions are: isNull notNull equal notEqual lessThan lessThanOrEqual greaterThan greaterThanOrEqual in notIn startsWith Supported expression operations are: and or not Constant expressions are: alwaysTrue alwaysFalse Expression binding \u00b6 When created, expressions are unbound. Before an expression is used, it will be bound to a data type to find the field ID the expression name represents, and to convert predicate literals. For example, before using the expression lessThan(\"x\", 10) , Iceberg needs to determine which column \"x\" refers to and convert 10 to that column\u2019s data type. If the expression could be bound to the type struct<1 x: long, 2 y: long> or to struct<11 x: int, 12 y: int> . Expression example \u00b6 table.newScan() .filter(Expressions.greaterThanOrEqual(\"x\", 5)) .filter(Expressions.lessThan(\"x\", 10)) Modules \u00b6 Iceberg table support is organized in library modules: iceberg-common contains utility classes used in other modules iceberg-api contains the public Iceberg API, including expressions, types, tables, and operations iceberg-arrow is an implementation of the Iceberg type system for reading and writing data stored in Iceberg tables using Apache Arrow as the in-memory data format iceberg-aws contains implementations of the Iceberg API to be used with tables stored on AWS S3 and/or for tables defined using the AWS Glue data catalog iceberg-core contains implementations of the Iceberg API and support for Avro data files, this is what processing engines should depend on iceberg-parquet is an optional module for working with tables backed by Parquet files iceberg-orc is an optional module for working with tables backed by ORC files ( experimental ) iceberg-hive-metastore is an implementation of Iceberg tables backed by the Hive metastore Thrift client This project Iceberg also has modules for adding Iceberg support to processing engines and associated tooling: iceberg-spark2 is an implementation of Spark\u2019s Datasource V2 API in 2.4 for Iceberg (use iceberg-spark-runtime for a shaded version) iceberg-spark3 is an implementation of Spark\u2019s Datasource V2 API in 3.0 for Iceberg (use iceberg-spark3-runtime for a shaded version) iceberg-flink is an implementation of Flink\u2019s Table and DataStream API for Iceberg (use iceberg-flink-runtime for a shaded version) iceberg-hive3 is an implementation of Hive 3 specific SerDe\u2019s for Timestamp, TimestampWithZone, and Date object inspectors (use iceberg-hive-runtime for a shaded version). iceberg-mr is an implementation of MapReduce and Hive InputFormats and SerDes for Iceberg (use iceberg-hive-runtime for a shaded version for use with Hive) iceberg-nessie is a module used to integrate Iceberg table metadata history and operations with Project Nessie iceberg-data is a client library used to read Iceberg tables from JVM applications iceberg-pig is an implementation of Pig\u2019s LoadFunc API for Iceberg iceberg-runtime generates a shaded runtime jar for Spark to integrate with iceberg tables","title":"Java API intro"},{"location":"api/#iceberg-java-api","text":"","title":"Iceberg Java API"},{"location":"api/#tables","text":"The main purpose of the Iceberg API is to manage table metadata, like schema, partition spec, metadata, and data files that store table data. Table metadata and operations are accessed through the Table interface. This interface will return table information.","title":"Tables"},{"location":"api/#table-metadata","text":"The Table interface provides access to the table metadata: schema returns the current table schema spec returns the current table partition spec properties returns a map of key-value properties currentSnapshot returns the current table snapshot snapshots returns all valid snapshots for the table snapshot(id) returns a specific snapshot by ID location returns the table\u2019s base location Tables also provide refresh to update the table to the latest version, and expose helpers: io returns the FileIO used to read and write table files locationProvider returns a LocationProvider used to create paths for data and metadata files","title":"Table metadata"},{"location":"api/#scanning","text":"","title":"Scanning"},{"location":"api/#file-level","text":"Iceberg table scans start by creating a TableScan object with newScan . TableScan scan = table.newScan(); To configure a scan, call filter and select on the TableScan to get a new TableScan with those changes. TableScan filteredScan = scan.filter(Expressions.equal(\"id\", 5)) Calls to configuration methods create a new TableScan so that each TableScan is immutable and won\u2019t change unexpectedly if shared across threads. When a scan is configured, planFiles , planTasks , and schema are used to return files, tasks, and the read projection. TableScan scan = table.newScan() .filter(Expressions.equal(\"id\", 5)) .select(\"id\", \"data\"); Schema projection = scan.schema(); Iterable<CombinedScanTask> tasks = scan.planTasks(); Use asOfTime or useSnapshot to configure the table snapshot for time travel queries.","title":"File level"},{"location":"api/#row-level","text":"Iceberg table scans start by creating a ScanBuilder object with IcebergGenerics.read . ScanBuilder scanBuilder = IcebergGenerics.read(table) To configure a scan, call where and select on the ScanBuilder to get a new ScanBuilder with those changes. scanBuilder.where(Expressions.equal(\"id\", 5)) When a scan is configured, call method build to execute scan. build return CloseableIterable<Record> CloseableIterable<Record> result = IcebergGenerics.read(table) .where(Expressions.lessThan(\"id\", 5)) .build(); where Record is Iceberg record for iceberg-data module org.apache.iceberg.data.Record .","title":"Row level"},{"location":"api/#update-operations","text":"Table also exposes operations that update the table. These operations use a builder pattern, PendingUpdate , that commits when PendingUpdate#commit is called. For example, updating the table schema is done by calling updateSchema , adding updates to the builder, and finally calling commit to commit the pending changes to the table: table.updateSchema() .addColumn(\"count\", Types.LongType.get()) .commit(); Available operations to update a table are: updateSchema \u2013 update the table schema updateProperties \u2013 update table properties updateLocation \u2013 update the table\u2019s base location newAppend \u2013 used to append data files newFastAppend \u2013 used to append data files, will not compact metadata newOverwrite \u2013 used to append data files and remove files that are overwritten newDelete \u2013 used to delete data files newRewrite \u2013 used to rewrite data files; will replace existing files with new versions newTransaction \u2013 create a new table-level transaction rewriteManifests \u2013 rewrite manifest data by clustering files, for faster scan planning rollback \u2013 rollback the table state to a specific snapshot","title":"Update operations"},{"location":"api/#transactions","text":"Transactions are used to commit multiple table changes in a single atomic operation. A transaction is used to create individual operations using factory methods, like newAppend , just like working with a Table . Operations created by a transaction are committed as a group when commitTransaction is called. For example, deleting and appending a file in the same transaction: Transaction t = table.newTransaction(); // commit operations to the transaction t.newDelete().deleteFromRowFilter(filter).commit(); t.newAppend().appendFile(data).commit(); // commit all the changes to the table t.commitTransaction();","title":"Transactions"},{"location":"api/#types","text":"Iceberg data types are located in the org.apache.iceberg.types package .","title":"Types"},{"location":"api/#primitives","text":"Primitive type instances are available from static methods in each type class. Types without parameters use get , and types like decimal use factory methods: Types.IntegerType.get() // int Types.DoubleType.get() // double Types.DecimalType.of(9, 2) // decimal(9, 2)","title":"Primitives"},{"location":"api/#nested-types","text":"Structs, maps, and lists are created using factory methods in type classes. Like struct fields, map keys or values and list elements are tracked as nested fields. Nested fields track field IDs and nullability. Struct fields are created using NestedField.optional or NestedField.required . Map value and list element nullability is set in the map and list factory methods. // struct<1 id: int, 2 data: optional string> StructType struct = Struct.of( Types.NestedField.required(1, \"id\", Types.IntegerType.get()), Types.NestedField.optional(2, \"data\", Types.StringType.get()) ) // map<1 key: int, 2 value: optional string> MapType map = MapType.ofOptional( 1, Types.IntegerType.get(), 2, Types.StringType.get() ) // array<1 element: int> ListType list = ListType.ofRequired(1, IntegerType.get());","title":"Nested types"},{"location":"api/#expressions","text":"Iceberg\u2019s expressions are used to configure table scans. To create expressions, use the factory methods in Expressions . Supported predicate expressions are: isNull notNull equal notEqual lessThan lessThanOrEqual greaterThan greaterThanOrEqual in notIn startsWith Supported expression operations are: and or not Constant expressions are: alwaysTrue alwaysFalse","title":"Expressions"},{"location":"api/#expression-binding","text":"When created, expressions are unbound. Before an expression is used, it will be bound to a data type to find the field ID the expression name represents, and to convert predicate literals. For example, before using the expression lessThan(\"x\", 10) , Iceberg needs to determine which column \"x\" refers to and convert 10 to that column\u2019s data type. If the expression could be bound to the type struct<1 x: long, 2 y: long> or to struct<11 x: int, 12 y: int> .","title":"Expression binding"},{"location":"api/#expression-example","text":"table.newScan() .filter(Expressions.greaterThanOrEqual(\"x\", 5)) .filter(Expressions.lessThan(\"x\", 10))","title":"Expression example"},{"location":"api/#modules","text":"Iceberg table support is organized in library modules: iceberg-common contains utility classes used in other modules iceberg-api contains the public Iceberg API, including expressions, types, tables, and operations iceberg-arrow is an implementation of the Iceberg type system for reading and writing data stored in Iceberg tables using Apache Arrow as the in-memory data format iceberg-aws contains implementations of the Iceberg API to be used with tables stored on AWS S3 and/or for tables defined using the AWS Glue data catalog iceberg-core contains implementations of the Iceberg API and support for Avro data files, this is what processing engines should depend on iceberg-parquet is an optional module for working with tables backed by Parquet files iceberg-orc is an optional module for working with tables backed by ORC files ( experimental ) iceberg-hive-metastore is an implementation of Iceberg tables backed by the Hive metastore Thrift client This project Iceberg also has modules for adding Iceberg support to processing engines and associated tooling: iceberg-spark2 is an implementation of Spark\u2019s Datasource V2 API in 2.4 for Iceberg (use iceberg-spark-runtime for a shaded version) iceberg-spark3 is an implementation of Spark\u2019s Datasource V2 API in 3.0 for Iceberg (use iceberg-spark3-runtime for a shaded version) iceberg-flink is an implementation of Flink\u2019s Table and DataStream API for Iceberg (use iceberg-flink-runtime for a shaded version) iceberg-hive3 is an implementation of Hive 3 specific SerDe\u2019s for Timestamp, TimestampWithZone, and Date object inspectors (use iceberg-hive-runtime for a shaded version). iceberg-mr is an implementation of MapReduce and Hive InputFormats and SerDes for Iceberg (use iceberg-hive-runtime for a shaded version for use with Hive) iceberg-nessie is a module used to integrate Iceberg table metadata history and operations with Project Nessie iceberg-data is a client library used to read Iceberg tables from JVM applications iceberg-pig is an implementation of Pig\u2019s LoadFunc API for Iceberg iceberg-runtime generates a shaded runtime jar for Spark to integrate with iceberg tables","title":"Modules"},{"location":"aws/","text":"Iceberg AWS Integrations \u00b6 Iceberg provides integration with different AWS services through the iceberg-aws module. This section describes how to use Iceberg with AWS. Enabling AWS Integration \u00b6 The iceberg-aws module is bundled with Spark and Flink engine runtimes for all versions from 0.11.0 onwards. However, the AWS clients are not bundled so that you can use the same client version as your application. You will need to provide the AWS v2 SDK because that is what Iceberg depends on. You can choose to use the AWS SDK bundle , or individual AWS client packages (Glue, S3, DynamoDB, KMS, STS) if you would like to have a minimal dependency footprint. All the default AWS clients use the URL Connection HTTP Client for HTTP connection management. This dependency is not part of the AWS SDK bundle and needs to be added separately. To choose a different HTTP client library such as Apache HTTP Client , see the section client customization for more details. All the AWS module features can be loaded through custom catalog properties, you can go to the documentations of each engine to see how to load a custom catalog. Here are some examples. Spark \u00b6 For example, to use AWS features with Spark 3 and AWS clients version 2.15.40, you can start the Spark SQL shell with: # add Iceberg dependency ICEBERG_VERSION=0.12.1 DEPENDENCIES=\"org.apache.iceberg:iceberg-spark3-runtime:$ICEBERG_VERSION\" # add AWS dependnecy AWS_SDK_VERSION=2.15.40 AWS_MAVEN_GROUP=software.amazon.awssdk AWS_PACKAGES=( \"bundle\" \"url-connection-client\" ) for pkg in \"${AWS_PACKAGES[@]}\"; do DEPENDENCIES+=\",$AWS_MAVEN_GROUP:$pkg:$AWS_SDK_VERSION\" done # start Spark SQL client shell spark-sql --packages $DEPENDENCIES \\ --conf spark.sql.catalog.my_catalog=org.apache.iceberg.spark.SparkCatalog \\ --conf spark.sql.catalog.my_catalog.warehouse=s3://my-bucket/my/key/prefix \\ --conf spark.sql.catalog.my_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog \\ --conf spark.sql.catalog.my_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO \\ --conf spark.sql.catalog.my_catalog.lock-impl=org.apache.iceberg.aws.glue.DynamoLockManager \\ --conf spark.sql.catalog.my_catalog.lock.table=myGlueLockTable As you can see, In the shell command, we use --packages to specify the additional AWS bundle and HTTP client dependencies with their version as 2.15.40 . Flink \u00b6 To use AWS module with Flink, you can download the necessary dependencies and specify them when starting the Flink SQL client: # download Iceberg dependency ICEBERG_VERSION=0.12.1 MAVEN_URL=https://repo1.maven.org/maven2 ICEBERG_MAVEN_URL=$MAVEN_URL/org/apache/iceberg wget $ICEBERG_MAVEN_URL/iceberg-flink-runtime/$ICEBERG_VERSION/iceberg-flink-runtime-$ICEBERG_VERSION.jar # download AWS dependnecy AWS_SDK_VERSION=2.15.40 AWS_MAVEN_URL=$MAVEN_URL/software/amazon/awssdk AWS_PACKAGES=( \"bundle\" \"url-connection-client\" ) for pkg in \"${AWS_PACKAGES[@]}\"; do wget $AWS_MAVEN_URL/$pkg/$AWS_SDK_VERSION/$pkg-$AWS_SDK_VERSION.jar done # start Flink SQL client shell /path/to/bin/sql-client.sh embedded \\ -j iceberg-flink-runtime-$ICEBERG_VERSION.jar \\ -j bundle-$AWS_SDK_VERSION.jar \\ -j url-connection-client-$AWS_SDK_VERSION.jar \\ shell With those dependencies, you can create a Flink catalog like the following: CREATE CATALOG my_catalog WITH ( 'type'='iceberg', 'warehouse'='s3://my-bucket/my/key/prefix', 'catalog-impl'='org.apache.iceberg.aws.glue.GlueCatalog', 'io-impl'='org.apache.iceberg.aws.s3.S3FileIO', 'lock-impl'='org.apache.iceberg.aws.glue.DynamoLockManager', 'lock.table'='myGlueLockTable' ); You can also specify the catalog configurations in sql-client-defaults.yaml to preload it: catalogs: - name: my_catalog type: iceberg warehouse: s3://my-bucket/my/key/prefix catalog-impl: org.apache.iceberg.aws.glue.GlueCatalog io-impl: org.apache.iceberg.aws.s3.S3FileIO lock-impl: org.apache.iceberg.aws.glue.DynamoLockManager lock.table: myGlueLockTable Hive \u00b6 To use AWS module with Hive, you can download the necessary dependencies similar to the Flink example, and then add them to the Hive classpath or add the jars at runtime in CLI: add jar /my/path/to/iceberg-hive-runtime.jar; add jar /my/path/to/aws/bundle.jar; add jar /my/path/to/aws/url-connection-client.jar; With those dependencies, you can register a Glue catalog and create external tables in Hive at runtime in CLI by: SET iceberg.engine.hive.enabled=true; SET hive.vectorized.execution.enabled=false; SET iceberg.catalog.glue.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog; SET iceberg.catalog.glue.warehouse=s3://my-bucket/my/key/prefix; SET iceberg.catalog.glue.lock-impl=org.apache.iceberg.aws.glue.DynamoLockManager; SET iceberg.catalog.glue.lock.table=myGlueLockTable; -- suppose you have an Iceberg table database_a.table_a created by GlueCatalog CREATE EXTERNAL TABLE database_a.table_a STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' TBLPROPERTIES ('iceberg.catalog'='glue'); You can also preload the catalog by setting the configurations above in hive-site.xml . Catalogs \u00b6 There are multiple different options that users can choose to build an Iceberg catalog with AWS. Glue Catalog \u00b6 Iceberg enables the use of AWS Glue as the Catalog implementation. When used, an Iceberg namespace is stored as a Glue Database , an Iceberg table is stored as a Glue Table , and every Iceberg table version is stored as a Glue TableVersion . You can start using Glue catalog by specifying the catalog-impl as org.apache.iceberg.aws.glue.GlueCatalog , just like what is shown in the enabling AWS integration section above. More details about loading the catalog can be found in individual engine pages, such as Spark and Flink . Glue Catalog ID \u00b6 There is a unique Glue metastore in each AWS account and each AWS region. By default, GlueCatalog chooses the Glue metastore to use based on the user\u2019s default AWS client credential and region setup. You can specify the Glue catalog ID through glue.id catalog property to point to a Glue catalog in a different AWS account. The Glue catalog ID is your numeric AWS account ID. If the Glue catalog is in a different region, you should configure you AWS client to point to the correct region, see more details in AWS client customization . Skip Archive \u00b6 By default, Glue stores all the table versions created and user can rollback a table to any historical version if needed. However, if you are streaming data to Iceberg, this will easily create a lot of Glue table versions. Therefore, it is recommended to turn off the archive feature in Glue by setting glue.skip-archive to true . For more details, please read Glue Quotas and the UpdateTable API . DynamoDB for Commit Locking \u00b6 Glue does not have a strong guarantee over concurrent updates to a table. Although it throws ConcurrentModificationException when detecting two processes updating a table at the same time, there is no guarantee that one update would not clobber the other update. Therefore, DynamoDB can be used for Glue, so that for every commit, GlueCatalog first obtains a lock using a helper DynamoDB table and then try to safely modify the Glue table. This feature requires the following lock related catalog properties: Set lock-impl as org.apache.iceberg.aws.glue.DynamoLockManager . Set lock.table as the DynamoDB table name you would like to use. If the lock table with the given name does not exist in DynamoDB, a new table is created with billing mode set as pay-per-request . Other lock related catalog properties can also be used to adjust locking behaviors such as heartbeat interval. For more details, please refer to Lock catalog properties . Warehouse Location \u00b6 Similar to all other catalog implementations, warehouse is a required catalog property to determine the root path of the data warehouse in storage. By default, Glue only allows a warehouse location in S3 because of the use of S3FileIO . To store data in a different local or cloud store, Glue catalog can switch to use HadoopFileIO or any custom FileIO by setting the io-impl catalog property. Details about this feature can be found in the custom FileIO section. Table Location \u00b6 By default, the root location for a table my_table of namespace my_ns is at my-warehouse-location/my-ns.db/my-table . This default root location can be changed at both namespace and table level. To use a different path prefix for all tables under a namespace, use AWS console or any AWS Glue client SDK you like to update the locationUri attribute of the corresponding Glue database. For example, you can update the locationUri of my_ns to s3://my-ns-bucket , then any newly created table will have a default root location under the new prefix. For instance, a new table my_table_2 will have its root location at s3://my-ns-bucket/my_table_2 . To use a completely different root path for a specific table, set the location table property to the desired root path value you want. For example, in Spark SQL you can do: CREATE TABLE my_catalog.my_ns.my_table ( id bigint, data string, category string) USING iceberg OPTIONS ('location'='s3://my-special-table-bucket') PARTITIONED BY (category); For engines like Spark that supports the LOCATION keyword, the above SQL statement is equivalent to: CREATE TABLE my_catalog.my_ns.my_table ( id bigint, data string, category string) USING iceberg LOCATION 's3://my-special-table-bucket' PARTITIONED BY (category); DynamoDB Catalog \u00b6 Iceberg supports using a DynamoDB table to record and manage database and table information. Configurations \u00b6 The DynamoDB catalog supports the following configurations: Property Default Description dynamodb.table-name iceberg name of the DynamoDB table used by DynamoDbCatalog Internal Table Design \u00b6 The DynamoDB table is designed with the following columns: Column Key Type Description identifier partition key string table identifier such as db1.table1 , or string NAMESPACE for namespaces namespace sort key string namespace name. A global secondary index (GSI) is created with namespace as partition key, identifier as sort key, no other projected columns v string row version, used for optimistic locking updated_at number timestamp (millis) of the last update created_at number timestamp (millis) of the table creation p.<property_key> string Iceberg-defined table properties including table_type , metadata_location and previous_metadata_location or namespace properties This design has the following benefits: it avoids potential hot partition issue if there are heavy write traffic to the tables within the same namespace, because the partition key is at the table level namespace operations are clustered in a single partition to avoid affecting table commit operations a sort key to partition key reverse GSI is used for list table operation, and all other operations are single row ops or single partition query. No full table scan is needed for any operation in the catalog. a string UUID version field v is used instead of updated_at to avoid 2 processes committing at the same millisecond multi-row transaction is used for catalog.renameTable to ensure idempotency properties are flattened as top level columns so that user can add custom GSI on any property field to customize the catalog. For example, users can store owner information as table property owner , and search tables by owner by adding a GSI on the p.owner column. RDS JDBC Catalog \u00b6 Iceberg also supports JDBC catalog which uses a table in a relational database to manage Iceberg tables. You can configure to use JDBC catalog with relational database services like AWS RDS . Read the JDBC integration page for guides and examples about using the JDBC catalog. Read this AWS documentation for more details about configuring JDBC catalog with IAM authentication. Which catalog to choose? \u00b6 With all the available options, we offer the following guidance when choosing the right catalog to use for your application: if your organization has an existing Glue metastore or plans to use the AWS analytics ecosystem including Glue, Athena , EMR , Redshift and LakeFormation , Glue catalog provides the easiest integration. if your application requires frequent updates to table or high read and write throughput (e.g. streaming write), DynamoDB catalog provides the best performance through optimistic locking. if you would like to enforce access control for tables in a catalog, Glue tables can be managed as an IAM resource , whereas DynamoDB catalog tables can only be managed through item-level permission which is much more complicated. if you would like to query tables based on table property information without the need to scan the entire catalog, DynamoDB catalog allows you to build secondary indexes for any arbitrary property field and provide efficient query performance. if you would like to have the benefit of DynamoDB catalog while also connect to Glue, you can enable DynamoDB stream with Lambda trigger to asynchronously update your Glue metastore with table information in the DynamoDB catalog. if your organization already maintains an existing relational database in RDS or uses serverless Aurora to manage tables, JDBC catalog provides the easiest integration. S3 FileIO \u00b6 Iceberg allows users to write data to S3 through S3FileIO . GlueCatalog by default uses this FileIO , and other catalogs can load this FileIO using the io-impl catalog property. Progressive Multipart Upload \u00b6 S3FileIO implements a customized progressive multipart upload algorithm to upload data. Data files are uploaded by parts in parallel as soon as each part is ready, and each file part is deleted as soon as its upload process completes. This provides maximized upload speed and minimized local disk usage during uploads. Here are the configurations that users can tune related to this feature: Property Default Description s3.multipart.num-threads the available number of processors in the system number of threads to use for uploading parts to S3 (shared across all output streams) s3.multipart.part-size-bytes 32MB the size of a single part for multipart upload requests s3.multipart.threshold 1.5 the threshold expressed as a factor times the multipart size at which to switch from uploading using a single put object request to uploading using multipart upload s3.staging-dir java.io.tmpdir property value the directory to hold temporary files S3 Server Side Encryption \u00b6 S3FileIO supports all 3 S3 server side encryption modes: SSE-S3 : When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data. SSE-KMS : Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS) is similar to SSE-S3, but with some additional benefits and charges for using this service. There are separate permissions for the use of a CMK that provides added protection against unauthorized access of your objects in Amazon S3. SSE-KMS also provides you with an audit trail that shows when your CMK was used and by whom. Additionally, you can create and manage customer managed CMKs or use AWS managed CMKs that are unique to you, your service, and your Region. SSE-C : With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption, when you access your objects. To enable server side encryption, use the following configuration properties: Property Default Description s3.sse.type none none , s3 , kms or custom s3.sse.key aws/s3 for kms type, null otherwise A KMS Key ID or ARN for kms type, or a custom base-64 AES256 symmetric key for custom type. s3.sse.md5 null If SSE type is custom , this value must be set as the base-64 MD5 digest of the symmetric key to ensure integrity. S3 Access Control List \u00b6 S3FileIO supports S3 access control list (ACL) for detailed access control. User can choose the ACL level by setting the s3.acl property. For more details, please read S3 ACL Documentation . Object Store File Layout \u00b6 S3 and many other cloud storage services throttle requests based on object prefix . Data stored in S3 with a traditional Hive storage layout can face S3 request throttling as objects are stored under the same filepath prefix. Iceberg by default uses the Hive storage layout, but can be switched to use the ObjectStoreLocationProvider . With ObjectStoreLocationProvider , a determenistic hash is generated for each stored file, with the hash appended directly after the write.data.path . This ensures files written to s3 are equally distributed across multiple prefixes in the S3 bucket. Resulting in minimized throttling and maximized throughput for S3-related IO operations. When using ObjectStoreLocationProvider having a shared and short write.data.path across your Iceberg tables will improve performance. For more information on how S3 scales API QPS, checkout the 2018 re:Invent session on Best Practices for Amazon S3 and Amazon S3 Glacier . At 53:39 it covers how S3 scales/partitions & at 54:50 it discusses the 30-60 minute wait time before new partitions are created. To use the ObjectStorageLocationProvider add 'write.object-storage.enabled'=true in the table\u2019s properties. Below is an example Spark SQL command to create a table using the ObjectStorageLocationProvider : CREATE TABLE my_catalog.my_ns.my_table ( id bigint, data string, category string) USING iceberg OPTIONS ( 'write.object-storage.enabled'=true, 'write.data.path'='s3://my-table-data-bucket') PARTITIONED BY (category); We can then insert a single row into this new table INSERT INTO my_catalog.my_ns.my_table VALUES (1, \"Pizza\", \"orders\"); Which will write the data to S3 with a hash ( 2d3905f8 ) appended directly after the write.object-storage.path , ensuring reads to the table are spread evenly across S3 bucket prefixes , and improving performance. s3://my-table-data-bucket/2d3905f8/my_ns.db/my_table/category=orders/00000-0-5affc076-96a4-48f2-9cd2-d5efbc9f0c94-00001.parquet Note, the path resolution logic for ObjectStoreLocationProvider is write.data.path then <tableLocation>/data . However, for the older versions up to 0.12.0, the logic is as follows: - before 0.12.0, write.object-storage.path must be set. - at 0.12.0, write.object-storage.path then write.folder-storage.path then <tableLocation>/data . For more details, please refer to the LocationProvider Configuration section. S3 Strong Consistency \u00b6 In November 2020, S3 announced strong consistency for all read operations, and Iceberg is updated to fully leverage this feature. There is no redundant consistency wait and check which might negatively impact performance during IO operations. Hadoop S3A FileSystem \u00b6 Before S3FileIO was introduced, many Iceberg users choose to use HadoopFileIO to write data to S3 through the S3A FileSystem . As introduced in the previous sections, S3FileIO adopts latest AWS clients and S3 features for optimized security and performance, and is thus recommend for S3 use cases rather than the S3A FileSystem. S3FileIO writes data with s3:// URI scheme, but it is also compatible with schemes written by the S3A FileSystem. This means for any table manifests containing s3a:// or s3n:// file paths, S3FileIO is still able to read them. This feature allows people to easily switch from S3A to S3FileIO . If for any reason you have to use S3A, here are the instructions: To store data using S3A, specify the warehouse catalog property to be an S3A path, e.g. s3a://my-bucket/my-warehouse For HiveCatalog , to also store metadata using S3A, specify the Hadoop config property hive.metastore.warehouse.dir to be an S3A path. Add hadoop-aws as a runtime dependency of your compute engine. Configure AWS settings based on hadoop-aws documentation (make sure you check the version, S3A configuration varies a lot based on the version you use). AWS Client Customization \u00b6 Many organizations have customized their way of configuring AWS clients with their own credential provider, access proxy, retry strategy, etc. Iceberg allows users to plug in their own implementation of org.apache.iceberg.aws.AwsClientFactory by setting the client.factory catalog property. Cross-Account and Cross-Region Access \u00b6 It is a common use case for organizations to have a centralized AWS account for Glue metastore and S3 buckets, and use different AWS accounts and regions for different teams to access those resources. In this case, a cross-account IAM role is needed to access those centralized resources. Iceberg provides an AWS client factory AssumeRoleAwsClientFactory to support this common use case. This also serves as an example for users who would like to implement their own AWS client factory. This client factory has the following configurable catalog properties: Property Default Description client.assume-role.arn null, requires user input ARN of the role to assume, e.g. arn:aws:iam::123456789:role/myRoleToAssume client.assume-role.region null, requires user input All AWS clients except the STS client will use the given region instead of the default region chain client.assume-role.external-id null An optional external ID client.assume-role.timeout-sec 1 hour Timeout of each assume role session. At the end of the timeout, a new set of role session credentials will be fetched through a STS client. By using this client factory, an STS client is initialized with the default credential and region to assume the specified role. The Glue, S3 and DynamoDB clients are then initialized with the assume-role credential and region to access resources. Here is an example to start Spark shell with this client factory: spark-sql --packages org.apache.iceberg:iceberg-spark3-runtime:0.12.1,software.amazon.awssdk:bundle:2.15.40 \\ --conf spark.sql.catalog.my_catalog=org.apache.iceberg.spark.SparkCatalog \\ --conf spark.sql.catalog.my_catalog.warehouse=s3://my-bucket/my/key/prefix \\ --conf spark.sql.catalog.my_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog \\ --conf spark.sql.catalog.my_catalog.client.factory=org.apache.iceberg.aws.AssumeRoleAwsClientFactory \\ --conf spark.sql.catalog.my_catalog.client.assume-role.arn=arn:aws:iam::123456789:role/myRoleToAssume \\ --conf spark.sql.catalog.my_catalog.client.assume-role.region=ap-northeast-1 Run Iceberg on AWS \u00b6 Amazon EMR \u00b6 Amazon EMR can provision clusters with Spark (EMR 6 for Spark 3, EMR 5 for Spark 2), Hive , Flink , Trino that can run Iceberg. You can use a bootstrap action similar to the following to pre-install all necessary dependencies: #!/bin/bash AWS_SDK_VERSION=2.15.40 ICEBERG_VERSION=0.12.1 MAVEN_URL=https://repo1.maven.org/maven2 ICEBERG_MAVEN_URL=$MAVEN_URL/org/apache/iceberg AWS_MAVEN_URL=$MAVEN_URL/software/amazon/awssdk # NOTE: this is just an example shared class path between Spark and Flink, # please choose a proper class path for production. LIB_PATH=/usr/share/aws/aws-java-sdk/ AWS_PACKAGES=( \"bundle\" \"url-connection-client\" ) ICEBERG_PACKAGES=( \"iceberg-spark3-runtime\" \"iceberg-flink-runtime\" ) install_dependencies () { install_path=$1 download_url=$2 version=$3 shift pkgs=(\"$@\") for pkg in \"${pkgs[@]}\"; do sudo wget -P $install_path $download_url/$pkg/$version/$pkg-$version.jar done } install_dependencies $LIB_PATH $ICEBERG_MAVEN_URL $ICEBERG_VERSION \"${ICEBERG_PACKAGES[@]}\" install_dependencies $LIB_PATH $AWS_MAVEN_URL $AWS_SDK_VERSION \"${AWS_PACKAGES[@]}\" Amazon Kinesis \u00b6 Amazon Kinesis Data Analytics provides a platform to run fully managed Apache Flink applications. You can include Iceberg in your application Jar and run it in the platform.","title":"AWS"},{"location":"aws/#iceberg-aws-integrations","text":"Iceberg provides integration with different AWS services through the iceberg-aws module. This section describes how to use Iceberg with AWS.","title":"Iceberg AWS Integrations"},{"location":"aws/#enabling-aws-integration","text":"The iceberg-aws module is bundled with Spark and Flink engine runtimes for all versions from 0.11.0 onwards. However, the AWS clients are not bundled so that you can use the same client version as your application. You will need to provide the AWS v2 SDK because that is what Iceberg depends on. You can choose to use the AWS SDK bundle , or individual AWS client packages (Glue, S3, DynamoDB, KMS, STS) if you would like to have a minimal dependency footprint. All the default AWS clients use the URL Connection HTTP Client for HTTP connection management. This dependency is not part of the AWS SDK bundle and needs to be added separately. To choose a different HTTP client library such as Apache HTTP Client , see the section client customization for more details. All the AWS module features can be loaded through custom catalog properties, you can go to the documentations of each engine to see how to load a custom catalog. Here are some examples.","title":"Enabling AWS Integration"},{"location":"aws/#spark","text":"For example, to use AWS features with Spark 3 and AWS clients version 2.15.40, you can start the Spark SQL shell with: # add Iceberg dependency ICEBERG_VERSION=0.12.1 DEPENDENCIES=\"org.apache.iceberg:iceberg-spark3-runtime:$ICEBERG_VERSION\" # add AWS dependnecy AWS_SDK_VERSION=2.15.40 AWS_MAVEN_GROUP=software.amazon.awssdk AWS_PACKAGES=( \"bundle\" \"url-connection-client\" ) for pkg in \"${AWS_PACKAGES[@]}\"; do DEPENDENCIES+=\",$AWS_MAVEN_GROUP:$pkg:$AWS_SDK_VERSION\" done # start Spark SQL client shell spark-sql --packages $DEPENDENCIES \\ --conf spark.sql.catalog.my_catalog=org.apache.iceberg.spark.SparkCatalog \\ --conf spark.sql.catalog.my_catalog.warehouse=s3://my-bucket/my/key/prefix \\ --conf spark.sql.catalog.my_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog \\ --conf spark.sql.catalog.my_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO \\ --conf spark.sql.catalog.my_catalog.lock-impl=org.apache.iceberg.aws.glue.DynamoLockManager \\ --conf spark.sql.catalog.my_catalog.lock.table=myGlueLockTable As you can see, In the shell command, we use --packages to specify the additional AWS bundle and HTTP client dependencies with their version as 2.15.40 .","title":"Spark"},{"location":"aws/#flink","text":"To use AWS module with Flink, you can download the necessary dependencies and specify them when starting the Flink SQL client: # download Iceberg dependency ICEBERG_VERSION=0.12.1 MAVEN_URL=https://repo1.maven.org/maven2 ICEBERG_MAVEN_URL=$MAVEN_URL/org/apache/iceberg wget $ICEBERG_MAVEN_URL/iceberg-flink-runtime/$ICEBERG_VERSION/iceberg-flink-runtime-$ICEBERG_VERSION.jar # download AWS dependnecy AWS_SDK_VERSION=2.15.40 AWS_MAVEN_URL=$MAVEN_URL/software/amazon/awssdk AWS_PACKAGES=( \"bundle\" \"url-connection-client\" ) for pkg in \"${AWS_PACKAGES[@]}\"; do wget $AWS_MAVEN_URL/$pkg/$AWS_SDK_VERSION/$pkg-$AWS_SDK_VERSION.jar done # start Flink SQL client shell /path/to/bin/sql-client.sh embedded \\ -j iceberg-flink-runtime-$ICEBERG_VERSION.jar \\ -j bundle-$AWS_SDK_VERSION.jar \\ -j url-connection-client-$AWS_SDK_VERSION.jar \\ shell With those dependencies, you can create a Flink catalog like the following: CREATE CATALOG my_catalog WITH ( 'type'='iceberg', 'warehouse'='s3://my-bucket/my/key/prefix', 'catalog-impl'='org.apache.iceberg.aws.glue.GlueCatalog', 'io-impl'='org.apache.iceberg.aws.s3.S3FileIO', 'lock-impl'='org.apache.iceberg.aws.glue.DynamoLockManager', 'lock.table'='myGlueLockTable' ); You can also specify the catalog configurations in sql-client-defaults.yaml to preload it: catalogs: - name: my_catalog type: iceberg warehouse: s3://my-bucket/my/key/prefix catalog-impl: org.apache.iceberg.aws.glue.GlueCatalog io-impl: org.apache.iceberg.aws.s3.S3FileIO lock-impl: org.apache.iceberg.aws.glue.DynamoLockManager lock.table: myGlueLockTable","title":"Flink"},{"location":"aws/#hive","text":"To use AWS module with Hive, you can download the necessary dependencies similar to the Flink example, and then add them to the Hive classpath or add the jars at runtime in CLI: add jar /my/path/to/iceberg-hive-runtime.jar; add jar /my/path/to/aws/bundle.jar; add jar /my/path/to/aws/url-connection-client.jar; With those dependencies, you can register a Glue catalog and create external tables in Hive at runtime in CLI by: SET iceberg.engine.hive.enabled=true; SET hive.vectorized.execution.enabled=false; SET iceberg.catalog.glue.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog; SET iceberg.catalog.glue.warehouse=s3://my-bucket/my/key/prefix; SET iceberg.catalog.glue.lock-impl=org.apache.iceberg.aws.glue.DynamoLockManager; SET iceberg.catalog.glue.lock.table=myGlueLockTable; -- suppose you have an Iceberg table database_a.table_a created by GlueCatalog CREATE EXTERNAL TABLE database_a.table_a STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' TBLPROPERTIES ('iceberg.catalog'='glue'); You can also preload the catalog by setting the configurations above in hive-site.xml .","title":"Hive"},{"location":"aws/#catalogs","text":"There are multiple different options that users can choose to build an Iceberg catalog with AWS.","title":"Catalogs"},{"location":"aws/#glue-catalog","text":"Iceberg enables the use of AWS Glue as the Catalog implementation. When used, an Iceberg namespace is stored as a Glue Database , an Iceberg table is stored as a Glue Table , and every Iceberg table version is stored as a Glue TableVersion . You can start using Glue catalog by specifying the catalog-impl as org.apache.iceberg.aws.glue.GlueCatalog , just like what is shown in the enabling AWS integration section above. More details about loading the catalog can be found in individual engine pages, such as Spark and Flink .","title":"Glue Catalog"},{"location":"aws/#glue-catalog-id","text":"There is a unique Glue metastore in each AWS account and each AWS region. By default, GlueCatalog chooses the Glue metastore to use based on the user\u2019s default AWS client credential and region setup. You can specify the Glue catalog ID through glue.id catalog property to point to a Glue catalog in a different AWS account. The Glue catalog ID is your numeric AWS account ID. If the Glue catalog is in a different region, you should configure you AWS client to point to the correct region, see more details in AWS client customization .","title":"Glue Catalog ID"},{"location":"aws/#skip-archive","text":"By default, Glue stores all the table versions created and user can rollback a table to any historical version if needed. However, if you are streaming data to Iceberg, this will easily create a lot of Glue table versions. Therefore, it is recommended to turn off the archive feature in Glue by setting glue.skip-archive to true . For more details, please read Glue Quotas and the UpdateTable API .","title":"Skip Archive"},{"location":"aws/#dynamodb-for-commit-locking","text":"Glue does not have a strong guarantee over concurrent updates to a table. Although it throws ConcurrentModificationException when detecting two processes updating a table at the same time, there is no guarantee that one update would not clobber the other update. Therefore, DynamoDB can be used for Glue, so that for every commit, GlueCatalog first obtains a lock using a helper DynamoDB table and then try to safely modify the Glue table. This feature requires the following lock related catalog properties: Set lock-impl as org.apache.iceberg.aws.glue.DynamoLockManager . Set lock.table as the DynamoDB table name you would like to use. If the lock table with the given name does not exist in DynamoDB, a new table is created with billing mode set as pay-per-request . Other lock related catalog properties can also be used to adjust locking behaviors such as heartbeat interval. For more details, please refer to Lock catalog properties .","title":"DynamoDB for Commit Locking"},{"location":"aws/#warehouse-location","text":"Similar to all other catalog implementations, warehouse is a required catalog property to determine the root path of the data warehouse in storage. By default, Glue only allows a warehouse location in S3 because of the use of S3FileIO . To store data in a different local or cloud store, Glue catalog can switch to use HadoopFileIO or any custom FileIO by setting the io-impl catalog property. Details about this feature can be found in the custom FileIO section.","title":"Warehouse Location"},{"location":"aws/#table-location","text":"By default, the root location for a table my_table of namespace my_ns is at my-warehouse-location/my-ns.db/my-table . This default root location can be changed at both namespace and table level. To use a different path prefix for all tables under a namespace, use AWS console or any AWS Glue client SDK you like to update the locationUri attribute of the corresponding Glue database. For example, you can update the locationUri of my_ns to s3://my-ns-bucket , then any newly created table will have a default root location under the new prefix. For instance, a new table my_table_2 will have its root location at s3://my-ns-bucket/my_table_2 . To use a completely different root path for a specific table, set the location table property to the desired root path value you want. For example, in Spark SQL you can do: CREATE TABLE my_catalog.my_ns.my_table ( id bigint, data string, category string) USING iceberg OPTIONS ('location'='s3://my-special-table-bucket') PARTITIONED BY (category); For engines like Spark that supports the LOCATION keyword, the above SQL statement is equivalent to: CREATE TABLE my_catalog.my_ns.my_table ( id bigint, data string, category string) USING iceberg LOCATION 's3://my-special-table-bucket' PARTITIONED BY (category);","title":"Table Location"},{"location":"aws/#dynamodb-catalog","text":"Iceberg supports using a DynamoDB table to record and manage database and table information.","title":"DynamoDB Catalog"},{"location":"aws/#configurations","text":"The DynamoDB catalog supports the following configurations: Property Default Description dynamodb.table-name iceberg name of the DynamoDB table used by DynamoDbCatalog","title":"Configurations"},{"location":"aws/#internal-table-design","text":"The DynamoDB table is designed with the following columns: Column Key Type Description identifier partition key string table identifier such as db1.table1 , or string NAMESPACE for namespaces namespace sort key string namespace name. A global secondary index (GSI) is created with namespace as partition key, identifier as sort key, no other projected columns v string row version, used for optimistic locking updated_at number timestamp (millis) of the last update created_at number timestamp (millis) of the table creation p.<property_key> string Iceberg-defined table properties including table_type , metadata_location and previous_metadata_location or namespace properties This design has the following benefits: it avoids potential hot partition issue if there are heavy write traffic to the tables within the same namespace, because the partition key is at the table level namespace operations are clustered in a single partition to avoid affecting table commit operations a sort key to partition key reverse GSI is used for list table operation, and all other operations are single row ops or single partition query. No full table scan is needed for any operation in the catalog. a string UUID version field v is used instead of updated_at to avoid 2 processes committing at the same millisecond multi-row transaction is used for catalog.renameTable to ensure idempotency properties are flattened as top level columns so that user can add custom GSI on any property field to customize the catalog. For example, users can store owner information as table property owner , and search tables by owner by adding a GSI on the p.owner column.","title":"Internal Table Design"},{"location":"aws/#rds-jdbc-catalog","text":"Iceberg also supports JDBC catalog which uses a table in a relational database to manage Iceberg tables. You can configure to use JDBC catalog with relational database services like AWS RDS . Read the JDBC integration page for guides and examples about using the JDBC catalog. Read this AWS documentation for more details about configuring JDBC catalog with IAM authentication.","title":"RDS JDBC Catalog"},{"location":"aws/#which-catalog-to-choose","text":"With all the available options, we offer the following guidance when choosing the right catalog to use for your application: if your organization has an existing Glue metastore or plans to use the AWS analytics ecosystem including Glue, Athena , EMR , Redshift and LakeFormation , Glue catalog provides the easiest integration. if your application requires frequent updates to table or high read and write throughput (e.g. streaming write), DynamoDB catalog provides the best performance through optimistic locking. if you would like to enforce access control for tables in a catalog, Glue tables can be managed as an IAM resource , whereas DynamoDB catalog tables can only be managed through item-level permission which is much more complicated. if you would like to query tables based on table property information without the need to scan the entire catalog, DynamoDB catalog allows you to build secondary indexes for any arbitrary property field and provide efficient query performance. if you would like to have the benefit of DynamoDB catalog while also connect to Glue, you can enable DynamoDB stream with Lambda trigger to asynchronously update your Glue metastore with table information in the DynamoDB catalog. if your organization already maintains an existing relational database in RDS or uses serverless Aurora to manage tables, JDBC catalog provides the easiest integration.","title":"Which catalog to choose?"},{"location":"aws/#s3-fileio","text":"Iceberg allows users to write data to S3 through S3FileIO . GlueCatalog by default uses this FileIO , and other catalogs can load this FileIO using the io-impl catalog property.","title":"S3 FileIO"},{"location":"aws/#progressive-multipart-upload","text":"S3FileIO implements a customized progressive multipart upload algorithm to upload data. Data files are uploaded by parts in parallel as soon as each part is ready, and each file part is deleted as soon as its upload process completes. This provides maximized upload speed and minimized local disk usage during uploads. Here are the configurations that users can tune related to this feature: Property Default Description s3.multipart.num-threads the available number of processors in the system number of threads to use for uploading parts to S3 (shared across all output streams) s3.multipart.part-size-bytes 32MB the size of a single part for multipart upload requests s3.multipart.threshold 1.5 the threshold expressed as a factor times the multipart size at which to switch from uploading using a single put object request to uploading using multipart upload s3.staging-dir java.io.tmpdir property value the directory to hold temporary files","title":"Progressive Multipart Upload"},{"location":"aws/#s3-server-side-encryption","text":"S3FileIO supports all 3 S3 server side encryption modes: SSE-S3 : When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data. SSE-KMS : Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS) is similar to SSE-S3, but with some additional benefits and charges for using this service. There are separate permissions for the use of a CMK that provides added protection against unauthorized access of your objects in Amazon S3. SSE-KMS also provides you with an audit trail that shows when your CMK was used and by whom. Additionally, you can create and manage customer managed CMKs or use AWS managed CMKs that are unique to you, your service, and your Region. SSE-C : With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption, when you access your objects. To enable server side encryption, use the following configuration properties: Property Default Description s3.sse.type none none , s3 , kms or custom s3.sse.key aws/s3 for kms type, null otherwise A KMS Key ID or ARN for kms type, or a custom base-64 AES256 symmetric key for custom type. s3.sse.md5 null If SSE type is custom , this value must be set as the base-64 MD5 digest of the symmetric key to ensure integrity.","title":"S3 Server Side Encryption"},{"location":"aws/#s3-access-control-list","text":"S3FileIO supports S3 access control list (ACL) for detailed access control. User can choose the ACL level by setting the s3.acl property. For more details, please read S3 ACL Documentation .","title":"S3 Access Control List"},{"location":"aws/#object-store-file-layout","text":"S3 and many other cloud storage services throttle requests based on object prefix . Data stored in S3 with a traditional Hive storage layout can face S3 request throttling as objects are stored under the same filepath prefix. Iceberg by default uses the Hive storage layout, but can be switched to use the ObjectStoreLocationProvider . With ObjectStoreLocationProvider , a determenistic hash is generated for each stored file, with the hash appended directly after the write.data.path . This ensures files written to s3 are equally distributed across multiple prefixes in the S3 bucket. Resulting in minimized throttling and maximized throughput for S3-related IO operations. When using ObjectStoreLocationProvider having a shared and short write.data.path across your Iceberg tables will improve performance. For more information on how S3 scales API QPS, checkout the 2018 re:Invent session on Best Practices for Amazon S3 and Amazon S3 Glacier . At 53:39 it covers how S3 scales/partitions & at 54:50 it discusses the 30-60 minute wait time before new partitions are created. To use the ObjectStorageLocationProvider add 'write.object-storage.enabled'=true in the table\u2019s properties. Below is an example Spark SQL command to create a table using the ObjectStorageLocationProvider : CREATE TABLE my_catalog.my_ns.my_table ( id bigint, data string, category string) USING iceberg OPTIONS ( 'write.object-storage.enabled'=true, 'write.data.path'='s3://my-table-data-bucket') PARTITIONED BY (category); We can then insert a single row into this new table INSERT INTO my_catalog.my_ns.my_table VALUES (1, \"Pizza\", \"orders\"); Which will write the data to S3 with a hash ( 2d3905f8 ) appended directly after the write.object-storage.path , ensuring reads to the table are spread evenly across S3 bucket prefixes , and improving performance. s3://my-table-data-bucket/2d3905f8/my_ns.db/my_table/category=orders/00000-0-5affc076-96a4-48f2-9cd2-d5efbc9f0c94-00001.parquet Note, the path resolution logic for ObjectStoreLocationProvider is write.data.path then <tableLocation>/data . However, for the older versions up to 0.12.0, the logic is as follows: - before 0.12.0, write.object-storage.path must be set. - at 0.12.0, write.object-storage.path then write.folder-storage.path then <tableLocation>/data . For more details, please refer to the LocationProvider Configuration section.","title":"Object Store File Layout"},{"location":"aws/#s3-strong-consistency","text":"In November 2020, S3 announced strong consistency for all read operations, and Iceberg is updated to fully leverage this feature. There is no redundant consistency wait and check which might negatively impact performance during IO operations.","title":"S3 Strong Consistency"},{"location":"aws/#hadoop-s3a-filesystem","text":"Before S3FileIO was introduced, many Iceberg users choose to use HadoopFileIO to write data to S3 through the S3A FileSystem . As introduced in the previous sections, S3FileIO adopts latest AWS clients and S3 features for optimized security and performance, and is thus recommend for S3 use cases rather than the S3A FileSystem. S3FileIO writes data with s3:// URI scheme, but it is also compatible with schemes written by the S3A FileSystem. This means for any table manifests containing s3a:// or s3n:// file paths, S3FileIO is still able to read them. This feature allows people to easily switch from S3A to S3FileIO . If for any reason you have to use S3A, here are the instructions: To store data using S3A, specify the warehouse catalog property to be an S3A path, e.g. s3a://my-bucket/my-warehouse For HiveCatalog , to also store metadata using S3A, specify the Hadoop config property hive.metastore.warehouse.dir to be an S3A path. Add hadoop-aws as a runtime dependency of your compute engine. Configure AWS settings based on hadoop-aws documentation (make sure you check the version, S3A configuration varies a lot based on the version you use).","title":"Hadoop S3A FileSystem"},{"location":"aws/#aws-client-customization","text":"Many organizations have customized their way of configuring AWS clients with their own credential provider, access proxy, retry strategy, etc. Iceberg allows users to plug in their own implementation of org.apache.iceberg.aws.AwsClientFactory by setting the client.factory catalog property.","title":"AWS Client Customization"},{"location":"aws/#cross-account-and-cross-region-access","text":"It is a common use case for organizations to have a centralized AWS account for Glue metastore and S3 buckets, and use different AWS accounts and regions for different teams to access those resources. In this case, a cross-account IAM role is needed to access those centralized resources. Iceberg provides an AWS client factory AssumeRoleAwsClientFactory to support this common use case. This also serves as an example for users who would like to implement their own AWS client factory. This client factory has the following configurable catalog properties: Property Default Description client.assume-role.arn null, requires user input ARN of the role to assume, e.g. arn:aws:iam::123456789:role/myRoleToAssume client.assume-role.region null, requires user input All AWS clients except the STS client will use the given region instead of the default region chain client.assume-role.external-id null An optional external ID client.assume-role.timeout-sec 1 hour Timeout of each assume role session. At the end of the timeout, a new set of role session credentials will be fetched through a STS client. By using this client factory, an STS client is initialized with the default credential and region to assume the specified role. The Glue, S3 and DynamoDB clients are then initialized with the assume-role credential and region to access resources. Here is an example to start Spark shell with this client factory: spark-sql --packages org.apache.iceberg:iceberg-spark3-runtime:0.12.1,software.amazon.awssdk:bundle:2.15.40 \\ --conf spark.sql.catalog.my_catalog=org.apache.iceberg.spark.SparkCatalog \\ --conf spark.sql.catalog.my_catalog.warehouse=s3://my-bucket/my/key/prefix \\ --conf spark.sql.catalog.my_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog \\ --conf spark.sql.catalog.my_catalog.client.factory=org.apache.iceberg.aws.AssumeRoleAwsClientFactory \\ --conf spark.sql.catalog.my_catalog.client.assume-role.arn=arn:aws:iam::123456789:role/myRoleToAssume \\ --conf spark.sql.catalog.my_catalog.client.assume-role.region=ap-northeast-1","title":"Cross-Account and Cross-Region Access"},{"location":"aws/#run-iceberg-on-aws","text":"","title":"Run Iceberg on AWS"},{"location":"aws/#amazon-emr","text":"Amazon EMR can provision clusters with Spark (EMR 6 for Spark 3, EMR 5 for Spark 2), Hive , Flink , Trino that can run Iceberg. You can use a bootstrap action similar to the following to pre-install all necessary dependencies: #!/bin/bash AWS_SDK_VERSION=2.15.40 ICEBERG_VERSION=0.12.1 MAVEN_URL=https://repo1.maven.org/maven2 ICEBERG_MAVEN_URL=$MAVEN_URL/org/apache/iceberg AWS_MAVEN_URL=$MAVEN_URL/software/amazon/awssdk # NOTE: this is just an example shared class path between Spark and Flink, # please choose a proper class path for production. LIB_PATH=/usr/share/aws/aws-java-sdk/ AWS_PACKAGES=( \"bundle\" \"url-connection-client\" ) ICEBERG_PACKAGES=( \"iceberg-spark3-runtime\" \"iceberg-flink-runtime\" ) install_dependencies () { install_path=$1 download_url=$2 version=$3 shift pkgs=(\"$@\") for pkg in \"${pkgs[@]}\"; do sudo wget -P $install_path $download_url/$pkg/$version/$pkg-$version.jar done } install_dependencies $LIB_PATH $ICEBERG_MAVEN_URL $ICEBERG_VERSION \"${ICEBERG_PACKAGES[@]}\" install_dependencies $LIB_PATH $AWS_MAVEN_URL $AWS_SDK_VERSION \"${AWS_PACKAGES[@]}\"","title":"Amazon EMR"},{"location":"aws/#amazon-kinesis","text":"Amazon Kinesis Data Analytics provides a platform to run fully managed Apache Flink applications. You can include Iceberg in your application Jar and run it in the platform.","title":"Amazon Kinesis"},{"location":"benchmarks/","text":"Available Benchmarks and how to run them \u00b6 Benchmarks are located under <project-name>/jmh . It is generally favorable to only run the tests of interest rather than running all available benchmarks. Also note that JMH benchmarks run within the same JVM as the system-under-test, so results might vary between runs. Running Benchmarks on GitHub \u00b6 It is possible to run one or more Benchmarks via the JMH Benchmarks GH action on your own fork of the Iceberg repo. This GH action takes the following inputs: * The repository name where those benchmarks should be run against, such as apache/iceberg or <user>/iceberg * The branch name to run benchmarks against, such as master or my-cool-feature-branch * A list of comma-separated double-quoted Benchmark names, such as \"IcebergSourceFlatParquetDataReadBenchmark\", \"IcebergSourceFlatParquetDataFilterBenchmark\", \"IcebergSourceNestedListParquetDataWriteBenchmark\" Benchmark results will be uploaded once all benchmarks are done. It is worth noting that the GH runners have limited resources so the benchmark results should rather be seen as an indicator to guide developers in understanding code changes. It is likely that there is variability in results across different runs, therefore the benchmark results shouldn\u2019t be used to form assumptions around production choices. Running Benchmarks locally \u00b6 Below are the existing benchmarks shown with the actual commands on how to run them locally. IcebergSourceNestedListParquetDataWriteBenchmark \u00b6 A benchmark that evaluates the performance of writing nested Parquet data using Iceberg and the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=IcebergSourceNestedListParquetDataWriteBenchmark -PjmhOutputPath=benchmark/iceberg-source-nested-list-parquet-data-write-benchmark-result.txt SparkParquetReadersNestedDataBenchmark \u00b6 A benchmark that evaluates the performance of reading nested Parquet data using Iceberg and Spark Parquet readers. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=SparkParquetReadersNestedDataBenchmark -PjmhOutputPath=benchmark/spark-parquet-readers-nested-data-benchmark-result.txt SparkParquetWritersFlatDataBenchmark \u00b6 A benchmark that evaluates the performance of writing Parquet data with a flat schema using Iceberg and Spark Parquet writers. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=SparkParquetWritersFlatDataBenchmark -PjmhOutputPath=benchmark/spark-parquet-writers-flat-data-benchmark-result.txt IcebergSourceFlatORCDataReadBenchmark \u00b6 A benchmark that evaluates the performance of reading ORC data with a flat schema using Iceberg and the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=IcebergSourceFlatORCDataReadBenchmark -PjmhOutputPath=benchmark/iceberg-source-flat-orc-data-read-benchmark-result.txt SparkParquetReadersFlatDataBenchmark \u00b6 A benchmark that evaluates the performance of reading Parquet data with a flat schema using Iceberg and Spark Parquet readers. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=SparkParquetReadersFlatDataBenchmark -PjmhOutputPath=benchmark/spark-parquet-readers-flat-data-benchmark-result.txt VectorizedReadDictionaryEncodedFlatParquetDataBenchmark \u00b6 A benchmark to compare performance of reading Parquet dictionary encoded data with a flat schema using vectorized Iceberg read path and the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=VectorizedReadDictionaryEncodedFlatParquetDataBenchmark -PjmhOutputPath=benchmark/vectorized-read-dict-encoded-flat-parquet-data-result.txt IcebergSourceNestedListORCDataWriteBenchmark \u00b6 A benchmark that evaluates the performance of writing nested Parquet data using Iceberg and the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=IcebergSourceNestedListORCDataWriteBenchmark -PjmhOutputPath=benchmark/iceberg-source-nested-list-orc-data-write-benchmark-result.txt VectorizedReadFlatParquetDataBenchmark \u00b6 A benchmark to compare performance of reading Parquet data with a flat schema using vectorized Iceberg read path and the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=VectorizedReadFlatParquetDataBenchmark -PjmhOutputPath=benchmark/vectorized-read-flat-parquet-data-result.txt IcebergSourceFlatParquetDataWriteBenchmark \u00b6 A benchmark that evaluates the performance of writing Parquet data with a flat schema using Iceberg and the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=IcebergSourceFlatParquetDataWriteBenchmark -PjmhOutputPath=benchmark/iceberg-source-flat-parquet-data-write-benchmark-result.txt IcebergSourceNestedAvroDataReadBenchmark \u00b6 A benchmark that evaluates the performance of reading Avro data with a flat schema using Iceberg and the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=IcebergSourceNestedAvroDataReadBenchmark -PjmhOutputPath=benchmark/iceberg-source-nested-avro-data-read-benchmark-result.txt IcebergSourceFlatAvroDataReadBenchmark \u00b6 A benchmark that evaluates the performance of reading Avro data with a flat schema using Iceberg and the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=IcebergSourceFlatAvroDataReadBenchmark -PjmhOutputPath=benchmark/iceberg-source-flat-avro-data-read-benchmark-result.txt IcebergSourceNestedParquetDataWriteBenchmark \u00b6 A benchmark that evaluates the performance of writing nested Parquet data using Iceberg and the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=IcebergSourceNestedParquetDataWriteBenchmark -PjmhOutputPath=benchmark/iceberg-source-nested-parquet-data-write-benchmark-result.txt IcebergSourceNestedParquetDataReadBenchmark \u00b6 A benchmark that evaluates the performance of reading nested Parquet data using Iceberg and the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=IcebergSourceNestedParquetDataReadBenchmark -PjmhOutputPath=benchmark/iceberg-source-nested-parquet-data-read-benchmark-result.txt IcebergSourceNestedORCDataReadBenchmark \u00b6 A benchmark that evaluates the performance of reading ORC data with a flat schema using Iceberg and the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=IcebergSourceNestedORCDataReadBenchmark -PjmhOutputPath=benchmark/iceberg-source-nested-orc-data-read-benchmark-result.txt IcebergSourceFlatParquetDataReadBenchmark \u00b6 A benchmark that evaluates the performance of reading Parquet data with a flat schema using Iceberg and the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=IcebergSourceFlatParquetDataReadBenchmark -PjmhOutputPath=benchmark/iceberg-source-flat-parquet-data-read-benchmark-result.txt IcebergSourceFlatParquetDataFilterBenchmark \u00b6 A benchmark that evaluates the file skipping capabilities in the Spark data source for Iceberg. This class uses a dataset with a flat schema, where the records are clustered according to the column used in the filter predicate. The performance is compared to the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=IcebergSourceFlatParquetDataFilterBenchmark -PjmhOutputPath=benchmark/iceberg-source-flat-parquet-data-filter-benchmark-result.txt IcebergSourceNestedParquetDataFilterBenchmark \u00b6 A benchmark that evaluates the file skipping capabilities in the Spark data source for Iceberg. This class uses a dataset with nested data, where the records are clustered according to the column used in the filter predicate. The performance is compared to the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=IcebergSourceNestedParquetDataFilterBenchmark -PjmhOutputPath=benchmark/iceberg-source-nested-parquet-data-filter-benchmark-result.txt SparkParquetWritersNestedDataBenchmark \u00b6 A benchmark that evaluates the performance of writing nested Parquet data using Iceberg and Spark Parquet writers. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=SparkParquetWritersNestedDataBenchmark -PjmhOutputPath=benchmark/spark-parquet-writers-nested-data-benchmark-result.txt","title":"Benchmarks"},{"location":"benchmarks/#available-benchmarks-and-how-to-run-them","text":"Benchmarks are located under <project-name>/jmh . It is generally favorable to only run the tests of interest rather than running all available benchmarks. Also note that JMH benchmarks run within the same JVM as the system-under-test, so results might vary between runs.","title":"Available Benchmarks and how to run them"},{"location":"benchmarks/#running-benchmarks-on-github","text":"It is possible to run one or more Benchmarks via the JMH Benchmarks GH action on your own fork of the Iceberg repo. This GH action takes the following inputs: * The repository name where those benchmarks should be run against, such as apache/iceberg or <user>/iceberg * The branch name to run benchmarks against, such as master or my-cool-feature-branch * A list of comma-separated double-quoted Benchmark names, such as \"IcebergSourceFlatParquetDataReadBenchmark\", \"IcebergSourceFlatParquetDataFilterBenchmark\", \"IcebergSourceNestedListParquetDataWriteBenchmark\" Benchmark results will be uploaded once all benchmarks are done. It is worth noting that the GH runners have limited resources so the benchmark results should rather be seen as an indicator to guide developers in understanding code changes. It is likely that there is variability in results across different runs, therefore the benchmark results shouldn\u2019t be used to form assumptions around production choices.","title":"Running Benchmarks on GitHub"},{"location":"benchmarks/#running-benchmarks-locally","text":"Below are the existing benchmarks shown with the actual commands on how to run them locally.","title":"Running Benchmarks locally"},{"location":"benchmarks/#icebergsourcenestedlistparquetdatawritebenchmark","text":"A benchmark that evaluates the performance of writing nested Parquet data using Iceberg and the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=IcebergSourceNestedListParquetDataWriteBenchmark -PjmhOutputPath=benchmark/iceberg-source-nested-list-parquet-data-write-benchmark-result.txt","title":"IcebergSourceNestedListParquetDataWriteBenchmark"},{"location":"benchmarks/#sparkparquetreadersnesteddatabenchmark","text":"A benchmark that evaluates the performance of reading nested Parquet data using Iceberg and Spark Parquet readers. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=SparkParquetReadersNestedDataBenchmark -PjmhOutputPath=benchmark/spark-parquet-readers-nested-data-benchmark-result.txt","title":"SparkParquetReadersNestedDataBenchmark"},{"location":"benchmarks/#sparkparquetwritersflatdatabenchmark","text":"A benchmark that evaluates the performance of writing Parquet data with a flat schema using Iceberg and Spark Parquet writers. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=SparkParquetWritersFlatDataBenchmark -PjmhOutputPath=benchmark/spark-parquet-writers-flat-data-benchmark-result.txt","title":"SparkParquetWritersFlatDataBenchmark"},{"location":"benchmarks/#icebergsourceflatorcdatareadbenchmark","text":"A benchmark that evaluates the performance of reading ORC data with a flat schema using Iceberg and the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=IcebergSourceFlatORCDataReadBenchmark -PjmhOutputPath=benchmark/iceberg-source-flat-orc-data-read-benchmark-result.txt","title":"IcebergSourceFlatORCDataReadBenchmark"},{"location":"benchmarks/#sparkparquetreadersflatdatabenchmark","text":"A benchmark that evaluates the performance of reading Parquet data with a flat schema using Iceberg and Spark Parquet readers. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=SparkParquetReadersFlatDataBenchmark -PjmhOutputPath=benchmark/spark-parquet-readers-flat-data-benchmark-result.txt","title":"SparkParquetReadersFlatDataBenchmark"},{"location":"benchmarks/#vectorizedreaddictionaryencodedflatparquetdatabenchmark","text":"A benchmark to compare performance of reading Parquet dictionary encoded data with a flat schema using vectorized Iceberg read path and the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=VectorizedReadDictionaryEncodedFlatParquetDataBenchmark -PjmhOutputPath=benchmark/vectorized-read-dict-encoded-flat-parquet-data-result.txt","title":"VectorizedReadDictionaryEncodedFlatParquetDataBenchmark"},{"location":"benchmarks/#icebergsourcenestedlistorcdatawritebenchmark","text":"A benchmark that evaluates the performance of writing nested Parquet data using Iceberg and the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=IcebergSourceNestedListORCDataWriteBenchmark -PjmhOutputPath=benchmark/iceberg-source-nested-list-orc-data-write-benchmark-result.txt","title":"IcebergSourceNestedListORCDataWriteBenchmark"},{"location":"benchmarks/#vectorizedreadflatparquetdatabenchmark","text":"A benchmark to compare performance of reading Parquet data with a flat schema using vectorized Iceberg read path and the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=VectorizedReadFlatParquetDataBenchmark -PjmhOutputPath=benchmark/vectorized-read-flat-parquet-data-result.txt","title":"VectorizedReadFlatParquetDataBenchmark"},{"location":"benchmarks/#icebergsourceflatparquetdatawritebenchmark","text":"A benchmark that evaluates the performance of writing Parquet data with a flat schema using Iceberg and the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=IcebergSourceFlatParquetDataWriteBenchmark -PjmhOutputPath=benchmark/iceberg-source-flat-parquet-data-write-benchmark-result.txt","title":"IcebergSourceFlatParquetDataWriteBenchmark"},{"location":"benchmarks/#icebergsourcenestedavrodatareadbenchmark","text":"A benchmark that evaluates the performance of reading Avro data with a flat schema using Iceberg and the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=IcebergSourceNestedAvroDataReadBenchmark -PjmhOutputPath=benchmark/iceberg-source-nested-avro-data-read-benchmark-result.txt","title":"IcebergSourceNestedAvroDataReadBenchmark"},{"location":"benchmarks/#icebergsourceflatavrodatareadbenchmark","text":"A benchmark that evaluates the performance of reading Avro data with a flat schema using Iceberg and the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=IcebergSourceFlatAvroDataReadBenchmark -PjmhOutputPath=benchmark/iceberg-source-flat-avro-data-read-benchmark-result.txt","title":"IcebergSourceFlatAvroDataReadBenchmark"},{"location":"benchmarks/#icebergsourcenestedparquetdatawritebenchmark","text":"A benchmark that evaluates the performance of writing nested Parquet data using Iceberg and the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=IcebergSourceNestedParquetDataWriteBenchmark -PjmhOutputPath=benchmark/iceberg-source-nested-parquet-data-write-benchmark-result.txt","title":"IcebergSourceNestedParquetDataWriteBenchmark"},{"location":"benchmarks/#icebergsourcenestedparquetdatareadbenchmark","text":"A benchmark that evaluates the performance of reading nested Parquet data using Iceberg and the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=IcebergSourceNestedParquetDataReadBenchmark -PjmhOutputPath=benchmark/iceberg-source-nested-parquet-data-read-benchmark-result.txt","title":"IcebergSourceNestedParquetDataReadBenchmark"},{"location":"benchmarks/#icebergsourcenestedorcdatareadbenchmark","text":"A benchmark that evaluates the performance of reading ORC data with a flat schema using Iceberg and the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=IcebergSourceNestedORCDataReadBenchmark -PjmhOutputPath=benchmark/iceberg-source-nested-orc-data-read-benchmark-result.txt","title":"IcebergSourceNestedORCDataReadBenchmark"},{"location":"benchmarks/#icebergsourceflatparquetdatareadbenchmark","text":"A benchmark that evaluates the performance of reading Parquet data with a flat schema using Iceberg and the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=IcebergSourceFlatParquetDataReadBenchmark -PjmhOutputPath=benchmark/iceberg-source-flat-parquet-data-read-benchmark-result.txt","title":"IcebergSourceFlatParquetDataReadBenchmark"},{"location":"benchmarks/#icebergsourceflatparquetdatafilterbenchmark","text":"A benchmark that evaluates the file skipping capabilities in the Spark data source for Iceberg. This class uses a dataset with a flat schema, where the records are clustered according to the column used in the filter predicate. The performance is compared to the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=IcebergSourceFlatParquetDataFilterBenchmark -PjmhOutputPath=benchmark/iceberg-source-flat-parquet-data-filter-benchmark-result.txt","title":"IcebergSourceFlatParquetDataFilterBenchmark"},{"location":"benchmarks/#icebergsourcenestedparquetdatafilterbenchmark","text":"A benchmark that evaluates the file skipping capabilities in the Spark data source for Iceberg. This class uses a dataset with nested data, where the records are clustered according to the column used in the filter predicate. The performance is compared to the built-in file source in Spark. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=IcebergSourceNestedParquetDataFilterBenchmark -PjmhOutputPath=benchmark/iceberg-source-nested-parquet-data-filter-benchmark-result.txt","title":"IcebergSourceNestedParquetDataFilterBenchmark"},{"location":"benchmarks/#sparkparquetwritersnesteddatabenchmark","text":"A benchmark that evaluates the performance of writing nested Parquet data using Iceberg and Spark Parquet writers. To run this benchmark for either spark-2 or spark-3: ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh -PjmhIncludeRegex=SparkParquetWritersNestedDataBenchmark -PjmhOutputPath=benchmark/spark-parquet-writers-nested-data-benchmark-result.txt","title":"SparkParquetWritersNestedDataBenchmark"},{"location":"blogs/","text":"Iceberg Blogs \u00b6 Here is a list of company blogs that talk about Iceberg. The blogs are ordered from most recent to oldest. Metadata Indexing in Iceberg \u00b6 Date : 10 October 2021, Company : Tabular Author : Ryan Blue Using Debezium to Create a Data Lake with Apache Iceberg \u00b6 Date : October 20th, 2021, Company : Memiiso Community Author : Ismail Simsek How to Analyze CDC Data in Iceberg Data Lake Using Flink \u00b6 Date : June 15, 2021, Company : Alibaba Cloud Community Author : Li Jinsong , Hu Zheng , Yang Weihai , Peidan Li Apache Iceberg: An Architectural Look Under the Covers \u00b6 Date : July 6th, 2021, Company : Dremio Author : Jason Hughes Migrating to Apache Iceberg at Adobe Experience Platform \u00b6 Date : Jun 17th, 2021, Company : Adobe Author : Romin Parekh , Miao Wang , Shone Sadler Flink + Iceberg: How to Construct a Whole-scenario Real-time Data Warehouse \u00b6 Date : Jun 8th, 2021, Company : Tencent Author Shu (Simon Su) Su Trino on Ice III: Iceberg Concurrency Model, Snapshots, and the Iceberg Spec \u00b6 Date : May 25th, 2021, Company : Starburst Author : Brian Olsen Trino on Ice II: In-Place Table Evolution and Cloud Compatibility with Iceberg \u00b6 Date : May 11th, 2021, Company : Starburst Author : Brian Olsen Trino On Ice I: A Gentle Introduction To Iceberg \u00b6 Date : Apr 27th, 2021, Company : Starburst Author : Brian Olsen Apache Iceberg: A Different Table Design for Big Data \u00b6 Date : Feb 1st, 2021, Company : thenewstack.io Author : Susan Hall A Short Introduction to Apache Iceberg \u00b6 Date : Jan 26th, 2021, Company : Expedia Author : Christine Mathiesen Taking Query Optimizations to the Next Level with Iceberg \u00b6 Date : Jan 14th, 2021, Company : Adobe Author : Gautam Kowshik , Xabriel J. Collazo Mojica FastIngest: Low-latency Gobblin with Apache Iceberg and ORC format \u00b6 Date : Jan 6th, 2021, Company : Linkedin Author : Zihan Li , Sudarshan Vasudevan , Lei Sun , Shirshanka Das High Throughput Ingestion with Iceberg \u00b6 Date : Dec 22nd, 2020, Company : Adobe Author : Andrei Ionescu , Shone Sadler , Anil Malkani Optimizing data warehouse storage \u00b6 Date : Dec 21st, 2020, Company : Netflix Author : Anupom Syam Iceberg at Adobe \u00b6 Date : Dec 3rd, 2020, Company : Adobe Author : Shone Sadler , Romin Parekh , Anil Malkani Bulldozer: Batch Data Moving from Data Warehouse to Online Key-Value Stores \u00b6 Date : Oct 27th, 2020, Company : Netflix Author : Tianlong Chen , Ioannis Papapanagiotou","title":"Blogs"},{"location":"blogs/#iceberg-blogs","text":"Here is a list of company blogs that talk about Iceberg. The blogs are ordered from most recent to oldest.","title":"Iceberg Blogs"},{"location":"blogs/#metadata-indexing-in-iceberg","text":"Date : 10 October 2021, Company : Tabular Author : Ryan Blue","title":"Metadata Indexing in Iceberg"},{"location":"blogs/#using-debezium-to-create-a-data-lake-with-apache-iceberg","text":"Date : October 20th, 2021, Company : Memiiso Community Author : Ismail Simsek","title":"Using Debezium to Create a Data Lake with Apache Iceberg"},{"location":"blogs/#how-to-analyze-cdc-data-in-iceberg-data-lake-using-flink","text":"Date : June 15, 2021, Company : Alibaba Cloud Community Author : Li Jinsong , Hu Zheng , Yang Weihai , Peidan Li","title":"How to Analyze CDC Data in Iceberg Data Lake Using Flink"},{"location":"blogs/#apache-iceberg-an-architectural-look-under-the-covers","text":"Date : July 6th, 2021, Company : Dremio Author : Jason Hughes","title":"Apache Iceberg: An Architectural Look Under the Covers"},{"location":"blogs/#migrating-to-apache-iceberg-at-adobe-experience-platform","text":"Date : Jun 17th, 2021, Company : Adobe Author : Romin Parekh , Miao Wang , Shone Sadler","title":"Migrating to Apache Iceberg at Adobe Experience Platform"},{"location":"blogs/#flink-iceberg-how-to-construct-a-whole-scenario-real-time-data-warehouse","text":"Date : Jun 8th, 2021, Company : Tencent Author Shu (Simon Su) Su","title":"Flink + Iceberg: How to Construct a Whole-scenario Real-time Data Warehouse"},{"location":"blogs/#trino-on-ice-iii-iceberg-concurrency-model-snapshots-and-the-iceberg-spec","text":"Date : May 25th, 2021, Company : Starburst Author : Brian Olsen","title":"Trino on Ice III: Iceberg Concurrency Model, Snapshots, and the Iceberg Spec"},{"location":"blogs/#trino-on-ice-ii-in-place-table-evolution-and-cloud-compatibility-with-iceberg","text":"Date : May 11th, 2021, Company : Starburst Author : Brian Olsen","title":"Trino on Ice II: In-Place Table Evolution and Cloud Compatibility with Iceberg"},{"location":"blogs/#trino-on-ice-i-a-gentle-introduction-to-iceberg","text":"Date : Apr 27th, 2021, Company : Starburst Author : Brian Olsen","title":"Trino On Ice I: A Gentle Introduction To Iceberg"},{"location":"blogs/#apache-iceberg-a-different-table-design-for-big-data","text":"Date : Feb 1st, 2021, Company : thenewstack.io Author : Susan Hall","title":"Apache Iceberg: A Different Table Design for Big Data"},{"location":"blogs/#a-short-introduction-to-apache-iceberg","text":"Date : Jan 26th, 2021, Company : Expedia Author : Christine Mathiesen","title":"A Short Introduction to Apache Iceberg"},{"location":"blogs/#taking-query-optimizations-to-the-next-level-with-iceberg","text":"Date : Jan 14th, 2021, Company : Adobe Author : Gautam Kowshik , Xabriel J. Collazo Mojica","title":"Taking Query Optimizations to the Next Level with Iceberg"},{"location":"blogs/#fastingest-low-latency-gobblin-with-apache-iceberg-and-orc-format","text":"Date : Jan 6th, 2021, Company : Linkedin Author : Zihan Li , Sudarshan Vasudevan , Lei Sun , Shirshanka Das","title":"FastIngest: Low-latency Gobblin with Apache Iceberg and ORC format"},{"location":"blogs/#high-throughput-ingestion-with-iceberg","text":"Date : Dec 22nd, 2020, Company : Adobe Author : Andrei Ionescu , Shone Sadler , Anil Malkani","title":"High Throughput Ingestion with Iceberg"},{"location":"blogs/#optimizing-data-warehouse-storage","text":"Date : Dec 21st, 2020, Company : Netflix Author : Anupom Syam","title":"Optimizing data warehouse storage"},{"location":"blogs/#iceberg-at-adobe","text":"Date : Dec 3rd, 2020, Company : Adobe Author : Shone Sadler , Romin Parekh , Anil Malkani","title":"Iceberg at Adobe"},{"location":"blogs/#bulldozer-batch-data-moving-from-data-warehouse-to-online-key-value-stores","text":"Date : Oct 27th, 2020, Company : Netflix Author : Tianlong Chen , Ioannis Papapanagiotou","title":"Bulldozer: Batch Data Moving from Data Warehouse to Online Key-Value Stores"},{"location":"community/","text":"Welcome! \u00b6 Apache Iceberg tracks issues in GitHub and prefers to receive contributions as pull requests. Community discussions happen primarily on the dev mailing list, on apache-iceberg Slack workspace, and on specific GitHub issues. Contributing \u00b6 The Iceberg Project is hosted on Github at https://github.com/apache/iceberg . The Iceberg community prefers to receive contributions as Github pull requests . View open pull requests Learn about pull requests Issues \u00b6 Issues are tracked in GitHub: View open issues Open a new issue Slack \u00b6 We use the Apache Iceberg workspace on Slack. To be invited, follow this invite link . Please note that this link may occasionally break when Slack does an upgrade. If you encounter problems using it, please let us know by sending an email to dev@iceberg.apache.org . Mailing Lists \u00b6 Iceberg has four mailing lists: Developers : dev@iceberg.apache.org \u2013 used for community discussions Subscribe Unsubscribe Archive Commits : commits@iceberg.apache.org \u2013 distributes commit notifications Subscribe Unsubscribe Archive Issues : issues@iceberg.apache.org \u2013 Github issue tracking Subscribe Unsubscribe Archive Private : private@iceberg.apache.org \u2013 private list for the PMC to discuss sensitive issues related to the health of the project Archive Setting up IDE and Code Style \u00b6 Configuring Code Formatter for IntelliJ IDEA \u00b6 In the Settings/Preferences dialog go to Editor > Code Style > Java . Click on the gear wheel and select Import Scheme to import IntelliJ IDEA XML code style settings. Point to intellij-java-palantir-style.xml and hit OK (you might need to enable Show Hidden Files and Directories in the dialog). The code itself can then be formatted via Code > Reformat Code . See also the IntelliJ Code Style docs and Reformat Code docs for additional details. Running Benchmarks \u00b6 Some PRs/changesets might require running benchmarks to determine whether they are affecting the baseline performance. Currently there is no \u201cpush a single button to get a performance comparison\u201d solution available, therefore one has to run JMH performance tests on their local machine and post the results on the PR. See Benchmarks for a summary of available benchmarks and how to run them.","title":"Community"},{"location":"community/#welcome","text":"Apache Iceberg tracks issues in GitHub and prefers to receive contributions as pull requests. Community discussions happen primarily on the dev mailing list, on apache-iceberg Slack workspace, and on specific GitHub issues.","title":"Welcome!"},{"location":"community/#contributing","text":"The Iceberg Project is hosted on Github at https://github.com/apache/iceberg . The Iceberg community prefers to receive contributions as Github pull requests . View open pull requests Learn about pull requests","title":"Contributing"},{"location":"community/#issues","text":"Issues are tracked in GitHub: View open issues Open a new issue","title":"Issues"},{"location":"community/#slack","text":"We use the Apache Iceberg workspace on Slack. To be invited, follow this invite link . Please note that this link may occasionally break when Slack does an upgrade. If you encounter problems using it, please let us know by sending an email to dev@iceberg.apache.org .","title":"Slack"},{"location":"community/#mailing-lists","text":"Iceberg has four mailing lists: Developers : dev@iceberg.apache.org \u2013 used for community discussions Subscribe Unsubscribe Archive Commits : commits@iceberg.apache.org \u2013 distributes commit notifications Subscribe Unsubscribe Archive Issues : issues@iceberg.apache.org \u2013 Github issue tracking Subscribe Unsubscribe Archive Private : private@iceberg.apache.org \u2013 private list for the PMC to discuss sensitive issues related to the health of the project Archive","title":"Mailing Lists"},{"location":"community/#setting-up-ide-and-code-style","text":"","title":"Setting up IDE and Code Style"},{"location":"community/#configuring-code-formatter-for-intellij-idea","text":"In the Settings/Preferences dialog go to Editor > Code Style > Java . Click on the gear wheel and select Import Scheme to import IntelliJ IDEA XML code style settings. Point to intellij-java-palantir-style.xml and hit OK (you might need to enable Show Hidden Files and Directories in the dialog). The code itself can then be formatted via Code > Reformat Code . See also the IntelliJ Code Style docs and Reformat Code docs for additional details.","title":"Configuring Code Formatter for IntelliJ IDEA"},{"location":"community/#running-benchmarks","text":"Some PRs/changesets might require running benchmarks to determine whether they are affecting the baseline performance. Currently there is no \u201cpush a single button to get a performance comparison\u201d solution available, therefore one has to run JMH performance tests on their local machine and post the results on the PR. See Benchmarks for a summary of available benchmarks and how to run them.","title":"Running Benchmarks"},{"location":"configuration/","text":"Configuration \u00b6 Table properties \u00b6 Iceberg tables support table properties to configure table behavior, like the default split size for readers. Read properties \u00b6 Property Default Description read.split.target-size 134217728 (128 MB) Target size when combining data input splits read.split.metadata-target-size 33554432 (32 MB) Target size when combining metadata input splits read.split.planning-lookback 10 Number of bins to consider when combining input splits read.split.open-file-cost 4194304 (4 MB) The estimated cost to open a file, used as a minimum weight when combining splits. Write properties \u00b6 Property Default Description write.format.default parquet Default file format for the table; parquet, avro, or orc write.parquet.row-group-size-bytes 134217728 (128 MB) Parquet row group size write.parquet.page-size-bytes 1048576 (1 MB) Parquet page size write.parquet.dict-size-bytes 2097152 (2 MB) Parquet dictionary page size write.parquet.compression-codec gzip Parquet compression codec write.parquet.compression-level null Parquet compression level write.avro.compression-codec gzip Avro compression codec write.location-provider.impl null Optional custom implemention for LocationProvider write.metadata.compression-codec none Metadata compression codec; none or gzip write.metadata.metrics.default truncate(16) Default metrics mode for all columns in the table; none, counts, truncate(length), or full write.metadata.metrics.column.col1 (not set) Metrics mode for column \u2018col1\u2019 to allow per-column tuning; none, counts, truncate(length), or full write.target-file-size-bytes 536870912 (512 MB) Controls the size of files generated to target about this many bytes write.distribution-mode none Defines distribution of write data: none : don\u2019t shuffle rows; hash : hash distribute by partition key ; range : range distribute by partition key or sort key if table has an SortOrder write.wap.enabled false Enables write-audit-publish writes write.summary.partition-limit 0 Includes partition-level summary stats in snapshot summaries if the changed partition count is less than this limit write.metadata.delete-after-commit.enabled false Controls whether to delete the oldest version metadata files after commit write.metadata.previous-versions-max 100 The max number of previous version metadata files to keep before deleting after commit write.spark.fanout.enabled false Enables Partitioned-Fanout-Writer writes in Spark Table behavior properties \u00b6 Property Default Description commit.retry.num-retries 4 Number of times to retry a commit before failing commit.retry.min-wait-ms 100 Minimum time in milliseconds to wait before retrying a commit commit.retry.max-wait-ms 60000 (1 min) Maximum time in milliseconds to wait before retrying a commit commit.retry.total-timeout-ms 1800000 (30 min) Maximum time in milliseconds to wait before retrying a commit commit.status-check.num-retries 3 Number of times to check whether a commit succeeded after a connection is lost before failing due to an unknown commit state commit.status-check.min-wait-ms 1000 (1s) Minimum time in milliseconds to wait before retrying a status-check commit.status-check.max-wait-ms 60000 (1 min) Maximum time in milliseconds to wait before retrying a status-check commit.status-check.total-timeout-ms 1800000 (30 min) Maximum time in milliseconds to wait before retrying a status-check commit.manifest.target-size-bytes 8388608 (8 MB) Target size when merging manifest files commit.manifest.min-count-to-merge 100 Minimum number of manifests to accumulate before merging commit.manifest-merge.enabled true Controls whether to automatically merge manifests on writes history.expire.max-snapshot-age-ms 432000000 (5 days) Default max age of snapshots to keep while expiring snapshots history.expire.min-snapshots-to-keep 1 Default min number of snapshots to keep while expiring snapshots Compatibility flags \u00b6 Property Default Description compatibility.snapshot-id-inheritance.enabled false Enables committing snapshots without explicit snapshot IDs Catalog properties \u00b6 Iceberg catalogs support using catalog properties to configure catalog behaviors. Here is a list of commonly used catalog properties: Property Default Description catalog-impl null a custom Catalog implementation to use by an engine io-impl null a custom FileIO implementation to use in a catalog warehouse null the root path of the data warehouse uri null a URI string, such as Hive metastore URI clients 2 client pool size HadoopCatalog and HiveCatalog can access the properties in their constructors. Any other custom catalog can access the properties by implementing Catalog.initialize(catalogName, catalogProperties) . The properties can be manually constructed or passed in from a compute engine like Spark or Flink. Spark uses its session properties as catalog properties, see more details in the Spark configuration section. Flink passes in catalog properties through CREATE CATALOG statement, see more details in the Flink section. Lock catalog properties \u00b6 Here are the catalog properties related to locking. They are used by some catalog implementations to control the locking behavior during commits. Property Default Description lock-impl null a custom implementation of the lock manager, the actual interface depends on the catalog used lock.table null an auxiliary table for locking, such as in AWS DynamoDB lock manager lock.acquire-interval-ms 5 seconds the interval to wait between each attempt to acquire a lock lock.acquire-timeout-ms 3 minutes the maximum time to try acquiring a lock lock.heartbeat-interval-ms 3 seconds the interval to wait between each heartbeat after acquiring a lock lock.heartbeat-timeout-ms 15 seconds the maximum time without a heartbeat to consider a lock expired Hadoop configuration \u00b6 The following properties from the Hadoop configuration are used by the Hive Metastore connector. Property Default Description iceberg.hive.client-pool-size 5 The size of the Hive client pool when tracking tables in HMS iceberg.hive.lock-timeout-ms 180000 (3 min) Maximum time in milliseconds to acquire a lock iceberg.hive.lock-check-min-wait-ms 50 Minimum time in milliseconds to check back on the status of lock acquisition iceberg.hive.lock-check-max-wait-ms 5000 Maximum time in milliseconds to check back on the status of lock acquisition Note: iceberg.hive.lock-check-max-wait-ms should be less than the transaction timeout of the Hive Metastore ( hive.txn.timeout or metastore.txn.timeout in the newer versions). Otherwise, the heartbeats on the lock (which happens during the lock checks) would end up expiring in the Hive Metastore before the lock is retried from Iceberg.","title":"Configuration"},{"location":"configuration/#configuration","text":"","title":"Configuration"},{"location":"configuration/#table-properties","text":"Iceberg tables support table properties to configure table behavior, like the default split size for readers.","title":"Table properties"},{"location":"configuration/#read-properties","text":"Property Default Description read.split.target-size 134217728 (128 MB) Target size when combining data input splits read.split.metadata-target-size 33554432 (32 MB) Target size when combining metadata input splits read.split.planning-lookback 10 Number of bins to consider when combining input splits read.split.open-file-cost 4194304 (4 MB) The estimated cost to open a file, used as a minimum weight when combining splits.","title":"Read properties"},{"location":"configuration/#write-properties","text":"Property Default Description write.format.default parquet Default file format for the table; parquet, avro, or orc write.parquet.row-group-size-bytes 134217728 (128 MB) Parquet row group size write.parquet.page-size-bytes 1048576 (1 MB) Parquet page size write.parquet.dict-size-bytes 2097152 (2 MB) Parquet dictionary page size write.parquet.compression-codec gzip Parquet compression codec write.parquet.compression-level null Parquet compression level write.avro.compression-codec gzip Avro compression codec write.location-provider.impl null Optional custom implemention for LocationProvider write.metadata.compression-codec none Metadata compression codec; none or gzip write.metadata.metrics.default truncate(16) Default metrics mode for all columns in the table; none, counts, truncate(length), or full write.metadata.metrics.column.col1 (not set) Metrics mode for column \u2018col1\u2019 to allow per-column tuning; none, counts, truncate(length), or full write.target-file-size-bytes 536870912 (512 MB) Controls the size of files generated to target about this many bytes write.distribution-mode none Defines distribution of write data: none : don\u2019t shuffle rows; hash : hash distribute by partition key ; range : range distribute by partition key or sort key if table has an SortOrder write.wap.enabled false Enables write-audit-publish writes write.summary.partition-limit 0 Includes partition-level summary stats in snapshot summaries if the changed partition count is less than this limit write.metadata.delete-after-commit.enabled false Controls whether to delete the oldest version metadata files after commit write.metadata.previous-versions-max 100 The max number of previous version metadata files to keep before deleting after commit write.spark.fanout.enabled false Enables Partitioned-Fanout-Writer writes in Spark","title":"Write properties"},{"location":"configuration/#table-behavior-properties","text":"Property Default Description commit.retry.num-retries 4 Number of times to retry a commit before failing commit.retry.min-wait-ms 100 Minimum time in milliseconds to wait before retrying a commit commit.retry.max-wait-ms 60000 (1 min) Maximum time in milliseconds to wait before retrying a commit commit.retry.total-timeout-ms 1800000 (30 min) Maximum time in milliseconds to wait before retrying a commit commit.status-check.num-retries 3 Number of times to check whether a commit succeeded after a connection is lost before failing due to an unknown commit state commit.status-check.min-wait-ms 1000 (1s) Minimum time in milliseconds to wait before retrying a status-check commit.status-check.max-wait-ms 60000 (1 min) Maximum time in milliseconds to wait before retrying a status-check commit.status-check.total-timeout-ms 1800000 (30 min) Maximum time in milliseconds to wait before retrying a status-check commit.manifest.target-size-bytes 8388608 (8 MB) Target size when merging manifest files commit.manifest.min-count-to-merge 100 Minimum number of manifests to accumulate before merging commit.manifest-merge.enabled true Controls whether to automatically merge manifests on writes history.expire.max-snapshot-age-ms 432000000 (5 days) Default max age of snapshots to keep while expiring snapshots history.expire.min-snapshots-to-keep 1 Default min number of snapshots to keep while expiring snapshots","title":"Table behavior properties"},{"location":"configuration/#compatibility-flags","text":"Property Default Description compatibility.snapshot-id-inheritance.enabled false Enables committing snapshots without explicit snapshot IDs","title":"Compatibility flags"},{"location":"configuration/#catalog-properties","text":"Iceberg catalogs support using catalog properties to configure catalog behaviors. Here is a list of commonly used catalog properties: Property Default Description catalog-impl null a custom Catalog implementation to use by an engine io-impl null a custom FileIO implementation to use in a catalog warehouse null the root path of the data warehouse uri null a URI string, such as Hive metastore URI clients 2 client pool size HadoopCatalog and HiveCatalog can access the properties in their constructors. Any other custom catalog can access the properties by implementing Catalog.initialize(catalogName, catalogProperties) . The properties can be manually constructed or passed in from a compute engine like Spark or Flink. Spark uses its session properties as catalog properties, see more details in the Spark configuration section. Flink passes in catalog properties through CREATE CATALOG statement, see more details in the Flink section.","title":"Catalog properties"},{"location":"configuration/#lock-catalog-properties","text":"Here are the catalog properties related to locking. They are used by some catalog implementations to control the locking behavior during commits. Property Default Description lock-impl null a custom implementation of the lock manager, the actual interface depends on the catalog used lock.table null an auxiliary table for locking, such as in AWS DynamoDB lock manager lock.acquire-interval-ms 5 seconds the interval to wait between each attempt to acquire a lock lock.acquire-timeout-ms 3 minutes the maximum time to try acquiring a lock lock.heartbeat-interval-ms 3 seconds the interval to wait between each heartbeat after acquiring a lock lock.heartbeat-timeout-ms 15 seconds the maximum time without a heartbeat to consider a lock expired","title":"Lock catalog properties"},{"location":"configuration/#hadoop-configuration","text":"The following properties from the Hadoop configuration are used by the Hive Metastore connector. Property Default Description iceberg.hive.client-pool-size 5 The size of the Hive client pool when tracking tables in HMS iceberg.hive.lock-timeout-ms 180000 (3 min) Maximum time in milliseconds to acquire a lock iceberg.hive.lock-check-min-wait-ms 50 Minimum time in milliseconds to check back on the status of lock acquisition iceberg.hive.lock-check-max-wait-ms 5000 Maximum time in milliseconds to check back on the status of lock acquisition Note: iceberg.hive.lock-check-max-wait-ms should be less than the transaction timeout of the Hive Metastore ( hive.txn.timeout or metastore.txn.timeout in the newer versions). Otherwise, the heartbeats on the lock (which happens during the lock checks) would end up expiring in the Hive Metastore before the lock is retried from Iceberg.","title":"Hadoop configuration"},{"location":"custom-catalog/","text":"Custom Catalog Implementation \u00b6 It\u2019s possible to read an iceberg table either from an hdfs path or from a hive table. It\u2019s also possible to use a custom metastore in place of hive. The steps to do that are as follows. Custom TableOperations Custom Catalog Custom FileIO Custom LocationProvider Custom IcebergSource Custom table operations implementation \u00b6 Extend BaseMetastoreTableOperations to provide implementation on how to read and write metadata Example: class CustomTableOperations extends BaseMetastoreTableOperations { private String dbName; private String tableName; private Configuration conf; private FileIO fileIO; protected CustomTableOperations(Configuration conf, String dbName, String tableName) { this.conf = conf; this.dbName = dbName; this.tableName = tableName; } // The doRefresh method should provide implementation on how to get the metadata location @Override public void doRefresh() { // Example custom service which returns the metadata location given a dbName and tableName String metadataLocation = CustomService.getMetadataForTable(conf, dbName, tableName); // When updating from a metadata file location, call the helper method refreshFromMetadataLocation(metadataLocation); } // The doCommit method should provide implementation on how to update with metadata location atomically @Override public void doCommit(TableMetadata base, TableMetadata metadata) { String oldMetadataLocation = base.location(); // Write new metadata using helper method String newMetadataLocation = writeNewMetadata(metadata, currentVersion() + 1); // Example custom service which updates the metadata location for the given db and table atomically CustomService.updateMetadataLocation(dbName, tableName, oldMetadataLocation, newMetadataLocation); } // The io method provides a FileIO which is used to read and write the table metadata files @Override public FileIO io() { if (fileIO == null) { fileIO = new HadoopFileIO(conf); } return fileIO; } } A TableOperations instance is usually obtained by calling Catalog.newTableOps(TableIdentifier) . See the next section about implementing and loading a custom catalog. Custom catalog implementation \u00b6 Extend BaseMetastoreCatalog to provide default warehouse locations and instantiate CustomTableOperations Example: public class CustomCatalog extends BaseMetastoreCatalog { private Configuration configuration; // must have a no-arg constructor to be dynamically loaded // initialize(String name, Map<String, String> properties) will be called to complete initialization public CustomCatalog() { } public CustomCatalog(Configuration configuration) { this.configuration = configuration; } @Override protected TableOperations newTableOps(TableIdentifier tableIdentifier) { String dbName = tableIdentifier.namespace().level(0); String tableName = tableIdentifier.name(); // instantiate the CustomTableOperations return new CustomTableOperations(configuration, dbName, tableName); } @Override protected String defaultWarehouseLocation(TableIdentifier tableIdentifier) { // Can choose to use any other configuration name String tableLocation = configuration.get(\"custom.iceberg.warehouse.location\"); // Can be an s3 or hdfs path if (tableLocation == null) { throw new RuntimeException(\"custom.iceberg.warehouse.location configuration not set!\"); } return String.format( \"%s/%s.db/%s\", tableLocation, tableIdentifier.namespace().levels()[0], tableIdentifier.name()); } @Override public boolean dropTable(TableIdentifier identifier, boolean purge) { // Example service to delete table CustomService.deleteTable(identifier.namepsace().level(0), identifier.name()); } @Override public void renameTable(TableIdentifier from, TableIdentifier to) { Preconditions.checkArgument(from.namespace().level(0).equals(to.namespace().level(0)), \"Cannot move table between databases\"); // Example service to rename table CustomService.renameTable(from.namepsace().level(0), from.name(), to.name()); } // implement this method to read catalog name and properties during initialization public void initialize(String name, Map<String, String> properties) { } } Catalog implementations can be dynamically loaded in most compute engines. For Spark and Flink, you can specify the catalog-impl catalog property to load it. Read the Configuration section for more details. For MapReduce, implement org.apache.iceberg.mr.CatalogLoader and set Hadoop property iceberg.mr.catalog.loader.class to load it. If your catalog must read Hadoop configuration to access certain environment properties, make your catalog implement org.apache.hadoop.conf.Configurable . Custom file IO implementation \u00b6 Extend FileIO and provide implementation to read and write data files Example: public class CustomFileIO implements FileIO { // must have a no-arg constructor to be dynamically loaded // initialize(Map<String, String> properties) will be called to complete initialization public CustomFileIO() { } @Override public InputFile newInputFile(String s) { // you also need to implement the InputFile interface for a custom input file return new CustomInputFile(s); } @Override public OutputFile newOutputFile(String s) { // you also need to implement the OutputFile interface for a custom output file return new CustomOutputFile(s); } @Override public void deleteFile(String path) { Path toDelete = new Path(path); FileSystem fs = Util.getFs(toDelete); try { fs.delete(toDelete, false /* not recursive */); } catch (IOException e) { throw new RuntimeIOException(e, \"Failed to delete file: %s\", path); } } // implement this method to read catalog properties during initialization public void initialize(Map<String, String> properties) { } } If you are already implementing your own catalog, you can implement TableOperations.io() to use your custom FileIO . In addition, custom FileIO implementations can also be dynamically loaded in HadoopCatalog and HiveCatalog by specifying the io-impl catalog property. Read the Configuration section for more details. If your FileIO must read Hadoop configuration to access certain environment properties, make your FileIO implement org.apache.hadoop.conf.Configurable . Custom location provider implementation \u00b6 Extend LocationProvider and provide implementation to determine the file path to write data Example: public class CustomLocationProvider implements LocationProvider { private String tableLocation; // must have a 2-arg constructor like this, or a no-arg constructor public CustomLocationProvider(String tableLocation, Map<String, String> properties) { this.tableLocation = tableLocation; } @Override public String newDataLocation(String filename) { // can use any custom method to generate a file path given a file name return String.format(\"%s/%s/%s\", tableLocation, UUID.randomUUID().toString(), filename); } @Override public String newDataLocation(PartitionSpec spec, StructLike partitionData, String filename) { // can use any custom method to generate a file path given a partition info and file name return newDataLocation(filename); } } If you are already implementing your own catalog, you can override TableOperations.locationProvider() to use your custom default LocationProvider . To use a different custom location provider for a specific table, specify the implementation when creating the table using table property write.location-provider.impl Example: CREATE TABLE hive.default.my_table ( id bigint, data string, category string) USING iceberg OPTIONS ( 'write.location-provider.impl'='com.my.CustomLocationProvider' ) PARTITIONED BY (category); Custom IcebergSource \u00b6 Extend IcebergSource and provide implementation to read from CustomCatalog Example: public class CustomIcebergSource extends IcebergSource { @Override protected Table findTable(DataSourceOptions options, Configuration conf) { Optional<String> path = options.get(\"path\"); Preconditions.checkArgument(path.isPresent(), \"Cannot open table: path is not set\"); // Read table from CustomCatalog CustomCatalog catalog = new CustomCatalog(conf); TableIdentifier tableIdentifier = TableIdentifier.parse(path.get()); return catalog.loadTable(tableIdentifier); } } Register the CustomIcebergSource by updating META-INF/services/org.apache.spark.sql.sources.DataSourceRegister with its fully qualified name","title":"Java Custom Catalog"},{"location":"custom-catalog/#custom-catalog-implementation","text":"It\u2019s possible to read an iceberg table either from an hdfs path or from a hive table. It\u2019s also possible to use a custom metastore in place of hive. The steps to do that are as follows. Custom TableOperations Custom Catalog Custom FileIO Custom LocationProvider Custom IcebergSource","title":"Custom Catalog Implementation"},{"location":"custom-catalog/#custom-table-operations-implementation","text":"Extend BaseMetastoreTableOperations to provide implementation on how to read and write metadata Example: class CustomTableOperations extends BaseMetastoreTableOperations { private String dbName; private String tableName; private Configuration conf; private FileIO fileIO; protected CustomTableOperations(Configuration conf, String dbName, String tableName) { this.conf = conf; this.dbName = dbName; this.tableName = tableName; } // The doRefresh method should provide implementation on how to get the metadata location @Override public void doRefresh() { // Example custom service which returns the metadata location given a dbName and tableName String metadataLocation = CustomService.getMetadataForTable(conf, dbName, tableName); // When updating from a metadata file location, call the helper method refreshFromMetadataLocation(metadataLocation); } // The doCommit method should provide implementation on how to update with metadata location atomically @Override public void doCommit(TableMetadata base, TableMetadata metadata) { String oldMetadataLocation = base.location(); // Write new metadata using helper method String newMetadataLocation = writeNewMetadata(metadata, currentVersion() + 1); // Example custom service which updates the metadata location for the given db and table atomically CustomService.updateMetadataLocation(dbName, tableName, oldMetadataLocation, newMetadataLocation); } // The io method provides a FileIO which is used to read and write the table metadata files @Override public FileIO io() { if (fileIO == null) { fileIO = new HadoopFileIO(conf); } return fileIO; } } A TableOperations instance is usually obtained by calling Catalog.newTableOps(TableIdentifier) . See the next section about implementing and loading a custom catalog.","title":"Custom table operations implementation"},{"location":"custom-catalog/#custom-catalog-implementation_1","text":"Extend BaseMetastoreCatalog to provide default warehouse locations and instantiate CustomTableOperations Example: public class CustomCatalog extends BaseMetastoreCatalog { private Configuration configuration; // must have a no-arg constructor to be dynamically loaded // initialize(String name, Map<String, String> properties) will be called to complete initialization public CustomCatalog() { } public CustomCatalog(Configuration configuration) { this.configuration = configuration; } @Override protected TableOperations newTableOps(TableIdentifier tableIdentifier) { String dbName = tableIdentifier.namespace().level(0); String tableName = tableIdentifier.name(); // instantiate the CustomTableOperations return new CustomTableOperations(configuration, dbName, tableName); } @Override protected String defaultWarehouseLocation(TableIdentifier tableIdentifier) { // Can choose to use any other configuration name String tableLocation = configuration.get(\"custom.iceberg.warehouse.location\"); // Can be an s3 or hdfs path if (tableLocation == null) { throw new RuntimeException(\"custom.iceberg.warehouse.location configuration not set!\"); } return String.format( \"%s/%s.db/%s\", tableLocation, tableIdentifier.namespace().levels()[0], tableIdentifier.name()); } @Override public boolean dropTable(TableIdentifier identifier, boolean purge) { // Example service to delete table CustomService.deleteTable(identifier.namepsace().level(0), identifier.name()); } @Override public void renameTable(TableIdentifier from, TableIdentifier to) { Preconditions.checkArgument(from.namespace().level(0).equals(to.namespace().level(0)), \"Cannot move table between databases\"); // Example service to rename table CustomService.renameTable(from.namepsace().level(0), from.name(), to.name()); } // implement this method to read catalog name and properties during initialization public void initialize(String name, Map<String, String> properties) { } } Catalog implementations can be dynamically loaded in most compute engines. For Spark and Flink, you can specify the catalog-impl catalog property to load it. Read the Configuration section for more details. For MapReduce, implement org.apache.iceberg.mr.CatalogLoader and set Hadoop property iceberg.mr.catalog.loader.class to load it. If your catalog must read Hadoop configuration to access certain environment properties, make your catalog implement org.apache.hadoop.conf.Configurable .","title":"Custom catalog implementation"},{"location":"custom-catalog/#custom-file-io-implementation","text":"Extend FileIO and provide implementation to read and write data files Example: public class CustomFileIO implements FileIO { // must have a no-arg constructor to be dynamically loaded // initialize(Map<String, String> properties) will be called to complete initialization public CustomFileIO() { } @Override public InputFile newInputFile(String s) { // you also need to implement the InputFile interface for a custom input file return new CustomInputFile(s); } @Override public OutputFile newOutputFile(String s) { // you also need to implement the OutputFile interface for a custom output file return new CustomOutputFile(s); } @Override public void deleteFile(String path) { Path toDelete = new Path(path); FileSystem fs = Util.getFs(toDelete); try { fs.delete(toDelete, false /* not recursive */); } catch (IOException e) { throw new RuntimeIOException(e, \"Failed to delete file: %s\", path); } } // implement this method to read catalog properties during initialization public void initialize(Map<String, String> properties) { } } If you are already implementing your own catalog, you can implement TableOperations.io() to use your custom FileIO . In addition, custom FileIO implementations can also be dynamically loaded in HadoopCatalog and HiveCatalog by specifying the io-impl catalog property. Read the Configuration section for more details. If your FileIO must read Hadoop configuration to access certain environment properties, make your FileIO implement org.apache.hadoop.conf.Configurable .","title":"Custom file IO implementation"},{"location":"custom-catalog/#custom-location-provider-implementation","text":"Extend LocationProvider and provide implementation to determine the file path to write data Example: public class CustomLocationProvider implements LocationProvider { private String tableLocation; // must have a 2-arg constructor like this, or a no-arg constructor public CustomLocationProvider(String tableLocation, Map<String, String> properties) { this.tableLocation = tableLocation; } @Override public String newDataLocation(String filename) { // can use any custom method to generate a file path given a file name return String.format(\"%s/%s/%s\", tableLocation, UUID.randomUUID().toString(), filename); } @Override public String newDataLocation(PartitionSpec spec, StructLike partitionData, String filename) { // can use any custom method to generate a file path given a partition info and file name return newDataLocation(filename); } } If you are already implementing your own catalog, you can override TableOperations.locationProvider() to use your custom default LocationProvider . To use a different custom location provider for a specific table, specify the implementation when creating the table using table property write.location-provider.impl Example: CREATE TABLE hive.default.my_table ( id bigint, data string, category string) USING iceberg OPTIONS ( 'write.location-provider.impl'='com.my.CustomLocationProvider' ) PARTITIONED BY (category);","title":"Custom location provider implementation"},{"location":"custom-catalog/#custom-icebergsource","text":"Extend IcebergSource and provide implementation to read from CustomCatalog Example: public class CustomIcebergSource extends IcebergSource { @Override protected Table findTable(DataSourceOptions options, Configuration conf) { Optional<String> path = options.get(\"path\"); Preconditions.checkArgument(path.isPresent(), \"Cannot open table: path is not set\"); // Read table from CustomCatalog CustomCatalog catalog = new CustomCatalog(conf); TableIdentifier tableIdentifier = TableIdentifier.parse(path.get()); return catalog.loadTable(tableIdentifier); } } Register the CustomIcebergSource by updating META-INF/services/org.apache.spark.sql.sources.DataSourceRegister with its fully qualified name","title":"Custom IcebergSource"},{"location":"evolution/","text":"Table Evolution \u00b6 Iceberg supports in-place table evolution . You can evolve a table schema just like SQL \u2013 even in nested structures \u2013 or change partition layout when data volume changes. Iceberg does not require costly distractions, like rewriting table data or migrating to a new table. For example, Hive table partitioning cannot change so moving from a daily partition layout to an hourly partition layout requires a new table. And because queries are dependent on partitions, queries must be rewritten for the new table. In some cases, even changes as simple as renaming a column are either not supported, or can cause data correctness problems. Schema evolution \u00b6 Iceberg supports the following schema evolution changes: Add \u2013 add a new column to the table or to a nested struct Drop \u2013 remove an existing column from the table or a nested struct Rename \u2013 rename an existing column or field in a nested struct Update \u2013 widen the type of a column, struct field, map key, map value, or list element Reorder \u2013 change the order of columns or fields in a nested struct Iceberg schema updates are metadata changes , so no data files need to be rewritten to perform the update. Note that map keys do not support adding or dropping struct fields that would change equality. Correctness \u00b6 Iceberg guarantees that schema evolution changes are independent and free of side-effects , without rewriting files: Added columns never read existing values from another column. Dropping a column or field does not change the values in any other column. Updating a column or field does not change values in any other column. Changing the order of columns or fields in a struct does not change the values associated with a column or field name. Iceberg uses unique IDs to track each column in a table. When you add a column, it is assigned a new ID so existing data is never used by mistake. Formats that track columns by name can inadvertently un-delete a column if a name is reused, which violates #1. Formats that track columns by position cannot delete columns without changing the names that are used for each column, which violates #2. Partition evolution \u00b6 Iceberg table partitioning can be updated in an existing table because queries do not reference partition values directly. When you evolve a partition spec, the old data written with an earlier spec remains unchanged. New data is written using the new spec in a new layout. Metadata for each of the partition versions is kept separately. Because of this, when you start writing queries, you get split planning. This is where each partition layout plans files separately using the filter it derives for that specific partition layout. Here\u2019s a visual representation of a contrived example: The data for 2008 is partitioned by month. Starting from 2009 the table is updated so that the data is instead partitioned by day. Both partitioning layouts are able to coexist in the same table. Iceberg uses hidden partitioning , so you don\u2019t need to write queries for a specific partition layout to be fast. Instead, you can write queries that select the data you need, and Iceberg automatically prunes out files that don\u2019t contain matching data. Partition evolution is a metadata operation and does not eagerly rewrite files. Iceberg\u2019s Java table API provides updateSpec API to update partition spec. For example, the following code could be used to update the partition spec to add a new partition field that places id column values into 8 buckets and remove an existing partition field category : Table sampleTable = ...; sampleTable.updateSpec() .addField(bucket(\"id\", 8)) .removeField(\"category\") .commit(); Spark supports updating partition spec through its ALTER TABLE SQL statement, see more details in Spark SQL . Sort order evolution \u00b6 Similar to partition spec, Iceberg sort order can also be updated in an existing table. When you evolve a sort order, the old data written with an earlier order remains unchanged. Engines can always choose to write data in the latest sort order or unsorted when sorting is prohibitively expensive. Iceberg\u2019s Java table API provides replaceSortOrder API to update partition spec. For example, the following code could be used to create a new sort order with id column sorted in ascending order with nulls last, and category column sorted in descending order with nulls first: Table sampleTable = ...; sampleTable.replaceSortOrder() .asc(\"id\", NullOrder.NULLS_LAST) .dec(\"category\", NullOrder.NULL_FIRST) .commit(); Spark supports updating sort order through its ALTER TABLE SQL statement, see more details in Spark SQL .","title":"Table evolution"},{"location":"evolution/#table-evolution","text":"Iceberg supports in-place table evolution . You can evolve a table schema just like SQL \u2013 even in nested structures \u2013 or change partition layout when data volume changes. Iceberg does not require costly distractions, like rewriting table data or migrating to a new table. For example, Hive table partitioning cannot change so moving from a daily partition layout to an hourly partition layout requires a new table. And because queries are dependent on partitions, queries must be rewritten for the new table. In some cases, even changes as simple as renaming a column are either not supported, or can cause data correctness problems.","title":"Table Evolution"},{"location":"evolution/#schema-evolution","text":"Iceberg supports the following schema evolution changes: Add \u2013 add a new column to the table or to a nested struct Drop \u2013 remove an existing column from the table or a nested struct Rename \u2013 rename an existing column or field in a nested struct Update \u2013 widen the type of a column, struct field, map key, map value, or list element Reorder \u2013 change the order of columns or fields in a nested struct Iceberg schema updates are metadata changes , so no data files need to be rewritten to perform the update. Note that map keys do not support adding or dropping struct fields that would change equality.","title":"Schema evolution"},{"location":"evolution/#correctness","text":"Iceberg guarantees that schema evolution changes are independent and free of side-effects , without rewriting files: Added columns never read existing values from another column. Dropping a column or field does not change the values in any other column. Updating a column or field does not change values in any other column. Changing the order of columns or fields in a struct does not change the values associated with a column or field name. Iceberg uses unique IDs to track each column in a table. When you add a column, it is assigned a new ID so existing data is never used by mistake. Formats that track columns by name can inadvertently un-delete a column if a name is reused, which violates #1. Formats that track columns by position cannot delete columns without changing the names that are used for each column, which violates #2.","title":"Correctness"},{"location":"evolution/#partition-evolution","text":"Iceberg table partitioning can be updated in an existing table because queries do not reference partition values directly. When you evolve a partition spec, the old data written with an earlier spec remains unchanged. New data is written using the new spec in a new layout. Metadata for each of the partition versions is kept separately. Because of this, when you start writing queries, you get split planning. This is where each partition layout plans files separately using the filter it derives for that specific partition layout. Here\u2019s a visual representation of a contrived example: The data for 2008 is partitioned by month. Starting from 2009 the table is updated so that the data is instead partitioned by day. Both partitioning layouts are able to coexist in the same table. Iceberg uses hidden partitioning , so you don\u2019t need to write queries for a specific partition layout to be fast. Instead, you can write queries that select the data you need, and Iceberg automatically prunes out files that don\u2019t contain matching data. Partition evolution is a metadata operation and does not eagerly rewrite files. Iceberg\u2019s Java table API provides updateSpec API to update partition spec. For example, the following code could be used to update the partition spec to add a new partition field that places id column values into 8 buckets and remove an existing partition field category : Table sampleTable = ...; sampleTable.updateSpec() .addField(bucket(\"id\", 8)) .removeField(\"category\") .commit(); Spark supports updating partition spec through its ALTER TABLE SQL statement, see more details in Spark SQL .","title":"Partition evolution"},{"location":"evolution/#sort-order-evolution","text":"Similar to partition spec, Iceberg sort order can also be updated in an existing table. When you evolve a sort order, the old data written with an earlier order remains unchanged. Engines can always choose to write data in the latest sort order or unsorted when sorting is prohibitively expensive. Iceberg\u2019s Java table API provides replaceSortOrder API to update partition spec. For example, the following code could be used to create a new sort order with id column sorted in ascending order with nulls last, and category column sorted in descending order with nulls first: Table sampleTable = ...; sampleTable.replaceSortOrder() .asc(\"id\", NullOrder.NULLS_LAST) .dec(\"category\", NullOrder.NULL_FIRST) .commit(); Spark supports updating sort order through its ALTER TABLE SQL statement, see more details in Spark SQL .","title":"Sort order evolution"},{"location":"flink-connector/","text":"Apache Flink supports creating Iceberg table directly without creating the explicit Flink catalog in Flink SQL. That means we can just create an iceberg table by specifying 'connector'='iceberg' table option in Flink SQL which is similar to usage in the Flink official document . In Flink, the SQL CREATE TABLE test (..) WITH ('connector'='iceberg', ...) will create a Flink table in current Flink catalog (use GenericInMemoryCatalog by default), which is just mapping to the underlying iceberg table instead of maintaining iceberg table directly in current Flink catalog. To create the table in Flink SQL by using SQL syntax CREATE TABLE test (..) WITH ('connector'='iceberg', ...) , Flink iceberg connector provides the following table properties: connector : Use the constant iceberg . catalog-name : User-specified catalog name. It\u2019s required because the connector don\u2019t have any default value. catalog-type : Default to use hive if don\u2019t specify any value. The optional values are: hive : The Hive metastore catalog. hadoop : The hadoop catalog. custom : The customized catalog, see custom catalog for more details. catalog-database : The iceberg database name in the backend catalog, use the current flink database name by default. catalog-table : The iceberg table name in the backend catalog. Default to use the table name in the flink CREATE TABLE sentence. Table managed in Hive catalog. \u00b6 Before executing the following SQL, please make sure you\u2019ve configured the Flink SQL client correctly according to the quick start document . The following SQL will create a Flink table in the current Flink catalog, which maps to the iceberg table default_database.iceberg_table managed in iceberg catalog. CREATE TABLE flink_table ( id BIGINT, data STRING ) WITH ( 'connector'='iceberg', 'catalog-name'='hive_prod', 'uri'='thrift://localhost:9083', 'warehouse'='hdfs://nn:8020/path/to/warehouse' ); If you want to create a Flink table mapping to a different iceberg table managed in Hive catalog (such as hive_db.hive_iceberg_table in Hive), then you can create Flink table as following: CREATE TABLE flink_table ( id BIGINT, data STRING ) WITH ( 'connector'='iceberg', 'catalog-name'='hive_prod', 'catalog-database'='hive_db', 'catalog-table'='hive_iceberg_table', 'uri'='thrift://localhost:9083', 'warehouse'='hdfs://nn:8020/path/to/warehouse' ); Note The underlying catalog database ( hive_db in the above example) will be created automatically if it does not exist when writing records into the Flink table. Table managed in hadoop catalog \u00b6 The following SQL will create a Flink table in current Flink catalog, which maps to the iceberg table default_database.flink_table managed in hadoop catalog. CREATE TABLE flink_table ( id BIGINT, data STRING ) WITH ( 'connector'='iceberg', 'catalog-name'='hadoop_prod', 'catalog-type'='hadoop', 'warehouse'='hdfs://nn:8020/path/to/warehouse' ); Table managed in custom catalog \u00b6 The following SQL will create a Flink table in current Flink catalog, which maps to the iceberg table default_database.flink_table managed in custom catalog. CREATE TABLE flink_table ( id BIGINT, data STRING ) WITH ( 'connector'='iceberg', 'catalog-name'='custom_prod', 'catalog-type'='custom', 'catalog-impl'='com.my.custom.CatalogImpl', -- More table properties for the customized catalog 'my-additional-catalog-config'='my-value', ... ); Please check sections under the Integrations tab for all custom catalogs. A complete example. \u00b6 Take the Hive catalog as an example: CREATE TABLE flink_table ( id BIGINT, data STRING ) WITH ( 'connector'='iceberg', 'catalog-name'='hive_prod', 'uri'='thrift://localhost:9083', 'warehouse'='file:///path/to/warehouse' ); INSERT INTO flink_table VALUES (1, 'AAA'), (2, 'BBB'), (3, 'CCC'); SET execution.result-mode=tableau; SELECT * FROM flink_table; +----+------+ | id | data | +----+------+ | 1 | AAA | | 2 | BBB | | 3 | CCC | +----+------+ 3 rows in set For more details, please refer to the Iceberg Flink document .","title":"Flink Connector"},{"location":"flink-connector/#table-managed-in-hive-catalog","text":"Before executing the following SQL, please make sure you\u2019ve configured the Flink SQL client correctly according to the quick start document . The following SQL will create a Flink table in the current Flink catalog, which maps to the iceberg table default_database.iceberg_table managed in iceberg catalog. CREATE TABLE flink_table ( id BIGINT, data STRING ) WITH ( 'connector'='iceberg', 'catalog-name'='hive_prod', 'uri'='thrift://localhost:9083', 'warehouse'='hdfs://nn:8020/path/to/warehouse' ); If you want to create a Flink table mapping to a different iceberg table managed in Hive catalog (such as hive_db.hive_iceberg_table in Hive), then you can create Flink table as following: CREATE TABLE flink_table ( id BIGINT, data STRING ) WITH ( 'connector'='iceberg', 'catalog-name'='hive_prod', 'catalog-database'='hive_db', 'catalog-table'='hive_iceberg_table', 'uri'='thrift://localhost:9083', 'warehouse'='hdfs://nn:8020/path/to/warehouse' ); Note The underlying catalog database ( hive_db in the above example) will be created automatically if it does not exist when writing records into the Flink table.","title":"Table managed in Hive catalog."},{"location":"flink-connector/#table-managed-in-hadoop-catalog","text":"The following SQL will create a Flink table in current Flink catalog, which maps to the iceberg table default_database.flink_table managed in hadoop catalog. CREATE TABLE flink_table ( id BIGINT, data STRING ) WITH ( 'connector'='iceberg', 'catalog-name'='hadoop_prod', 'catalog-type'='hadoop', 'warehouse'='hdfs://nn:8020/path/to/warehouse' );","title":"Table managed in hadoop catalog"},{"location":"flink-connector/#table-managed-in-custom-catalog","text":"The following SQL will create a Flink table in current Flink catalog, which maps to the iceberg table default_database.flink_table managed in custom catalog. CREATE TABLE flink_table ( id BIGINT, data STRING ) WITH ( 'connector'='iceberg', 'catalog-name'='custom_prod', 'catalog-type'='custom', 'catalog-impl'='com.my.custom.CatalogImpl', -- More table properties for the customized catalog 'my-additional-catalog-config'='my-value', ... ); Please check sections under the Integrations tab for all custom catalogs.","title":"Table managed in custom catalog"},{"location":"flink-connector/#a-complete-example","text":"Take the Hive catalog as an example: CREATE TABLE flink_table ( id BIGINT, data STRING ) WITH ( 'connector'='iceberg', 'catalog-name'='hive_prod', 'uri'='thrift://localhost:9083', 'warehouse'='file:///path/to/warehouse' ); INSERT INTO flink_table VALUES (1, 'AAA'), (2, 'BBB'), (3, 'CCC'); SET execution.result-mode=tableau; SELECT * FROM flink_table; +----+------+ | id | data | +----+------+ | 1 | AAA | | 2 | BBB | | 3 | CCC | +----+------+ 3 rows in set For more details, please refer to the Iceberg Flink document .","title":"A complete example."},{"location":"flink/","text":"Flink \u00b6 Apache Iceberg supports both Apache Flink \u2018s DataStream API and Table API to write records into an Iceberg table. Currently, we only integrate Iceberg with Apache Flink 1.11.x. Feature support Flink 1.11.0 Notes SQL create catalog \u2714\ufe0f SQL create database \u2714\ufe0f SQL create table \u2714\ufe0f SQL create table like \u2714\ufe0f SQL alter table \u2714\ufe0f Only support altering table properties, Columns/PartitionKey changes are not supported now SQL drop_table \u2714\ufe0f SQL select \u2714\ufe0f Support both streaming and batch mode SQL insert into \u2714\ufe0f \ufe0f Support both streaming and batch mode SQL insert overwrite \u2714\ufe0f \ufe0f DataStream read \u2714\ufe0f \ufe0f DataStream append \u2714\ufe0f \ufe0f DataStream overwrite \u2714\ufe0f \ufe0f Metadata tables \ufe0f Support Java API but does not support Flink SQL Rewrite files action \u2714\ufe0f \ufe0f Preparation when using Flink SQL Client \u00b6 To create iceberg table in flink, we recommend to use Flink SQL Client because it\u2019s easier for users to understand the concepts. Step.1 Downloading the flink 1.11.x binary package from the apache flink download page . We now use scala 2.12 to archive the apache iceberg-flink-runtime jar, so it\u2019s recommended to use flink 1.11 bundled with scala 2.12. FLINK_VERSION=1.11.1 SCALA_VERSION=2.12 APACHE_FLINK_URL=archive.apache.org/dist/flink/ wget ${APACHE_FLINK_URL}/flink-${FLINK_VERSION}/flink-${FLINK_VERSION}-bin-scala_${SCALA_VERSION}.tgz tar xzvf flink-${FLINK_VERSION}-bin-scala_${SCALA_VERSION}.tgz Step.2 Start a standalone flink cluster within hadoop environment. # HADOOP_HOME is your hadoop root directory after unpack the binary package. export HADOOP_CLASSPATH=`$HADOOP_HOME/bin/hadoop classpath` # Start the flink standalone cluster ./bin/start-cluster.sh Step.3 Start the flink SQL client. We\u2019ve created a separate flink-runtime module in iceberg project to generate a bundled jar, which could be loaded by flink SQL client directly. If we want to build the flink-runtime bundled jar manually, please just build the iceberg project and it will generate the jar under <iceberg-root-dir>/flink-runtime/build/libs . Of course, we could also download the flink-runtime jar from the apache official repository . # HADOOP_HOME is your hadoop root directory after unpack the binary package. export HADOOP_CLASSPATH=`$HADOOP_HOME/bin/hadoop classpath` ./bin/sql-client.sh embedded -j <flink-runtime-directory>/iceberg-flink-runtime-xxx.jar shell By default, iceberg has included hadoop jars for hadoop catalog. If we want to use hive catalog, we will need to load the hive jars when opening the flink sql client. Fortunately, apache flink has provided a bundled hive jar for sql client. So we could open the sql client as the following: # HADOOP_HOME is your hadoop root directory after unpack the binary package. export HADOOP_CLASSPATH=`$HADOOP_HOME/bin/hadoop classpath` # download Iceberg dependency ICEBERG_VERSION=0.11.1 MAVEN_URL=https://repo1.maven.org/maven2 ICEBERG_MAVEN_URL=${MAVEN_URL}/org/apache/iceberg ICEBERG_PACKAGE=iceberg-flink-runtime wget ${ICEBERG_MAVEN_URL}/${ICEBERG_PACKAGE}/${ICEBERG_VERSION}/${ICEBERG_PACKAGE}-${ICEBERG_VERSION}.jar # download the flink-sql-connector-hive-${HIVE_VERSION}_${SCALA_VERSION}-${FLINK_VERSION}.jar HIVE_VERSION=2.3.6 SCALA_VERSION=2.11 FLINK_VERSION=1.11.0 FLINK_CONNECTOR_URL=${MAVEN_URL}/org/apache/flink FLINK_CONNECTOR_PACKAGE=flink-sql-connector-hive wget ${FLINK_CONNECTOR_URL}/${FLINK_CONNECTOR_PACKAGE}-${HIVE_VERSION}_${SCALA_VERSION}/${FLINK_VERSION}/${FLINK_CONNECTOR_PACKAGE}-${HIVE_VERSION}_${SCALA_VERSION}-${FLINK_VERSION}.jar # open the SQL client. /path/to/bin/sql-client.sh embedded \\ -j ${ICEBERG_PACKAGE}-${ICEBERG_VERSION}.jar \\ -j ${FLINK_CONNECTOR_PACKAGE}-${HIVE_VERSION}_${SCALA_VERSION}-${FLINK_VERSION}.jar \\ shell Preparation when using Flink\u2019s Python API \u00b6 Install the Apache Flink dependency using pip pip install apache-flink==1.11.1 In order for pyflink to function properly, it needs to have access to all Hadoop jars. For pyflink we need to copy those Hadoop jars to the installation directory of pyflink , which can be found under <PYTHON_ENV_INSTALL_DIR>/site-packages/pyflink/lib/ (see also a mention of this on the Flink ML ). We can use the following short Python script to copy all Hadoop jars (you need to make sure that HADOOP_HOME points to your Hadoop installation): import os import shutil import site def copy_all_hadoop_jars_to_pyflink(): if not os.getenv(\"HADOOP_HOME\"): raise Exception(\"The HADOOP_HOME env var must be set and point to a valid Hadoop installation\") jar_files = [] def find_pyflink_lib_dir(): for dir in site.getsitepackages(): package_dir = os.path.join(dir, \"pyflink\", \"lib\") if os.path.exists(package_dir): return package_dir return None for root, _, files in os.walk(os.getenv(\"HADOOP_HOME\")): for file in files: if file.endswith(\".jar\"): jar_files.append(os.path.join(root, file)) pyflink_lib_dir = find_pyflink_lib_dir() num_jar_files = len(jar_files) print(f\"Copying {num_jar_files} Hadoop jar files to pyflink's lib directory at {pyflink_lib_dir}\") for jar in jar_files: shutil.copy(jar, pyflink_lib_dir) if __name__ == '__main__': copy_all_hadoop_jars_to_pyflink() Once the script finished, you should see output similar to Copying 645 Hadoop jar files to pyflink's lib directory at <PYTHON_DIR>/lib/python3.8/site-packages/pyflink/lib Now we need to provide a file:// path to the iceberg-flink-runtime jar, which we can either get by building the project and looking at <iceberg-root-dir>/flink-runtime/build/libs , or downloading it from the Apache official repository . Third-party libs can be added to pyflink via env.add_jars(\"file:///my/jar/path/connector.jar\") / table_env.get_config().get_configuration().set_string(\"pipeline.jars\", \"file:///my/jar/path/connector.jar\") , which is also mentioned in the official docs . In our example we\u2019re using env.add_jars(..) as shown below: import os from pyflink.datastream import StreamExecutionEnvironment env = StreamExecutionEnvironment.get_execution_environment() iceberg_flink_runtime_jar = os.path.join(os.getcwd(), \"iceberg-flink-runtime-0.12.1.jar\") env.add_jars(\"file://{}\".format(iceberg_flink_runtime_jar)) Once we reached this point, we can then create a StreamTableEnvironment and execute Flink SQL statements. The below example shows how to create a custom catalog via the Python Table API: from pyflink.table import StreamTableEnvironment table_env = StreamTableEnvironment.create(env) table_env.execute_sql(\"CREATE CATALOG my_catalog WITH (\" \"'type'='iceberg', \" \"'catalog-impl'='com.my.custom.CatalogImpl', \" \"'my-additional-catalog-config'='my-value')\") For more details, please refer to the Python Table API . Creating catalogs and using catalogs. \u00b6 Flink 1.11 support to create catalogs by using flink sql. Catalog Configuration \u00b6 A catalog is created and named by executing the following query (replace <catalog_name> with your catalog name and <config_key> = <config_value> with catalog implementation config): CREATE CATALOG <catalog_name> WITH ( 'type'='iceberg', `<config_key>`=`<config_value>` ); The following properties can be set globally and are not limited to a specific catalog implementation: type : Must be iceberg . (required) catalog-type : hive or hadoop for built-in catalogs, or left unset for custom catalog implementations using catalog-impl. (Optional) catalog-impl : The fully-qualified class name custom catalog implementation, must be set if catalog-type is unset. (Optional) property-version : Version number to describe the property version. This property can be used for backwards compatibility in case the property format changes. The current property version is 1 . (Optional) cache-enabled : Whether to enable catalog cache, default value is true Hive catalog \u00b6 This creates an iceberg catalog named hive_catalog that can be configured using 'catalog-type'='hive' , which loads tables from a hive metastore: CREATE CATALOG hive_catalog WITH ( 'type'='iceberg', 'catalog-type'='hive', 'uri'='thrift://localhost:9083', 'clients'='5', 'property-version'='1', 'warehouse'='hdfs://nn:8020/warehouse/path' ); The following properties can be set if using the Hive catalog: uri : The Hive metastore\u2019s thrift URI. (Required) clients : The Hive metastore client pool size, default value is 2. (Optional) warehouse : The Hive warehouse location, users should specify this path if neither set the hive-conf-dir to specify a location containing a hive-site.xml configuration file nor add a correct hive-site.xml to classpath. hive-conf-dir : Path to a directory containing a hive-site.xml configuration file which will be used to provide custom Hive configuration values. The value of hive.metastore.warehouse.dir from <hive-conf-dir>/hive-site.xml (or hive configure file from classpath) will be overwrote with the warehouse value if setting both hive-conf-dir and warehouse when creating iceberg catalog. Hadoop catalog \u00b6 Iceberg also supports a directory-based catalog in HDFS that can be configured using 'catalog-type'='hadoop' : CREATE CATALOG hadoop_catalog WITH ( 'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='hdfs://nn:8020/warehouse/path', 'property-version'='1' ); The following properties can be set if using the Hadoop catalog: warehouse : The HDFS directory to store metadata files and data files. (Required) We could execute the sql command USE CATALOG hive_catalog to set the current catalog. Custom catalog \u00b6 Flink also supports loading a custom Iceberg Catalog implementation by specifying the catalog-impl property. Here is an example: CREATE CATALOG my_catalog WITH ( 'type'='iceberg', 'catalog-impl'='com.my.custom.CatalogImpl', 'my-additional-catalog-config'='my-value' ); Create through YAML config \u00b6 Catalogs can be registered in sql-client-defaults.yaml before starting the SQL client. Here is an example: catalogs: - name: my_catalog type: iceberg catalog-type: hadoop warehouse: hdfs://nn:8020/warehouse/path DDL commands \u00b6 CREATE DATABASE \u00b6 By default, iceberg will use the default database in flink. Using the following example to create a separate database if we don\u2019t want to create tables under the default database: CREATE DATABASE iceberg_db; USE iceberg_db; CREATE TABLE \u00b6 CREATE TABLE `hive_catalog`.`default`.`sample` ( id BIGINT COMMENT 'unique id', data STRING ); Table create commands support the most commonly used flink create clauses now, including: PARTITION BY (column1, column2, ...) to configure partitioning, apache flink does not yet support hidden partitioning. COMMENT 'table document' to set a table description. WITH ('key'='value', ...) to set table configuration which will be stored in apache iceberg table properties. Currently, it does not support computed column, primary key and watermark definition etc. PARTITIONED BY \u00b6 To create a partition table, use PARTITIONED BY : CREATE TABLE `hive_catalog`.`default`.`sample` ( id BIGINT COMMENT 'unique id', data STRING ) PARTITIONED BY (data); Apache Iceberg support hidden partition but apache flink don\u2019t support partitioning by a function on columns, so we\u2019ve no way to support hidden partition in flink DDL now, we will improve apache flink DDL in future. CREATE TABLE LIKE \u00b6 To create a table with the same schema, partitioning, and table properties as another table, use CREATE TABLE LIKE . CREATE TABLE `hive_catalog`.`default`.`sample` ( id BIGINT COMMENT 'unique id', data STRING ); CREATE TABLE `hive_catalog`.`default`.`sample_like` LIKE `hive_catalog`.`default`.`sample`; For more details, refer to the Flink CREATE TABLE documentation . ALTER TABLE \u00b6 Iceberg only support altering table properties in flink 1.11 now. ALTER TABLE `hive_catalog`.`default`.`sample` SET ('write.format.default'='avro') ALTER TABLE .. RENAME TO \u00b6 ALTER TABLE `hive_catalog`.`default`.`sample` RENAME TO `hive_catalog`.`default`.`new_sample`; DROP TABLE \u00b6 To delete a table, run: DROP TABLE `hive_catalog`.`default`.`sample`; Querying with SQL \u00b6 Iceberg support both streaming and batch read in flink now. we could execute the following sql command to switch the execute type from \u2018streaming\u2019 mode to \u2018batch\u2019 mode, and vice versa: -- Execute the flink job in streaming mode for current session context SET execution.type = streaming -- Execute the flink job in batch mode for current session context SET execution.type = batch Flink batch read \u00b6 If want to check all the rows in iceberg table by submitting a flink batch job, you could execute the following sentences: -- Execute the flink job in batch mode for current session context SET execution.type = batch ; SELECT * FROM sample ; Flink streaming read \u00b6 Iceberg supports processing incremental data in flink streaming jobs which starts from a historical snapshot-id: -- Submit the flink job in streaming mode for current session. SET execution.type = streaming ; -- Enable this switch because streaming read SQL will provide few job options in flink SQL hint options. SET table.dynamic-table-options.enabled=true; -- Read all the records from the iceberg current snapshot, and then read incremental data starting from that snapshot. SELECT * FROM sample /*+ OPTIONS('streaming'='true', 'monitor-interval'='1s')*/ ; -- Read all incremental data starting from the snapshot-id '3821550127947089987' (records from this snapshot will be excluded). SELECT * FROM sample /*+ OPTIONS('streaming'='true', 'monitor-interval'='1s', 'start-snapshot-id'='3821550127947089987')*/ ; Those are the options that could be set in flink SQL hint options for streaming job: monitor-interval: time interval for consecutively monitoring newly committed data files (default value: \u20181s\u2019). start-snapshot-id: the snapshot id that streaming job starts from. Writing with SQL \u00b6 Iceberg support both INSERT INTO and INSERT OVERWRITE in flink 1.11 now. INSERT INTO \u00b6 To append new data to a table with a flink streaming job, use INSERT INTO : INSERT INTO `hive_catalog`.`default`.`sample` VALUES (1, 'a'); INSERT INTO `hive_catalog`.`default`.`sample` SELECT id, data from other_kafka_table; INSERT OVERWRITE \u00b6 To replace data in the table with the result of a query, use INSERT OVERWRITE in batch job (flink streaming job does not support INSERT OVERWRITE ). Overwrites are atomic operations for Iceberg tables. Partitions that have rows produced by the SELECT query will be replaced, for example: INSERT OVERWRITE sample VALUES (1, 'a'); Iceberg also support overwriting given partitions by the select values: INSERT OVERWRITE `hive_catalog`.`default`.`sample` PARTITION(data='a') SELECT 6; For a partitioned iceberg table, when all the partition columns are set a value in PARTITION clause, it is inserting into a static partition, otherwise if partial partition columns (prefix part of all partition columns) are set a value in PARTITION clause, it is writing the query result into a dynamic partition. For an unpartitioned iceberg table, its data will be completely overwritten by INSERT OVERWRITE . Reading with DataStream \u00b6 Iceberg support streaming or batch read in Java API now. Batch Read \u00b6 This example will read all records from iceberg table and then print to the stdout console in flink batch job: StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(); TableLoader tableLoader = TableLoader.fromHadoopTable(\"hdfs://nn:8020/warehouse/path\"); DataStream<RowData> batch = FlinkSource.forRowData() .env(env) .tableLoader(tableLoader) .streaming(false) .build(); // Print all records to stdout. batch.print(); // Submit and execute this batch read job. env.execute(\"Test Iceberg Batch Read\"); Streaming read \u00b6 This example will read incremental records which start from snapshot-id \u20183821550127947089987\u2019 and print to stdout console in flink streaming job: StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(); TableLoader tableLoader = TableLoader.fromHadoopTable(\"hdfs://nn:8020/warehouse/path\"); DataStream<RowData> stream = FlinkSource.forRowData() .env(env) .tableLoader(tableLoader) .streaming(true) .startSnapshotId(3821550127947089987L) .build(); // Print all records to stdout. stream.print(); // Submit and execute this streaming read job. env.execute(\"Test Iceberg Batch Read\"); There are other options that we could set by Java API, please see the FlinkSource#Builder . Writing with DataStream \u00b6 Iceberg support writing to iceberg table from different DataStream input. Appending data. \u00b6 we have supported writing DataStream<RowData> and DataStream<Row> to the sink iceberg table natively. StreamExecutionEnvironment env = ...; DataStream<RowData> input = ... ; Configuration hadoopConf = new Configuration(); TableLoader tableLoader = TableLoader.fromHadoopTable(\"hdfs://nn:8020/warehouse/path\", hadoopConf); FlinkSink.forRowData(input) .tableLoader(tableLoader) .build(); env.execute(\"Test Iceberg DataStream\"); The iceberg API also allows users to write generic DataStream<T> to iceberg table, more example could be found in this unit test . Overwrite data \u00b6 To overwrite the data in existing iceberg table dynamically, we could set the overwrite flag in FlinkSink builder. StreamExecutionEnvironment env = ...; DataStream<RowData> input = ... ; Configuration hadoopConf = new Configuration(); TableLoader tableLoader = TableLoader.fromHadoopTable(\"hdfs://nn:8020/warehouse/path\", hadoopConf); FlinkSink.forRowData(input) .tableLoader(tableLoader) .overwrite(true) .build(); env.execute(\"Test Iceberg DataStream\"); Inspecting tables. \u00b6 Iceberg does not support inspecting table in flink sql now, we need to use iceberg\u2019s Java API to read iceberg\u2019s meta data to get those table information. Rewrite files action. \u00b6 Iceberg provides API to rewrite small files into large files by submitting flink batch job. The behavior of this flink action is the same as the spark\u2019s rewriteDataFiles . import org.apache.iceberg.flink.actions.Actions; TableLoader tableLoader = TableLoader.fromHadoopTable(\"hdfs://nn:8020/warehouse/path\"); Table table = tableLoader.loadTable(); RewriteDataFilesActionResult result = Actions.forTable(table) .rewriteDataFiles() .execute(); For more doc about options of the rewrite files action, please see RewriteDataFilesAction Future improvement. \u00b6 There are some features that we do not yet support in the current flink iceberg integration work: Don\u2019t support creating iceberg table with hidden partitioning. Discussion in flink mail list. Don\u2019t support creating iceberg table with computed column. Don\u2019t support creating iceberg table with watermark. Don\u2019t support adding columns, removing columns, renaming columns, changing columns. FLINK-19062 is tracking this.","title":"Getting Started"},{"location":"flink/#flink","text":"Apache Iceberg supports both Apache Flink \u2018s DataStream API and Table API to write records into an Iceberg table. Currently, we only integrate Iceberg with Apache Flink 1.11.x. Feature support Flink 1.11.0 Notes SQL create catalog \u2714\ufe0f SQL create database \u2714\ufe0f SQL create table \u2714\ufe0f SQL create table like \u2714\ufe0f SQL alter table \u2714\ufe0f Only support altering table properties, Columns/PartitionKey changes are not supported now SQL drop_table \u2714\ufe0f SQL select \u2714\ufe0f Support both streaming and batch mode SQL insert into \u2714\ufe0f \ufe0f Support both streaming and batch mode SQL insert overwrite \u2714\ufe0f \ufe0f DataStream read \u2714\ufe0f \ufe0f DataStream append \u2714\ufe0f \ufe0f DataStream overwrite \u2714\ufe0f \ufe0f Metadata tables \ufe0f Support Java API but does not support Flink SQL Rewrite files action \u2714\ufe0f \ufe0f","title":"Flink"},{"location":"flink/#preparation-when-using-flink-sql-client","text":"To create iceberg table in flink, we recommend to use Flink SQL Client because it\u2019s easier for users to understand the concepts. Step.1 Downloading the flink 1.11.x binary package from the apache flink download page . We now use scala 2.12 to archive the apache iceberg-flink-runtime jar, so it\u2019s recommended to use flink 1.11 bundled with scala 2.12. FLINK_VERSION=1.11.1 SCALA_VERSION=2.12 APACHE_FLINK_URL=archive.apache.org/dist/flink/ wget ${APACHE_FLINK_URL}/flink-${FLINK_VERSION}/flink-${FLINK_VERSION}-bin-scala_${SCALA_VERSION}.tgz tar xzvf flink-${FLINK_VERSION}-bin-scala_${SCALA_VERSION}.tgz Step.2 Start a standalone flink cluster within hadoop environment. # HADOOP_HOME is your hadoop root directory after unpack the binary package. export HADOOP_CLASSPATH=`$HADOOP_HOME/bin/hadoop classpath` # Start the flink standalone cluster ./bin/start-cluster.sh Step.3 Start the flink SQL client. We\u2019ve created a separate flink-runtime module in iceberg project to generate a bundled jar, which could be loaded by flink SQL client directly. If we want to build the flink-runtime bundled jar manually, please just build the iceberg project and it will generate the jar under <iceberg-root-dir>/flink-runtime/build/libs . Of course, we could also download the flink-runtime jar from the apache official repository . # HADOOP_HOME is your hadoop root directory after unpack the binary package. export HADOOP_CLASSPATH=`$HADOOP_HOME/bin/hadoop classpath` ./bin/sql-client.sh embedded -j <flink-runtime-directory>/iceberg-flink-runtime-xxx.jar shell By default, iceberg has included hadoop jars for hadoop catalog. If we want to use hive catalog, we will need to load the hive jars when opening the flink sql client. Fortunately, apache flink has provided a bundled hive jar for sql client. So we could open the sql client as the following: # HADOOP_HOME is your hadoop root directory after unpack the binary package. export HADOOP_CLASSPATH=`$HADOOP_HOME/bin/hadoop classpath` # download Iceberg dependency ICEBERG_VERSION=0.11.1 MAVEN_URL=https://repo1.maven.org/maven2 ICEBERG_MAVEN_URL=${MAVEN_URL}/org/apache/iceberg ICEBERG_PACKAGE=iceberg-flink-runtime wget ${ICEBERG_MAVEN_URL}/${ICEBERG_PACKAGE}/${ICEBERG_VERSION}/${ICEBERG_PACKAGE}-${ICEBERG_VERSION}.jar # download the flink-sql-connector-hive-${HIVE_VERSION}_${SCALA_VERSION}-${FLINK_VERSION}.jar HIVE_VERSION=2.3.6 SCALA_VERSION=2.11 FLINK_VERSION=1.11.0 FLINK_CONNECTOR_URL=${MAVEN_URL}/org/apache/flink FLINK_CONNECTOR_PACKAGE=flink-sql-connector-hive wget ${FLINK_CONNECTOR_URL}/${FLINK_CONNECTOR_PACKAGE}-${HIVE_VERSION}_${SCALA_VERSION}/${FLINK_VERSION}/${FLINK_CONNECTOR_PACKAGE}-${HIVE_VERSION}_${SCALA_VERSION}-${FLINK_VERSION}.jar # open the SQL client. /path/to/bin/sql-client.sh embedded \\ -j ${ICEBERG_PACKAGE}-${ICEBERG_VERSION}.jar \\ -j ${FLINK_CONNECTOR_PACKAGE}-${HIVE_VERSION}_${SCALA_VERSION}-${FLINK_VERSION}.jar \\ shell","title":"Preparation when using Flink SQL Client"},{"location":"flink/#preparation-when-using-flinks-python-api","text":"Install the Apache Flink dependency using pip pip install apache-flink==1.11.1 In order for pyflink to function properly, it needs to have access to all Hadoop jars. For pyflink we need to copy those Hadoop jars to the installation directory of pyflink , which can be found under <PYTHON_ENV_INSTALL_DIR>/site-packages/pyflink/lib/ (see also a mention of this on the Flink ML ). We can use the following short Python script to copy all Hadoop jars (you need to make sure that HADOOP_HOME points to your Hadoop installation): import os import shutil import site def copy_all_hadoop_jars_to_pyflink(): if not os.getenv(\"HADOOP_HOME\"): raise Exception(\"The HADOOP_HOME env var must be set and point to a valid Hadoop installation\") jar_files = [] def find_pyflink_lib_dir(): for dir in site.getsitepackages(): package_dir = os.path.join(dir, \"pyflink\", \"lib\") if os.path.exists(package_dir): return package_dir return None for root, _, files in os.walk(os.getenv(\"HADOOP_HOME\")): for file in files: if file.endswith(\".jar\"): jar_files.append(os.path.join(root, file)) pyflink_lib_dir = find_pyflink_lib_dir() num_jar_files = len(jar_files) print(f\"Copying {num_jar_files} Hadoop jar files to pyflink's lib directory at {pyflink_lib_dir}\") for jar in jar_files: shutil.copy(jar, pyflink_lib_dir) if __name__ == '__main__': copy_all_hadoop_jars_to_pyflink() Once the script finished, you should see output similar to Copying 645 Hadoop jar files to pyflink's lib directory at <PYTHON_DIR>/lib/python3.8/site-packages/pyflink/lib Now we need to provide a file:// path to the iceberg-flink-runtime jar, which we can either get by building the project and looking at <iceberg-root-dir>/flink-runtime/build/libs , or downloading it from the Apache official repository . Third-party libs can be added to pyflink via env.add_jars(\"file:///my/jar/path/connector.jar\") / table_env.get_config().get_configuration().set_string(\"pipeline.jars\", \"file:///my/jar/path/connector.jar\") , which is also mentioned in the official docs . In our example we\u2019re using env.add_jars(..) as shown below: import os from pyflink.datastream import StreamExecutionEnvironment env = StreamExecutionEnvironment.get_execution_environment() iceberg_flink_runtime_jar = os.path.join(os.getcwd(), \"iceberg-flink-runtime-0.12.1.jar\") env.add_jars(\"file://{}\".format(iceberg_flink_runtime_jar)) Once we reached this point, we can then create a StreamTableEnvironment and execute Flink SQL statements. The below example shows how to create a custom catalog via the Python Table API: from pyflink.table import StreamTableEnvironment table_env = StreamTableEnvironment.create(env) table_env.execute_sql(\"CREATE CATALOG my_catalog WITH (\" \"'type'='iceberg', \" \"'catalog-impl'='com.my.custom.CatalogImpl', \" \"'my-additional-catalog-config'='my-value')\") For more details, please refer to the Python Table API .","title":"Preparation when using Flink's Python API"},{"location":"flink/#creating-catalogs-and-using-catalogs","text":"Flink 1.11 support to create catalogs by using flink sql.","title":"Creating catalogs and using catalogs."},{"location":"flink/#catalog-configuration","text":"A catalog is created and named by executing the following query (replace <catalog_name> with your catalog name and <config_key> = <config_value> with catalog implementation config): CREATE CATALOG <catalog_name> WITH ( 'type'='iceberg', `<config_key>`=`<config_value>` ); The following properties can be set globally and are not limited to a specific catalog implementation: type : Must be iceberg . (required) catalog-type : hive or hadoop for built-in catalogs, or left unset for custom catalog implementations using catalog-impl. (Optional) catalog-impl : The fully-qualified class name custom catalog implementation, must be set if catalog-type is unset. (Optional) property-version : Version number to describe the property version. This property can be used for backwards compatibility in case the property format changes. The current property version is 1 . (Optional) cache-enabled : Whether to enable catalog cache, default value is true","title":"Catalog Configuration"},{"location":"flink/#hive-catalog","text":"This creates an iceberg catalog named hive_catalog that can be configured using 'catalog-type'='hive' , which loads tables from a hive metastore: CREATE CATALOG hive_catalog WITH ( 'type'='iceberg', 'catalog-type'='hive', 'uri'='thrift://localhost:9083', 'clients'='5', 'property-version'='1', 'warehouse'='hdfs://nn:8020/warehouse/path' ); The following properties can be set if using the Hive catalog: uri : The Hive metastore\u2019s thrift URI. (Required) clients : The Hive metastore client pool size, default value is 2. (Optional) warehouse : The Hive warehouse location, users should specify this path if neither set the hive-conf-dir to specify a location containing a hive-site.xml configuration file nor add a correct hive-site.xml to classpath. hive-conf-dir : Path to a directory containing a hive-site.xml configuration file which will be used to provide custom Hive configuration values. The value of hive.metastore.warehouse.dir from <hive-conf-dir>/hive-site.xml (or hive configure file from classpath) will be overwrote with the warehouse value if setting both hive-conf-dir and warehouse when creating iceberg catalog.","title":"Hive catalog"},{"location":"flink/#hadoop-catalog","text":"Iceberg also supports a directory-based catalog in HDFS that can be configured using 'catalog-type'='hadoop' : CREATE CATALOG hadoop_catalog WITH ( 'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='hdfs://nn:8020/warehouse/path', 'property-version'='1' ); The following properties can be set if using the Hadoop catalog: warehouse : The HDFS directory to store metadata files and data files. (Required) We could execute the sql command USE CATALOG hive_catalog to set the current catalog.","title":"Hadoop catalog"},{"location":"flink/#custom-catalog","text":"Flink also supports loading a custom Iceberg Catalog implementation by specifying the catalog-impl property. Here is an example: CREATE CATALOG my_catalog WITH ( 'type'='iceberg', 'catalog-impl'='com.my.custom.CatalogImpl', 'my-additional-catalog-config'='my-value' );","title":"Custom catalog"},{"location":"flink/#create-through-yaml-config","text":"Catalogs can be registered in sql-client-defaults.yaml before starting the SQL client. Here is an example: catalogs: - name: my_catalog type: iceberg catalog-type: hadoop warehouse: hdfs://nn:8020/warehouse/path","title":"Create through YAML config"},{"location":"flink/#ddl-commands","text":"","title":"DDL commands"},{"location":"flink/#create-database","text":"By default, iceberg will use the default database in flink. Using the following example to create a separate database if we don\u2019t want to create tables under the default database: CREATE DATABASE iceberg_db; USE iceberg_db;","title":"CREATE DATABASE"},{"location":"flink/#create-table","text":"CREATE TABLE `hive_catalog`.`default`.`sample` ( id BIGINT COMMENT 'unique id', data STRING ); Table create commands support the most commonly used flink create clauses now, including: PARTITION BY (column1, column2, ...) to configure partitioning, apache flink does not yet support hidden partitioning. COMMENT 'table document' to set a table description. WITH ('key'='value', ...) to set table configuration which will be stored in apache iceberg table properties. Currently, it does not support computed column, primary key and watermark definition etc.","title":"CREATE TABLE"},{"location":"flink/#partitioned-by","text":"To create a partition table, use PARTITIONED BY : CREATE TABLE `hive_catalog`.`default`.`sample` ( id BIGINT COMMENT 'unique id', data STRING ) PARTITIONED BY (data); Apache Iceberg support hidden partition but apache flink don\u2019t support partitioning by a function on columns, so we\u2019ve no way to support hidden partition in flink DDL now, we will improve apache flink DDL in future.","title":"PARTITIONED BY"},{"location":"flink/#create-table-like","text":"To create a table with the same schema, partitioning, and table properties as another table, use CREATE TABLE LIKE . CREATE TABLE `hive_catalog`.`default`.`sample` ( id BIGINT COMMENT 'unique id', data STRING ); CREATE TABLE `hive_catalog`.`default`.`sample_like` LIKE `hive_catalog`.`default`.`sample`; For more details, refer to the Flink CREATE TABLE documentation .","title":"CREATE TABLE LIKE"},{"location":"flink/#alter-table","text":"Iceberg only support altering table properties in flink 1.11 now. ALTER TABLE `hive_catalog`.`default`.`sample` SET ('write.format.default'='avro')","title":"ALTER TABLE"},{"location":"flink/#alter-table-rename-to","text":"ALTER TABLE `hive_catalog`.`default`.`sample` RENAME TO `hive_catalog`.`default`.`new_sample`;","title":"ALTER TABLE .. RENAME TO"},{"location":"flink/#drop-table","text":"To delete a table, run: DROP TABLE `hive_catalog`.`default`.`sample`;","title":"DROP TABLE"},{"location":"flink/#querying-with-sql","text":"Iceberg support both streaming and batch read in flink now. we could execute the following sql command to switch the execute type from \u2018streaming\u2019 mode to \u2018batch\u2019 mode, and vice versa: -- Execute the flink job in streaming mode for current session context SET execution.type = streaming -- Execute the flink job in batch mode for current session context SET execution.type = batch","title":"Querying with SQL"},{"location":"flink/#flink-batch-read","text":"If want to check all the rows in iceberg table by submitting a flink batch job, you could execute the following sentences: -- Execute the flink job in batch mode for current session context SET execution.type = batch ; SELECT * FROM sample ;","title":"Flink batch read"},{"location":"flink/#flink-streaming-read","text":"Iceberg supports processing incremental data in flink streaming jobs which starts from a historical snapshot-id: -- Submit the flink job in streaming mode for current session. SET execution.type = streaming ; -- Enable this switch because streaming read SQL will provide few job options in flink SQL hint options. SET table.dynamic-table-options.enabled=true; -- Read all the records from the iceberg current snapshot, and then read incremental data starting from that snapshot. SELECT * FROM sample /*+ OPTIONS('streaming'='true', 'monitor-interval'='1s')*/ ; -- Read all incremental data starting from the snapshot-id '3821550127947089987' (records from this snapshot will be excluded). SELECT * FROM sample /*+ OPTIONS('streaming'='true', 'monitor-interval'='1s', 'start-snapshot-id'='3821550127947089987')*/ ; Those are the options that could be set in flink SQL hint options for streaming job: monitor-interval: time interval for consecutively monitoring newly committed data files (default value: \u20181s\u2019). start-snapshot-id: the snapshot id that streaming job starts from.","title":"Flink streaming read"},{"location":"flink/#writing-with-sql","text":"Iceberg support both INSERT INTO and INSERT OVERWRITE in flink 1.11 now.","title":"Writing with SQL"},{"location":"flink/#insert-into","text":"To append new data to a table with a flink streaming job, use INSERT INTO : INSERT INTO `hive_catalog`.`default`.`sample` VALUES (1, 'a'); INSERT INTO `hive_catalog`.`default`.`sample` SELECT id, data from other_kafka_table;","title":"INSERT INTO"},{"location":"flink/#insert-overwrite","text":"To replace data in the table with the result of a query, use INSERT OVERWRITE in batch job (flink streaming job does not support INSERT OVERWRITE ). Overwrites are atomic operations for Iceberg tables. Partitions that have rows produced by the SELECT query will be replaced, for example: INSERT OVERWRITE sample VALUES (1, 'a'); Iceberg also support overwriting given partitions by the select values: INSERT OVERWRITE `hive_catalog`.`default`.`sample` PARTITION(data='a') SELECT 6; For a partitioned iceberg table, when all the partition columns are set a value in PARTITION clause, it is inserting into a static partition, otherwise if partial partition columns (prefix part of all partition columns) are set a value in PARTITION clause, it is writing the query result into a dynamic partition. For an unpartitioned iceberg table, its data will be completely overwritten by INSERT OVERWRITE .","title":"INSERT OVERWRITE"},{"location":"flink/#reading-with-datastream","text":"Iceberg support streaming or batch read in Java API now.","title":"Reading with DataStream"},{"location":"flink/#batch-read","text":"This example will read all records from iceberg table and then print to the stdout console in flink batch job: StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(); TableLoader tableLoader = TableLoader.fromHadoopTable(\"hdfs://nn:8020/warehouse/path\"); DataStream<RowData> batch = FlinkSource.forRowData() .env(env) .tableLoader(tableLoader) .streaming(false) .build(); // Print all records to stdout. batch.print(); // Submit and execute this batch read job. env.execute(\"Test Iceberg Batch Read\");","title":"Batch Read"},{"location":"flink/#streaming-read","text":"This example will read incremental records which start from snapshot-id \u20183821550127947089987\u2019 and print to stdout console in flink streaming job: StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(); TableLoader tableLoader = TableLoader.fromHadoopTable(\"hdfs://nn:8020/warehouse/path\"); DataStream<RowData> stream = FlinkSource.forRowData() .env(env) .tableLoader(tableLoader) .streaming(true) .startSnapshotId(3821550127947089987L) .build(); // Print all records to stdout. stream.print(); // Submit and execute this streaming read job. env.execute(\"Test Iceberg Batch Read\"); There are other options that we could set by Java API, please see the FlinkSource#Builder .","title":"Streaming read"},{"location":"flink/#writing-with-datastream","text":"Iceberg support writing to iceberg table from different DataStream input.","title":"Writing with DataStream"},{"location":"flink/#appending-data","text":"we have supported writing DataStream<RowData> and DataStream<Row> to the sink iceberg table natively. StreamExecutionEnvironment env = ...; DataStream<RowData> input = ... ; Configuration hadoopConf = new Configuration(); TableLoader tableLoader = TableLoader.fromHadoopTable(\"hdfs://nn:8020/warehouse/path\", hadoopConf); FlinkSink.forRowData(input) .tableLoader(tableLoader) .build(); env.execute(\"Test Iceberg DataStream\"); The iceberg API also allows users to write generic DataStream<T> to iceberg table, more example could be found in this unit test .","title":"Appending data."},{"location":"flink/#overwrite-data","text":"To overwrite the data in existing iceberg table dynamically, we could set the overwrite flag in FlinkSink builder. StreamExecutionEnvironment env = ...; DataStream<RowData> input = ... ; Configuration hadoopConf = new Configuration(); TableLoader tableLoader = TableLoader.fromHadoopTable(\"hdfs://nn:8020/warehouse/path\", hadoopConf); FlinkSink.forRowData(input) .tableLoader(tableLoader) .overwrite(true) .build(); env.execute(\"Test Iceberg DataStream\");","title":"Overwrite data"},{"location":"flink/#inspecting-tables","text":"Iceberg does not support inspecting table in flink sql now, we need to use iceberg\u2019s Java API to read iceberg\u2019s meta data to get those table information.","title":"Inspecting tables."},{"location":"flink/#rewrite-files-action","text":"Iceberg provides API to rewrite small files into large files by submitting flink batch job. The behavior of this flink action is the same as the spark\u2019s rewriteDataFiles . import org.apache.iceberg.flink.actions.Actions; TableLoader tableLoader = TableLoader.fromHadoopTable(\"hdfs://nn:8020/warehouse/path\"); Table table = tableLoader.loadTable(); RewriteDataFilesActionResult result = Actions.forTable(table) .rewriteDataFiles() .execute(); For more doc about options of the rewrite files action, please see RewriteDataFilesAction","title":"Rewrite files action."},{"location":"flink/#future-improvement","text":"There are some features that we do not yet support in the current flink iceberg integration work: Don\u2019t support creating iceberg table with hidden partitioning. Discussion in flink mail list. Don\u2019t support creating iceberg table with computed column. Don\u2019t support creating iceberg table with watermark. Don\u2019t support adding columns, removing columns, renaming columns, changing columns. FLINK-19062 is tracking this.","title":"Future improvement."},{"location":"getting-started/","text":"Getting Started \u00b6 The latest version of Iceberg is 0.12.1 . Spark is currently the most feature-rich compute engine for Iceberg operations. We recommend you to get started with Spark to understand Iceberg concepts and features with examples. You can also view documentations of using Iceberg with other compute engine under the Engines tab. Using Iceberg in Spark 3 \u00b6 To use Iceberg in a Spark shell, use the --packages option: spark-shell --packages org.apache.iceberg:iceberg-spark3-runtime:0.12.1 Note If you want to include Iceberg in your Spark installation, add the iceberg-spark3-runtime Jar to Spark\u2019s jars folder. Adding catalogs \u00b6 Iceberg comes with catalogs that enable SQL commands to manage tables and load them by name. Catalogs are configured using properties under spark.sql.catalog.(catalog_name) . This command creates a path-based catalog named local for tables under $PWD/warehouse and adds support for Iceberg tables to Spark\u2019s built-in catalog: spark-sql --packages org.apache.iceberg:iceberg-spark3-runtime:0.12.1\\ --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \\ --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog \\ --conf spark.sql.catalog.spark_catalog.type=hive \\ --conf spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog \\ --conf spark.sql.catalog.local.type=hadoop \\ --conf spark.sql.catalog.local.warehouse=$PWD/warehouse Creating a table \u00b6 To create your first Iceberg table in Spark, use the spark-sql shell or spark.sql(...) to run a CREATE TABLE command: -- local is the path-based catalog defined above CREATE TABLE local.db.table (id bigint, data string) USING iceberg Iceberg catalogs support the full range of SQL DDL commands, including: CREATE TABLE ... PARTITIONED BY CREATE TABLE ... AS SELECT ALTER TABLE DROP TABLE Writing \u00b6 Once your table is created, insert data using INSERT INTO : INSERT INTO local.db.table VALUES (1, 'a'), (2, 'b'), (3, 'c'); INSERT INTO local.db.table SELECT id, data FROM source WHERE length(data) = 1; Iceberg also adds row-level SQL updates to Spark, MERGE INTO and DELETE FROM : MERGE INTO local.db.target t USING (SELECT * FROM updates) u ON t.id = u.id WHEN MATCHED THEN UPDATE SET t.count = t.count + u.count WHEN NOT MATCHED THEN INSERT * Iceberg supports writing DataFrames using the new v2 DataFrame write API : spark.table(\"source\").select(\"id\", \"data\") .writeTo(\"local.db.table\").append() The old write API is supported, but not recommended. Reading \u00b6 To read with SQL, use the an Iceberg table name in a SELECT query: SELECT count(1) as count, data FROM local.db.table GROUP BY data SQL is also the recommended way to inspect tables . To view all of the snapshots in a table, use the snapshots metadata table: SELECT * FROM local.db.table.snapshots +-------------------------+----------------+-----------+-----------+----------------------------------------------------+-----+ | committed_at | snapshot_id | parent_id | operation | manifest_list | ... | +-------------------------+----------------+-----------+-----------+----------------------------------------------------+-----+ | 2019-02-08 03:29:51.215 | 57897183625154 | null | append | s3://.../table/metadata/snap-57897183625154-1.avro | ... | | | | | | | ... | | | | | | | ... | | ... | ... | ... | ... | ... | ... | +-------------------------+----------------+-----------+-----------+----------------------------------------------------+-----+ DataFrame reads are supported and can now reference tables by name using spark.table : val df = spark.table(\"local.db.table\") df.count() Next steps \u00b6 Next, you can learn more about Iceberg tables in Spark: DDL commands : CREATE , ALTER , and DROP Querying data : SELECT queries and metadata tables Writing data : INSERT INTO and MERGE INTO Maintaining tables with stored procedures","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"The latest version of Iceberg is 0.12.1 . Spark is currently the most feature-rich compute engine for Iceberg operations. We recommend you to get started with Spark to understand Iceberg concepts and features with examples. You can also view documentations of using Iceberg with other compute engine under the Engines tab.","title":"Getting Started"},{"location":"getting-started/#using-iceberg-in-spark-3","text":"To use Iceberg in a Spark shell, use the --packages option: spark-shell --packages org.apache.iceberg:iceberg-spark3-runtime:0.12.1 Note If you want to include Iceberg in your Spark installation, add the iceberg-spark3-runtime Jar to Spark\u2019s jars folder.","title":"Using Iceberg in Spark 3"},{"location":"getting-started/#adding-catalogs","text":"Iceberg comes with catalogs that enable SQL commands to manage tables and load them by name. Catalogs are configured using properties under spark.sql.catalog.(catalog_name) . This command creates a path-based catalog named local for tables under $PWD/warehouse and adds support for Iceberg tables to Spark\u2019s built-in catalog: spark-sql --packages org.apache.iceberg:iceberg-spark3-runtime:0.12.1\\ --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \\ --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog \\ --conf spark.sql.catalog.spark_catalog.type=hive \\ --conf spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog \\ --conf spark.sql.catalog.local.type=hadoop \\ --conf spark.sql.catalog.local.warehouse=$PWD/warehouse","title":"Adding catalogs"},{"location":"getting-started/#creating-a-table","text":"To create your first Iceberg table in Spark, use the spark-sql shell or spark.sql(...) to run a CREATE TABLE command: -- local is the path-based catalog defined above CREATE TABLE local.db.table (id bigint, data string) USING iceberg Iceberg catalogs support the full range of SQL DDL commands, including: CREATE TABLE ... PARTITIONED BY CREATE TABLE ... AS SELECT ALTER TABLE DROP TABLE","title":"Creating a table"},{"location":"getting-started/#writing","text":"Once your table is created, insert data using INSERT INTO : INSERT INTO local.db.table VALUES (1, 'a'), (2, 'b'), (3, 'c'); INSERT INTO local.db.table SELECT id, data FROM source WHERE length(data) = 1; Iceberg also adds row-level SQL updates to Spark, MERGE INTO and DELETE FROM : MERGE INTO local.db.target t USING (SELECT * FROM updates) u ON t.id = u.id WHEN MATCHED THEN UPDATE SET t.count = t.count + u.count WHEN NOT MATCHED THEN INSERT * Iceberg supports writing DataFrames using the new v2 DataFrame write API : spark.table(\"source\").select(\"id\", \"data\") .writeTo(\"local.db.table\").append() The old write API is supported, but not recommended.","title":"Writing"},{"location":"getting-started/#reading","text":"To read with SQL, use the an Iceberg table name in a SELECT query: SELECT count(1) as count, data FROM local.db.table GROUP BY data SQL is also the recommended way to inspect tables . To view all of the snapshots in a table, use the snapshots metadata table: SELECT * FROM local.db.table.snapshots +-------------------------+----------------+-----------+-----------+----------------------------------------------------+-----+ | committed_at | snapshot_id | parent_id | operation | manifest_list | ... | +-------------------------+----------------+-----------+-----------+----------------------------------------------------+-----+ | 2019-02-08 03:29:51.215 | 57897183625154 | null | append | s3://.../table/metadata/snap-57897183625154-1.avro | ... | | | | | | | ... | | | | | | | ... | | ... | ... | ... | ... | ... | ... | +-------------------------+----------------+-----------+-----------+----------------------------------------------------+-----+ DataFrame reads are supported and can now reference tables by name using spark.table : val df = spark.table(\"local.db.table\") df.count()","title":"Reading"},{"location":"getting-started/#next-steps","text":"Next, you can learn more about Iceberg tables in Spark: DDL commands : CREATE , ALTER , and DROP Querying data : SELECT queries and metadata tables Writing data : INSERT INTO and MERGE INTO Maintaining tables with stored procedures","title":"Next steps"},{"location":"hive/","text":"Hive \u00b6 Iceberg supports reading and writing Iceberg tables through Hive by using a StorageHandler . Here is the current compatibility matrix for Iceberg Hive support: Feature Hive 2.x Hive 3.1.2 CREATE EXTERNAL TABLE \u2714\ufe0f \u2714\ufe0f CREATE TABLE \u2714\ufe0f \u2714\ufe0f DROP TABLE \u2714\ufe0f \u2714\ufe0f SELECT \u2714\ufe0f (MapReduce and Tez) \u2714\ufe0f (MapReduce and Tez) INSERT INTO \u2714\ufe0f (MapReduce only)\ufe0f \u2714\ufe0f (MapReduce only) Enabling Iceberg support in Hive \u00b6 Loading runtime jar \u00b6 To enable Iceberg support in Hive, the HiveIcebergStorageHandler and supporting classes need to be made available on Hive\u2019s classpath. These are provided by the iceberg-hive-runtime jar file. For example, if using the Hive shell, this can be achieved by issuing a statement like so: add jar /path/to/iceberg-hive-runtime.jar; There are many others ways to achieve this including adding the jar file to Hive\u2019s auxiliary classpath so it is available by default. Please refer to Hive\u2019s documentation for more information. Enabling support \u00b6 If the Iceberg storage handler is not in Hive\u2019s classpath, then Hive cannot load or update the metadata for an Iceberg table when the storage handler is set. To avoid the appearance of broken tables in Hive, Iceberg will not add the storage handler to a table unless Hive support is enabled. The storage handler is kept in sync (added or removed) every time Hive engine support for the table is updated, i.e. turned on or off in the table properties. There are two ways to enable Hive support: globally in Hadoop Configuration and per-table using a table property. Hadoop configuration \u00b6 To enable Hive support globally for an application, set iceberg.engine.hive.enabled=true in its Hadoop configuration. For example, setting this in the hive-site.xml loaded by Spark will enable the storage handler for all tables created by Spark. Warning Starting with Apache Iceberg 0.11.0 , when using Hive with Tez you also have to disable vectorization ( hive.vectorized.execution.enabled=false ). Table property configuration \u00b6 Alternatively, the property engine.hive.enabled can be set to true and added to the table properties when creating the Iceberg table. Here is an example of doing it programmatically: Catalog catalog = ...; Map<String, String> tableProperties = Maps.newHashMap(); tableProperties.put(TableProperties.ENGINE_HIVE_ENABLED, \"true\"); // engine.hive.enabled=true catalog.createTable(tableId, schema, spec, tableProperties); The table level configuration overrides the global Hadoop configuration. Catalog Management \u00b6 Global Hive catalog \u00b6 From the Hive engine\u2019s perspective, there is only one global data catalog that is defined in the Hadoop configuration in the runtime environment. In contrast, Iceberg supports multiple different data catalog types such as Hive, Hadoop, AWS Glue, or custom catalog implementations. Iceberg also allows loading a table directly based on its path in the file system. Those tables do not belong to any catalog. Users might want to read these cross-catalog and path-based tables through the Hive engine for use cases like join. To support this, a table in the Hive metastore can represent three different ways of loading an Iceberg table, depending on the table\u2019s iceberg.catalog property: The table will be loaded using a HiveCatalog that corresponds to the metastore configured in the Hive environment if no iceberg.catalog is set The table will be loaded using a custom catalog if iceberg.catalog is set to a catalog name (see below) The table can be loaded directly using the table\u2019s root location if iceberg.catalog is set to location_based_table For cases 2 and 3 above, users can create an overlay of an Iceberg table in the Hive metastore, so that different table types can work together in the same Hive environment. See CREATE EXTERNAL TABLE and CREATE TABLE for more details. Custom Iceberg catalogs \u00b6 To globally register different catalogs, set the following Hadoop configurations: Config Key Description iceberg.catalog.<catalog_name>.type type of catalog: hive , hadoop , or left unset if using a custom catalog iceberg.catalog.<catalog_name>.catalog-impl catalog implementation, must not be null if type is empty iceberg.catalog.<catalog_name>.<key> any config key and value pairs for the catalog Here are some examples using Hive CLI: Register a HiveCatalog called another_hive : SET iceberg.catalog.another_hive.type=hive; SET iceberg.catalog.another_hive.uri=thrift://example.com:9083; SET iceberg.catalog.another_hive.clients=10; SET iceberg.catalog.another_hive.warehouse=hdfs://example.com:8020/warehouse; Register a HadoopCatalog called hadoop : SET iceberg.catalog.hadoop.type=hadoop; SET iceberg.catalog.hadoop.warehouse=hdfs://example.com:8020/warehouse; Register an AWS GlueCatalog called glue : SET iceberg.catalog.glue.catalog-impl=org.apache.iceberg.aws.GlueCatalog; SET iceberg.catalog.glue.warehouse=s3://my-bucket/my/key/prefix; SET iceberg.catalog.glue.lock-impl=org.apache.iceberg.aws.glue.DynamoLockManager; SET iceberg.catalog.glue.lock.table=myGlueLockTable; DDL Commands \u00b6 CREATE EXTERNAL TABLE \u00b6 The CREATE EXTERNAL TABLE command is used to overlay a Hive table \u201con top of\u201d an existing Iceberg table. Iceberg tables are created using either a Catalog , or an implementation of the Tables interface, and Hive needs to be configured accordingly to operate on these different types of table. Hive catalog tables \u00b6 As described before, tables created by the HiveCatalog with Hive engine feature enabled are directly visible by the Hive engine, so there is no need to create an overlay. Custom catalog tables \u00b6 For a table in a registered catalog, specify the catalog name in the statement using table property iceberg.catalog . For example, the SQL below creates an overlay for a table in a hadoop type catalog named hadoop_cat : SET iceberg.catalog.hadoop_cat.type=hadoop; SET iceberg.catalog.hadoop_cat.warehouse=hdfs://example.com:8020/hadoop_cat; CREATE EXTERNAL TABLE database_a.table_a STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' TBLPROPERTIES ('iceberg.catalog'='hadoop_cat'); When iceberg.catalog is missing from both table properties and the global Hadoop configuration, HiveCatalog will be used as default. Path-based Hadoop tables \u00b6 Iceberg tables created using HadoopTables are stored entirely in a directory in a filesystem like HDFS. These tables are considered to have no catalog. To indicate that, set iceberg.catalog property to location_based_table . For example: CREATE EXTERNAL TABLE table_a STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' LOCATION 'hdfs://some_bucket/some_path/table_a' TBLPROPERTIES ('iceberg.catalog'='location_based_table'); CREATE TABLE \u00b6 Hive also supports directly creating a new Iceberg table through CREATE TABLE statement. For example: CREATE TABLE database_a.table_a ( id bigint, name string ) PARTITIONED BY ( dept string ) STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler'; Note to Hive, the table appears to be unpartitioned although the underlying Iceberg table is partitioned. Note Due to the limitation of Hive PARTITIONED BY syntax, if you use Hive CREATE TABLE , currently you can only partition by columns, which is translated to Iceberg identity partition transform. You cannot partition by other Iceberg partition transforms such as days(timestamp) . To create table with all partition transforms, you need to create the table with other engines like Spark or Flink. Custom catalog table \u00b6 You can also create a new table that is managed by a custom catalog. For example, the following code creates a table in a custom Hadoop catalog: SET iceberg.catalog.hadoop_cat.type=hadoop; SET iceberg.catalog.hadoop_cat.warehouse=hdfs://example.com:8020/hadoop_cat; CREATE TABLE database_a.table_a ( id bigint, name string ) PARTITIONED BY ( dept string ) STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' TBLPROPERTIES ('iceberg.catalog'='hadoop_cat'); Warning If the table to create already exists in the custom catalog, this will create a managed overlay table. This means technically you can omit the EXTERNAL keyword when creating an overlay table. However, this is not recommended because creating managed overlay tables could pose a risk to the shared data files in case of accidental drop table commands from the Hive side, which would unintentionally remove all the data in the table. DROP TABLE \u00b6 Tables can be dropped using the DROP TABLE command: DROP TABLE [IF EXISTS] table_name [PURGE]; You can configure purge behavior through global Hadoop configuration or Hive metastore table properties: Config key Default Description external.table.purge true if all data and metadata should be purged in a table by default Each Iceberg table\u2019s default purge behavior can also be configured through Iceberg table properties: Property Default Description gc.enabled true if all data and metadata should be purged in the table by default When changing gc.enabled on the Iceberg table via UpdateProperties , external.table.purge is also updated on HMS table accordingly. When setting external.table.purge as a table prop during Hive CREATE TABLE , gc.enabled is pushed down accordingly to the Iceberg table properties. This makes sure that the 2 properties are always consistent at table level between Hive and Iceberg. Warning Changing external.table.purge via Hive ALTER TABLE SET TBLPROPERTIES does not update gc.enabled on the Iceberg table. This is a limitation on Hive 3.1.2 because the HiveMetaHook doesn\u2019t have all the hooks for alter tables yet. Querying with SQL \u00b6 Here are the features highlights for Iceberg Hive read support: Predicate pushdown : Pushdown of the Hive SQL WHERE clause has been implemented so that these filters are used at the Iceberg TableScan level as well as by the Parquet and ORC Readers. Column projection : Columns from the Hive SQL SELECT clause are projected down to the Iceberg readers to reduce the number of columns read. Hive query engines : Both the MapReduce and Tez query execution engines are supported. Configurations \u00b6 Here are the Hadoop configurations that one can adjust for the Hive reader: Config key Default Description iceberg.mr.reuse.containers false if Avro reader should reuse containers iceberg.mr.case.sensitive true if the query is case-sensitive SELECT \u00b6 You should now be able to issue Hive SQL SELECT queries and see the results returned from the underlying Iceberg table, for example: SELECT * from table_a; Writing with SQL \u00b6 Configurations \u00b6 Here are the Hadoop configurations that one can adjust for the Hive writer: Config key Default Description iceberg.mr.commit.table.thread.pool.size 10 the number of threads of a shared thread pool to execute parallel commits for output tables iceberg.mr.commit.file.thread.pool.size 10 the number of threads of a shared thread pool to execute parallel commits for files in each output table INSERT INTO \u00b6 Hive supports the standard single-table INSERT INTO operation: INSERT INTO table_a VALUES ('a', 1); INSERT INTO table_a SELECT ...; Multi-table insert is also supported, but it will not be atomic and are committed one table at a time. Partial changes will be visible during the commit process and failures can leave partial changes committed. Changes within a single table will remain atomic. Here is an example of inserting into multiple tables at once in Hive SQL: FROM customers INSERT INTO target1 SELECT customer_id, first_name INSERT INTO target2 SELECT last_name, customer_id; Type compatibility \u00b6 Hive and Iceberg support different set of types. Iceberg can perform type conversion automatically, but not for all combinations, so you may want to understand the type conversion in Iceberg in prior to design the types of columns in your tables. You can enable auto-conversion through Hadoop configuration (not enabled by default): Config key Default Description iceberg.mr.schema.auto.conversion false if Hive should perform type auto-conversion Hive type to Iceberg type \u00b6 This type conversion table describes how Hive types are converted to the Iceberg types. The conversion applies on both creating Iceberg table and writing to Iceberg table via Hive. Hive Iceberg Notes boolean boolean short integer auto-conversion byte integer auto-conversion integer integer long long float float double double date date timestamp timestamp without timezone timestamplocaltz timestamp with timezone Hive 3 only interval_year_month not supported interval_day_time not supported char string auto-conversion varchar string auto-conversion string string binary binary decimal decimal struct struct list list map map union not supported","title":"Hive"},{"location":"hive/#hive","text":"Iceberg supports reading and writing Iceberg tables through Hive by using a StorageHandler . Here is the current compatibility matrix for Iceberg Hive support: Feature Hive 2.x Hive 3.1.2 CREATE EXTERNAL TABLE \u2714\ufe0f \u2714\ufe0f CREATE TABLE \u2714\ufe0f \u2714\ufe0f DROP TABLE \u2714\ufe0f \u2714\ufe0f SELECT \u2714\ufe0f (MapReduce and Tez) \u2714\ufe0f (MapReduce and Tez) INSERT INTO \u2714\ufe0f (MapReduce only)\ufe0f \u2714\ufe0f (MapReduce only)","title":"Hive"},{"location":"hive/#enabling-iceberg-support-in-hive","text":"","title":"Enabling Iceberg support in Hive"},{"location":"hive/#loading-runtime-jar","text":"To enable Iceberg support in Hive, the HiveIcebergStorageHandler and supporting classes need to be made available on Hive\u2019s classpath. These are provided by the iceberg-hive-runtime jar file. For example, if using the Hive shell, this can be achieved by issuing a statement like so: add jar /path/to/iceberg-hive-runtime.jar; There are many others ways to achieve this including adding the jar file to Hive\u2019s auxiliary classpath so it is available by default. Please refer to Hive\u2019s documentation for more information.","title":"Loading runtime jar"},{"location":"hive/#enabling-support","text":"If the Iceberg storage handler is not in Hive\u2019s classpath, then Hive cannot load or update the metadata for an Iceberg table when the storage handler is set. To avoid the appearance of broken tables in Hive, Iceberg will not add the storage handler to a table unless Hive support is enabled. The storage handler is kept in sync (added or removed) every time Hive engine support for the table is updated, i.e. turned on or off in the table properties. There are two ways to enable Hive support: globally in Hadoop Configuration and per-table using a table property.","title":"Enabling support"},{"location":"hive/#hadoop-configuration","text":"To enable Hive support globally for an application, set iceberg.engine.hive.enabled=true in its Hadoop configuration. For example, setting this in the hive-site.xml loaded by Spark will enable the storage handler for all tables created by Spark. Warning Starting with Apache Iceberg 0.11.0 , when using Hive with Tez you also have to disable vectorization ( hive.vectorized.execution.enabled=false ).","title":"Hadoop configuration"},{"location":"hive/#table-property-configuration","text":"Alternatively, the property engine.hive.enabled can be set to true and added to the table properties when creating the Iceberg table. Here is an example of doing it programmatically: Catalog catalog = ...; Map<String, String> tableProperties = Maps.newHashMap(); tableProperties.put(TableProperties.ENGINE_HIVE_ENABLED, \"true\"); // engine.hive.enabled=true catalog.createTable(tableId, schema, spec, tableProperties); The table level configuration overrides the global Hadoop configuration.","title":"Table property configuration"},{"location":"hive/#catalog-management","text":"","title":"Catalog Management"},{"location":"hive/#global-hive-catalog","text":"From the Hive engine\u2019s perspective, there is only one global data catalog that is defined in the Hadoop configuration in the runtime environment. In contrast, Iceberg supports multiple different data catalog types such as Hive, Hadoop, AWS Glue, or custom catalog implementations. Iceberg also allows loading a table directly based on its path in the file system. Those tables do not belong to any catalog. Users might want to read these cross-catalog and path-based tables through the Hive engine for use cases like join. To support this, a table in the Hive metastore can represent three different ways of loading an Iceberg table, depending on the table\u2019s iceberg.catalog property: The table will be loaded using a HiveCatalog that corresponds to the metastore configured in the Hive environment if no iceberg.catalog is set The table will be loaded using a custom catalog if iceberg.catalog is set to a catalog name (see below) The table can be loaded directly using the table\u2019s root location if iceberg.catalog is set to location_based_table For cases 2 and 3 above, users can create an overlay of an Iceberg table in the Hive metastore, so that different table types can work together in the same Hive environment. See CREATE EXTERNAL TABLE and CREATE TABLE for more details.","title":"Global Hive catalog"},{"location":"hive/#custom-iceberg-catalogs","text":"To globally register different catalogs, set the following Hadoop configurations: Config Key Description iceberg.catalog.<catalog_name>.type type of catalog: hive , hadoop , or left unset if using a custom catalog iceberg.catalog.<catalog_name>.catalog-impl catalog implementation, must not be null if type is empty iceberg.catalog.<catalog_name>.<key> any config key and value pairs for the catalog Here are some examples using Hive CLI: Register a HiveCatalog called another_hive : SET iceberg.catalog.another_hive.type=hive; SET iceberg.catalog.another_hive.uri=thrift://example.com:9083; SET iceberg.catalog.another_hive.clients=10; SET iceberg.catalog.another_hive.warehouse=hdfs://example.com:8020/warehouse; Register a HadoopCatalog called hadoop : SET iceberg.catalog.hadoop.type=hadoop; SET iceberg.catalog.hadoop.warehouse=hdfs://example.com:8020/warehouse; Register an AWS GlueCatalog called glue : SET iceberg.catalog.glue.catalog-impl=org.apache.iceberg.aws.GlueCatalog; SET iceberg.catalog.glue.warehouse=s3://my-bucket/my/key/prefix; SET iceberg.catalog.glue.lock-impl=org.apache.iceberg.aws.glue.DynamoLockManager; SET iceberg.catalog.glue.lock.table=myGlueLockTable;","title":"Custom Iceberg catalogs"},{"location":"hive/#ddl-commands","text":"","title":"DDL Commands"},{"location":"hive/#create-external-table","text":"The CREATE EXTERNAL TABLE command is used to overlay a Hive table \u201con top of\u201d an existing Iceberg table. Iceberg tables are created using either a Catalog , or an implementation of the Tables interface, and Hive needs to be configured accordingly to operate on these different types of table.","title":"CREATE EXTERNAL TABLE"},{"location":"hive/#hive-catalog-tables","text":"As described before, tables created by the HiveCatalog with Hive engine feature enabled are directly visible by the Hive engine, so there is no need to create an overlay.","title":"Hive catalog tables"},{"location":"hive/#custom-catalog-tables","text":"For a table in a registered catalog, specify the catalog name in the statement using table property iceberg.catalog . For example, the SQL below creates an overlay for a table in a hadoop type catalog named hadoop_cat : SET iceberg.catalog.hadoop_cat.type=hadoop; SET iceberg.catalog.hadoop_cat.warehouse=hdfs://example.com:8020/hadoop_cat; CREATE EXTERNAL TABLE database_a.table_a STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' TBLPROPERTIES ('iceberg.catalog'='hadoop_cat'); When iceberg.catalog is missing from both table properties and the global Hadoop configuration, HiveCatalog will be used as default.","title":"Custom catalog tables"},{"location":"hive/#path-based-hadoop-tables","text":"Iceberg tables created using HadoopTables are stored entirely in a directory in a filesystem like HDFS. These tables are considered to have no catalog. To indicate that, set iceberg.catalog property to location_based_table . For example: CREATE EXTERNAL TABLE table_a STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' LOCATION 'hdfs://some_bucket/some_path/table_a' TBLPROPERTIES ('iceberg.catalog'='location_based_table');","title":"Path-based Hadoop tables"},{"location":"hive/#create-table","text":"Hive also supports directly creating a new Iceberg table through CREATE TABLE statement. For example: CREATE TABLE database_a.table_a ( id bigint, name string ) PARTITIONED BY ( dept string ) STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler'; Note to Hive, the table appears to be unpartitioned although the underlying Iceberg table is partitioned. Note Due to the limitation of Hive PARTITIONED BY syntax, if you use Hive CREATE TABLE , currently you can only partition by columns, which is translated to Iceberg identity partition transform. You cannot partition by other Iceberg partition transforms such as days(timestamp) . To create table with all partition transforms, you need to create the table with other engines like Spark or Flink.","title":"CREATE TABLE"},{"location":"hive/#custom-catalog-table","text":"You can also create a new table that is managed by a custom catalog. For example, the following code creates a table in a custom Hadoop catalog: SET iceberg.catalog.hadoop_cat.type=hadoop; SET iceberg.catalog.hadoop_cat.warehouse=hdfs://example.com:8020/hadoop_cat; CREATE TABLE database_a.table_a ( id bigint, name string ) PARTITIONED BY ( dept string ) STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' TBLPROPERTIES ('iceberg.catalog'='hadoop_cat'); Warning If the table to create already exists in the custom catalog, this will create a managed overlay table. This means technically you can omit the EXTERNAL keyword when creating an overlay table. However, this is not recommended because creating managed overlay tables could pose a risk to the shared data files in case of accidental drop table commands from the Hive side, which would unintentionally remove all the data in the table.","title":"Custom catalog table"},{"location":"hive/#drop-table","text":"Tables can be dropped using the DROP TABLE command: DROP TABLE [IF EXISTS] table_name [PURGE]; You can configure purge behavior through global Hadoop configuration or Hive metastore table properties: Config key Default Description external.table.purge true if all data and metadata should be purged in a table by default Each Iceberg table\u2019s default purge behavior can also be configured through Iceberg table properties: Property Default Description gc.enabled true if all data and metadata should be purged in the table by default When changing gc.enabled on the Iceberg table via UpdateProperties , external.table.purge is also updated on HMS table accordingly. When setting external.table.purge as a table prop during Hive CREATE TABLE , gc.enabled is pushed down accordingly to the Iceberg table properties. This makes sure that the 2 properties are always consistent at table level between Hive and Iceberg. Warning Changing external.table.purge via Hive ALTER TABLE SET TBLPROPERTIES does not update gc.enabled on the Iceberg table. This is a limitation on Hive 3.1.2 because the HiveMetaHook doesn\u2019t have all the hooks for alter tables yet.","title":"DROP TABLE"},{"location":"hive/#querying-with-sql","text":"Here are the features highlights for Iceberg Hive read support: Predicate pushdown : Pushdown of the Hive SQL WHERE clause has been implemented so that these filters are used at the Iceberg TableScan level as well as by the Parquet and ORC Readers. Column projection : Columns from the Hive SQL SELECT clause are projected down to the Iceberg readers to reduce the number of columns read. Hive query engines : Both the MapReduce and Tez query execution engines are supported.","title":"Querying with SQL"},{"location":"hive/#configurations","text":"Here are the Hadoop configurations that one can adjust for the Hive reader: Config key Default Description iceberg.mr.reuse.containers false if Avro reader should reuse containers iceberg.mr.case.sensitive true if the query is case-sensitive","title":"Configurations"},{"location":"hive/#select","text":"You should now be able to issue Hive SQL SELECT queries and see the results returned from the underlying Iceberg table, for example: SELECT * from table_a;","title":"SELECT"},{"location":"hive/#writing-with-sql","text":"","title":"Writing with SQL"},{"location":"hive/#configurations_1","text":"Here are the Hadoop configurations that one can adjust for the Hive writer: Config key Default Description iceberg.mr.commit.table.thread.pool.size 10 the number of threads of a shared thread pool to execute parallel commits for output tables iceberg.mr.commit.file.thread.pool.size 10 the number of threads of a shared thread pool to execute parallel commits for files in each output table","title":"Configurations"},{"location":"hive/#insert-into","text":"Hive supports the standard single-table INSERT INTO operation: INSERT INTO table_a VALUES ('a', 1); INSERT INTO table_a SELECT ...; Multi-table insert is also supported, but it will not be atomic and are committed one table at a time. Partial changes will be visible during the commit process and failures can leave partial changes committed. Changes within a single table will remain atomic. Here is an example of inserting into multiple tables at once in Hive SQL: FROM customers INSERT INTO target1 SELECT customer_id, first_name INSERT INTO target2 SELECT last_name, customer_id;","title":"INSERT INTO"},{"location":"hive/#type-compatibility","text":"Hive and Iceberg support different set of types. Iceberg can perform type conversion automatically, but not for all combinations, so you may want to understand the type conversion in Iceberg in prior to design the types of columns in your tables. You can enable auto-conversion through Hadoop configuration (not enabled by default): Config key Default Description iceberg.mr.schema.auto.conversion false if Hive should perform type auto-conversion","title":"Type compatibility"},{"location":"hive/#hive-type-to-iceberg-type","text":"This type conversion table describes how Hive types are converted to the Iceberg types. The conversion applies on both creating Iceberg table and writing to Iceberg table via Hive. Hive Iceberg Notes boolean boolean short integer auto-conversion byte integer auto-conversion integer integer long long float float double double date date timestamp timestamp without timezone timestamplocaltz timestamp with timezone Hive 3 only interval_year_month not supported interval_day_time not supported char string auto-conversion varchar string auto-conversion string string binary binary decimal decimal struct struct list list map map union not supported","title":"Hive type to Iceberg type"},{"location":"how-to-release/","text":"Setup \u00b6 To create a release candidate, you will need: Apache LDAP credentals for Nexus and SVN A GPG key for signing , published in KEYS Nexus access \u00b6 Nexus credentials are configured in your personal ~/.gradle/gradle.properties file using mavenUser and mavenPassword : mavenUser=yourApacheID mavenPassword=SomePassword PGP signing \u00b6 The release scripts use the command-line gpg utility so that signing can use the gpg-agent and does not require writing your private key\u2019s passphrase to a configuration file. To configure gradle to sign convenience binary artifacts, add the following settings to ~/.gradle/gradle.properties : signing.gnupg.keyName=Your Name (CODE SIGNING KEY) To use gpg instead of gpg2 , also set signing.gnupg.executable=gpg For more information, see the Gradle signing documentation . Creating a release candidate \u00b6 Build the source release \u00b6 To create the source release artifacts, run the source-release.sh script with the release version and release candidate number: dev/source-release.sh 0.8.1 0 Preparing source for apache-iceberg-0.8.1-rc0 ... Success! The release candidate is available here: https://dist.apache.org/repos/dist/dev/iceberg/apache-iceberg-0.8.1-rc0/ Commit SHA1: 4b4716c76559b3cdf3487e6b60ab52950241989b The source release script will create a candidate tag based on the HEAD revision in git and will prepare the release tarball, signature, and checksum files. It will also upload the source artifacts to SVN. Note the commit SHA1 and candidate location because those will be added to the vote thread. Once the source release is ready, use it to stage convenience binary artifacts in Nexus. Build and stage convenience binaries \u00b6 Convenience binaries are created using the source release tarball from in the last step. Untar the source release and go into the release directory: tar xzf apache-iceberg-0.8.1.tar.gz cd apache-iceberg-0.8.1 To build and publish the convenience binaries, run the dev/stage-binaries.sh script. This will push to a release staging repository. dev/stage-binaries.sh Next, you need to close the staging repository: Go to Nexus and log in In the menu on the left, choose \u201cStaging Repositories\u201d Select the Iceberg repository At the top, select \u201cClose\u201d and follow the instructions In the comment field use \u201cApache Iceberg <version> RC<num>\u201c Start a VOTE thread \u00b6 The last step for a candidate is to create a VOTE thread on the dev mailing list. Subject: [VOTE] Release Apache Iceberg <VERSION> RC<NUM> Hi everyone, I propose the following RC to be released as official Apache Iceberg <VERSION> release. The commit id is <SHA1> * This corresponds to the tag: apache-iceberg-<VERSION>-rc<NUM> * https://github.com/apache/iceberg/commits/apache-iceberg-<VERSION>-rc<NUM> * https://github.com/apache/iceberg/tree/<SHA1> The release tarball, signature, and checksums are here: * https://dist.apache.org/repos/dist/dev/iceberg/apache-iceberg-<VERSION>-rc<NUM>/ You can find the KEYS file here: * https://dist.apache.org/repos/dist/dev/iceberg/KEYS Convenience binary artifacts are staged in Nexus. The Maven repository URL is: * https://repository.apache.org/content/repositories/orgapacheiceberg-<ID>/ This release includes important changes that I should have summarized here, but I'm lazy. Please download, verify, and test. Please vote in the next 72 hours. [ ] +1 Release this as Apache Iceberg <VERSION> [ ] +0 [ ] -1 Do not release this because... When a candidate is passed or rejected, reply with the voting result: Subject: [RESULT][VOTE] Release Apache Iceberg <VERSION> RC<NUM> Thanks everyone who participated in the vote for Release Apache Iceberg <VERSION> RC<NUM>. The vote result is: +1: 3 (binding), 5 (non-binding) +0: 0 (binding), 0 (non-binding) -1: 0 (binding), 0 (non-binding) Therefore, the release candidate is passed/rejected. Finishing the release \u00b6 After the release vote has passed, you need to release the last candidate\u2019s artifacts. First, copy the source release directory to releases: mkdir iceberg cd iceberg svn co https://dist.apache.org/repos/dist/dev/iceberg candidates svn co https://dist.apache.org/repos/dist/release/iceberg releases cp -r candidates/apache-iceberg-<VERSION>-rcN/ releases/apache-iceberg-<VERSION> cd releases svn add apache-iceberg-<VERSION> svn ci -m 'Iceberg: Add release <VERSION>' Next, add a release tag to the git repository based on the passing candidate tag: git tag -am 'Release Apache Iceberg <VERSION>' apache-iceberg-<VERSION> apache-iceberg-<VERSION>-rcN Then release the candidate repository in Nexus . To announce the release, wait until Maven central has mirrored the Apache binaries, then update the Iceberg site and send an announcement email: [ANNOUNCE] Apache Iceberg release <VERSION> I'm please to announce the release of Apache Iceberg <VERSION>! Apache Iceberg is an open table format for huge analytic datasets. Iceberg delivers high query performance for tables with tens of petabytes of data, along with atomic commits, concurrent writes, and SQL-compatible table evolution. This release can be downloaded from: https://www.apache.org/dyn/closer.cgi/iceberg/<TARBALL NAME WITHOUT .tar.gz>/<TARBALL NAME> Java artifacts are available from Maven Central. Thanks to everyone for contributing!","title":"How To Release"},{"location":"how-to-release/#setup","text":"To create a release candidate, you will need: Apache LDAP credentals for Nexus and SVN A GPG key for signing , published in KEYS","title":"Setup"},{"location":"how-to-release/#nexus-access","text":"Nexus credentials are configured in your personal ~/.gradle/gradle.properties file using mavenUser and mavenPassword : mavenUser=yourApacheID mavenPassword=SomePassword","title":"Nexus access"},{"location":"how-to-release/#pgp-signing","text":"The release scripts use the command-line gpg utility so that signing can use the gpg-agent and does not require writing your private key\u2019s passphrase to a configuration file. To configure gradle to sign convenience binary artifacts, add the following settings to ~/.gradle/gradle.properties : signing.gnupg.keyName=Your Name (CODE SIGNING KEY) To use gpg instead of gpg2 , also set signing.gnupg.executable=gpg For more information, see the Gradle signing documentation .","title":"PGP signing"},{"location":"how-to-release/#creating-a-release-candidate","text":"","title":"Creating a release candidate"},{"location":"how-to-release/#build-the-source-release","text":"To create the source release artifacts, run the source-release.sh script with the release version and release candidate number: dev/source-release.sh 0.8.1 0 Preparing source for apache-iceberg-0.8.1-rc0 ... Success! The release candidate is available here: https://dist.apache.org/repos/dist/dev/iceberg/apache-iceberg-0.8.1-rc0/ Commit SHA1: 4b4716c76559b3cdf3487e6b60ab52950241989b The source release script will create a candidate tag based on the HEAD revision in git and will prepare the release tarball, signature, and checksum files. It will also upload the source artifacts to SVN. Note the commit SHA1 and candidate location because those will be added to the vote thread. Once the source release is ready, use it to stage convenience binary artifacts in Nexus.","title":"Build the source release"},{"location":"how-to-release/#build-and-stage-convenience-binaries","text":"Convenience binaries are created using the source release tarball from in the last step. Untar the source release and go into the release directory: tar xzf apache-iceberg-0.8.1.tar.gz cd apache-iceberg-0.8.1 To build and publish the convenience binaries, run the dev/stage-binaries.sh script. This will push to a release staging repository. dev/stage-binaries.sh Next, you need to close the staging repository: Go to Nexus and log in In the menu on the left, choose \u201cStaging Repositories\u201d Select the Iceberg repository At the top, select \u201cClose\u201d and follow the instructions In the comment field use \u201cApache Iceberg <version> RC<num>\u201c","title":"Build and stage convenience binaries"},{"location":"how-to-release/#start-a-vote-thread","text":"The last step for a candidate is to create a VOTE thread on the dev mailing list. Subject: [VOTE] Release Apache Iceberg <VERSION> RC<NUM> Hi everyone, I propose the following RC to be released as official Apache Iceberg <VERSION> release. The commit id is <SHA1> * This corresponds to the tag: apache-iceberg-<VERSION>-rc<NUM> * https://github.com/apache/iceberg/commits/apache-iceberg-<VERSION>-rc<NUM> * https://github.com/apache/iceberg/tree/<SHA1> The release tarball, signature, and checksums are here: * https://dist.apache.org/repos/dist/dev/iceberg/apache-iceberg-<VERSION>-rc<NUM>/ You can find the KEYS file here: * https://dist.apache.org/repos/dist/dev/iceberg/KEYS Convenience binary artifacts are staged in Nexus. The Maven repository URL is: * https://repository.apache.org/content/repositories/orgapacheiceberg-<ID>/ This release includes important changes that I should have summarized here, but I'm lazy. Please download, verify, and test. Please vote in the next 72 hours. [ ] +1 Release this as Apache Iceberg <VERSION> [ ] +0 [ ] -1 Do not release this because... When a candidate is passed or rejected, reply with the voting result: Subject: [RESULT][VOTE] Release Apache Iceberg <VERSION> RC<NUM> Thanks everyone who participated in the vote for Release Apache Iceberg <VERSION> RC<NUM>. The vote result is: +1: 3 (binding), 5 (non-binding) +0: 0 (binding), 0 (non-binding) -1: 0 (binding), 0 (non-binding) Therefore, the release candidate is passed/rejected.","title":"Start a VOTE thread"},{"location":"how-to-release/#finishing-the-release","text":"After the release vote has passed, you need to release the last candidate\u2019s artifacts. First, copy the source release directory to releases: mkdir iceberg cd iceberg svn co https://dist.apache.org/repos/dist/dev/iceberg candidates svn co https://dist.apache.org/repos/dist/release/iceberg releases cp -r candidates/apache-iceberg-<VERSION>-rcN/ releases/apache-iceberg-<VERSION> cd releases svn add apache-iceberg-<VERSION> svn ci -m 'Iceberg: Add release <VERSION>' Next, add a release tag to the git repository based on the passing candidate tag: git tag -am 'Release Apache Iceberg <VERSION>' apache-iceberg-<VERSION> apache-iceberg-<VERSION>-rcN Then release the candidate repository in Nexus . To announce the release, wait until Maven central has mirrored the Apache binaries, then update the Iceberg site and send an announcement email: [ANNOUNCE] Apache Iceberg release <VERSION> I'm please to announce the release of Apache Iceberg <VERSION>! Apache Iceberg is an open table format for huge analytic datasets. Iceberg delivers high query performance for tables with tens of petabytes of data, along with atomic commits, concurrent writes, and SQL-compatible table evolution. This release can be downloaded from: https://www.apache.org/dyn/closer.cgi/iceberg/<TARBALL NAME WITHOUT .tar.gz>/<TARBALL NAME> Java artifacts are available from Maven Central. Thanks to everyone for contributing!","title":"Finishing the release"},{"location":"java-api-quickstart/","text":"Java API Quickstart \u00b6 Create a table \u00b6 Tables are created using either a Catalog or an implementation of the Tables interface. Using a Hive catalog \u00b6 The Hive catalog connects to a Hive metastore to keep track of Iceberg tables. You can initialize a Hive catalog with a name and some properties. (see: Catalog properties ) Note: Currently, setConf is always required for hive catalogs, but this will change in the future. import org.apache.iceberg.hive.HiveCatalog; Catalog catalog = new HiveCatalog(); catalog.setConf(spark.sparkContext().hadoopConfiguration()); // Configure using Spark's Hadoop configuration Map <String, String> properties = new HashMap<String, String>(); properties.put(\"warehouse\", \"...\"); properties.put(\"uri\", \"...\"); catalog.initialize(\"hive\", properties); The Catalog interface defines methods for working with tables, like createTable , loadTable , renameTable , and dropTable . To create a table, pass an Identifier and a Schema along with other initial metadata: import org.apache.iceberg.Table; import org.apache.iceberg.catalog.TableIdentifier; TableIdentifier name = TableIdentifier.of(\"logging\", \"logs\"); Table table = catalog.createTable(name, schema, spec); // or to load an existing table, use the following line // Table table = catalog.loadTable(name); The logs schema and partition spec are created below. Using a Hadoop catalog \u00b6 A Hadoop catalog doesn\u2019t need to connect to a Hive MetaStore, but can only be used with HDFS or similar file systems that support atomic rename. Concurrent writes with a Hadoop catalog are not safe with a local FS or S3. To create a Hadoop catalog: import org.apache.hadoop.conf.Configuration; import org.apache.iceberg.hadoop.HadoopCatalog; Configuration conf = new Configuration(); String warehousePath = \"hdfs://host:8020/warehouse_path\"; HadoopCatalog catalog = new HadoopCatalog(conf, warehousePath); Like the Hive catalog, HadoopCatalog implements Catalog , so it also has methods for working with tables, like createTable , loadTable , and dropTable . This example creates a table with Hadoop catalog: import org.apache.iceberg.Table; import org.apache.iceberg.catalog.TableIdentifier; TableIdentifier name = TableIdentifier.of(\"logging\", \"logs\"); Table table = catalog.createTable(name, schema, spec); // or to load an existing table, use the following line // Table table = catalog.loadTable(name); The logs schema and partition spec are created below. Using Hadoop tables \u00b6 Iceberg also supports tables that are stored in a directory in HDFS. Concurrent writes with a Hadoop tables are not safe when stored in the local FS or S3. Directory tables don\u2019t support all catalog operations, like rename, so they use the Tables interface instead of Catalog . To create a table in HDFS, use HadoopTables : import org.apache.hadoop.conf.Configuration; import org.apache.iceberg.hadoop.HadoopTables; import org.apache.iceberg.Table; Configuration conf = new Configuration(); HadoopTables tables = new HadoopTables(conf); Table table = tables.create(schema, spec, table_location); // or to load an existing table, use the following line // Table table = tables.load(table_location); Warning Hadoop tables shouldn\u2019t be used with file systems that do not support atomic rename. Iceberg relies on rename to synchronize concurrent commits for directory tables. Tables in Spark \u00b6 Spark uses both HiveCatalog and HadoopTables to load tables. Hive is used when the identifier passed to load or save is not a path, otherwise Spark assumes it is a path-based table. To read and write to tables from Spark see: SQL queries in Spark INSERT INTO in Spark MERGE INTO in Spark Schemas \u00b6 Create a schema \u00b6 This example creates a schema for a logs table: import org.apache.iceberg.Schema; import org.apache.iceberg.types.Types; Schema schema = new Schema( Types.NestedField.required(1, \"level\", Types.StringType.get()), Types.NestedField.required(2, \"event_time\", Types.TimestampType.withZone()), Types.NestedField.required(3, \"message\", Types.StringType.get()), Types.NestedField.optional(4, \"call_stack\", Types.ListType.ofRequired(5, Types.StringType.get())) ); When using the Iceberg API directly, type IDs are required. Conversions from other schema formats, like Spark, Avro, and Parquet will automatically assign new IDs. When a table is created, all IDs in the schema are re-assigned to ensure uniqueness. Convert a schema from Avro \u00b6 To create an Iceberg schema from an existing Avro schema, use converters in AvroSchemaUtil : import org.apache.avro.Schema; import org.apache.avro.Schema.Parser; import org.apache.iceberg.avro.AvroSchemaUtil; Schema avroSchema = new Parser().parse(\"{\\\"type\\\": \\\"record\\\" , ... }\"); Schema icebergSchema = AvroSchemaUtil.toIceberg(avroSchema); Convert a schema from Spark \u00b6 To create an Iceberg schema from an existing table, use converters in SparkSchemaUtil : import org.apache.iceberg.spark.SparkSchemaUtil; Schema schema = SparkSchemaUtil.schemaForTable(sparkSession, table_name); Partitioning \u00b6 Create a partition spec \u00b6 Partition specs describe how Iceberg should group records into data files. Partition specs are created for a table\u2019s schema using a builder. This example creates a partition spec for the logs table that partitions records by the hour of the log event\u2019s timestamp and by log level: import org.apache.iceberg.PartitionSpec; PartitionSpec spec = PartitionSpec.builderFor(schema) .hour(\"event_time\") .identity(\"level\") .build(); For more information on the different partition transforms that Iceberg offers, visit this page .","title":"Java Quickstart"},{"location":"java-api-quickstart/#java-api-quickstart","text":"","title":"Java API Quickstart"},{"location":"java-api-quickstart/#create-a-table","text":"Tables are created using either a Catalog or an implementation of the Tables interface.","title":"Create a table"},{"location":"java-api-quickstart/#using-a-hive-catalog","text":"The Hive catalog connects to a Hive metastore to keep track of Iceberg tables. You can initialize a Hive catalog with a name and some properties. (see: Catalog properties ) Note: Currently, setConf is always required for hive catalogs, but this will change in the future. import org.apache.iceberg.hive.HiveCatalog; Catalog catalog = new HiveCatalog(); catalog.setConf(spark.sparkContext().hadoopConfiguration()); // Configure using Spark's Hadoop configuration Map <String, String> properties = new HashMap<String, String>(); properties.put(\"warehouse\", \"...\"); properties.put(\"uri\", \"...\"); catalog.initialize(\"hive\", properties); The Catalog interface defines methods for working with tables, like createTable , loadTable , renameTable , and dropTable . To create a table, pass an Identifier and a Schema along with other initial metadata: import org.apache.iceberg.Table; import org.apache.iceberg.catalog.TableIdentifier; TableIdentifier name = TableIdentifier.of(\"logging\", \"logs\"); Table table = catalog.createTable(name, schema, spec); // or to load an existing table, use the following line // Table table = catalog.loadTable(name); The logs schema and partition spec are created below.","title":"Using a Hive catalog"},{"location":"java-api-quickstart/#using-a-hadoop-catalog","text":"A Hadoop catalog doesn\u2019t need to connect to a Hive MetaStore, but can only be used with HDFS or similar file systems that support atomic rename. Concurrent writes with a Hadoop catalog are not safe with a local FS or S3. To create a Hadoop catalog: import org.apache.hadoop.conf.Configuration; import org.apache.iceberg.hadoop.HadoopCatalog; Configuration conf = new Configuration(); String warehousePath = \"hdfs://host:8020/warehouse_path\"; HadoopCatalog catalog = new HadoopCatalog(conf, warehousePath); Like the Hive catalog, HadoopCatalog implements Catalog , so it also has methods for working with tables, like createTable , loadTable , and dropTable . This example creates a table with Hadoop catalog: import org.apache.iceberg.Table; import org.apache.iceberg.catalog.TableIdentifier; TableIdentifier name = TableIdentifier.of(\"logging\", \"logs\"); Table table = catalog.createTable(name, schema, spec); // or to load an existing table, use the following line // Table table = catalog.loadTable(name); The logs schema and partition spec are created below.","title":"Using a Hadoop catalog"},{"location":"java-api-quickstart/#using-hadoop-tables","text":"Iceberg also supports tables that are stored in a directory in HDFS. Concurrent writes with a Hadoop tables are not safe when stored in the local FS or S3. Directory tables don\u2019t support all catalog operations, like rename, so they use the Tables interface instead of Catalog . To create a table in HDFS, use HadoopTables : import org.apache.hadoop.conf.Configuration; import org.apache.iceberg.hadoop.HadoopTables; import org.apache.iceberg.Table; Configuration conf = new Configuration(); HadoopTables tables = new HadoopTables(conf); Table table = tables.create(schema, spec, table_location); // or to load an existing table, use the following line // Table table = tables.load(table_location); Warning Hadoop tables shouldn\u2019t be used with file systems that do not support atomic rename. Iceberg relies on rename to synchronize concurrent commits for directory tables.","title":"Using Hadoop tables"},{"location":"java-api-quickstart/#tables-in-spark","text":"Spark uses both HiveCatalog and HadoopTables to load tables. Hive is used when the identifier passed to load or save is not a path, otherwise Spark assumes it is a path-based table. To read and write to tables from Spark see: SQL queries in Spark INSERT INTO in Spark MERGE INTO in Spark","title":"Tables in Spark"},{"location":"java-api-quickstart/#schemas","text":"","title":"Schemas"},{"location":"java-api-quickstart/#create-a-schema","text":"This example creates a schema for a logs table: import org.apache.iceberg.Schema; import org.apache.iceberg.types.Types; Schema schema = new Schema( Types.NestedField.required(1, \"level\", Types.StringType.get()), Types.NestedField.required(2, \"event_time\", Types.TimestampType.withZone()), Types.NestedField.required(3, \"message\", Types.StringType.get()), Types.NestedField.optional(4, \"call_stack\", Types.ListType.ofRequired(5, Types.StringType.get())) ); When using the Iceberg API directly, type IDs are required. Conversions from other schema formats, like Spark, Avro, and Parquet will automatically assign new IDs. When a table is created, all IDs in the schema are re-assigned to ensure uniqueness.","title":"Create a schema"},{"location":"java-api-quickstart/#convert-a-schema-from-avro","text":"To create an Iceberg schema from an existing Avro schema, use converters in AvroSchemaUtil : import org.apache.avro.Schema; import org.apache.avro.Schema.Parser; import org.apache.iceberg.avro.AvroSchemaUtil; Schema avroSchema = new Parser().parse(\"{\\\"type\\\": \\\"record\\\" , ... }\"); Schema icebergSchema = AvroSchemaUtil.toIceberg(avroSchema);","title":"Convert a schema from Avro"},{"location":"java-api-quickstart/#convert-a-schema-from-spark","text":"To create an Iceberg schema from an existing table, use converters in SparkSchemaUtil : import org.apache.iceberg.spark.SparkSchemaUtil; Schema schema = SparkSchemaUtil.schemaForTable(sparkSession, table_name);","title":"Convert a schema from Spark"},{"location":"java-api-quickstart/#partitioning","text":"","title":"Partitioning"},{"location":"java-api-quickstart/#create-a-partition-spec","text":"Partition specs describe how Iceberg should group records into data files. Partition specs are created for a table\u2019s schema using a builder. This example creates a partition spec for the logs table that partitions records by the hour of the log event\u2019s timestamp and by log level: import org.apache.iceberg.PartitionSpec; PartitionSpec spec = PartitionSpec.builderFor(schema) .hour(\"event_time\") .identity(\"level\") .build(); For more information on the different partition transforms that Iceberg offers, visit this page .","title":"Create a partition spec"},{"location":"jdbc/","text":"Iceberg JDBC Integration \u00b6 JDBC Catalog \u00b6 Iceberg supports using a table in a relational database to manage Iceberg tables through JDBC. The database that JDBC connects to must support atomic transaction to allow the JDBC catalog implementation to properly support atomic Iceberg table commits and read serializable isolation. Configurations \u00b6 Because each database and database service provider might require different configurations, the JDBC catalog allows arbitrary configurations through: Property Default Description uri the JDBC connection string jdbc.<property_key> any key value pairs to configure the JDBC connection Examples \u00b6 Spark \u00b6 You can start a Spark session with a MySQL JDBC connection using the following configurations: spark-sql --packages org.apache.iceberg:iceberg-spark3-runtime:0.12.1 \\ --conf spark.sql.catalog.my_catalog=org.apache.iceberg.spark.SparkCatalog \\ --conf spark.sql.catalog.my_catalog.warehouse=s3://my-bucket/my/key/prefix \\ --conf spark.sql.catalog.my_catalog.catalog-impl=org.apache.iceberg.jdbc.JdbcCatalog \\ --conf spark.sql.catalog.my_catalog.uri=jdbc:mysql://test.1234567890.us-west-2.rds.amazonaws.com:3306/default \\ --conf spark.sql.catalog.my_catalog.jdbc.verifyServerCertificate=true \\ --conf spark.sql.catalog.my_catalog.jdbc.useSSL=true \\ --conf spark.sql.catalog.my_catalog.jdbc.user=admin \\ --conf spark.sql.catalog.my_catalog.jdbc.password=pass Java API \u00b6 Class.forName(\"com.mysql.cj.jdbc.Driver\"); // ensure JDBC driver is at runtime classpath Map<String, String> properties = new HashMap<>(); properties.put(CatalogProperties.CATALOG_IMPL, JdbcCatalog.class.getName()); properties.put(CatalogProperties.URI, \"jdbc:mysql://localhost:3306/test\"); properties.put(JdbcCatalog.PROPERTY_PREFIX + \"user\", \"admin\"); properties.put(JdbcCatalog.PROPERTY_PREFIX + \"password\", \"pass\"); properties.put(CatalogProperties.WAREHOUSE_LOCATION, \"s3://warehouse/path\"); Configuration hadoopConf = new Configuration(); // configs if you use HadoopFileIO JdbcCatalog catalog = CatalogUtil.buildIcebergCatalog(\"test_jdbc_catalog\", properties, hadoopConf);","title":"JDBC"},{"location":"jdbc/#iceberg-jdbc-integration","text":"","title":"Iceberg JDBC Integration"},{"location":"jdbc/#jdbc-catalog","text":"Iceberg supports using a table in a relational database to manage Iceberg tables through JDBC. The database that JDBC connects to must support atomic transaction to allow the JDBC catalog implementation to properly support atomic Iceberg table commits and read serializable isolation.","title":"JDBC Catalog"},{"location":"jdbc/#configurations","text":"Because each database and database service provider might require different configurations, the JDBC catalog allows arbitrary configurations through: Property Default Description uri the JDBC connection string jdbc.<property_key> any key value pairs to configure the JDBC connection","title":"Configurations"},{"location":"jdbc/#examples","text":"","title":"Examples"},{"location":"jdbc/#spark","text":"You can start a Spark session with a MySQL JDBC connection using the following configurations: spark-sql --packages org.apache.iceberg:iceberg-spark3-runtime:0.12.1 \\ --conf spark.sql.catalog.my_catalog=org.apache.iceberg.spark.SparkCatalog \\ --conf spark.sql.catalog.my_catalog.warehouse=s3://my-bucket/my/key/prefix \\ --conf spark.sql.catalog.my_catalog.catalog-impl=org.apache.iceberg.jdbc.JdbcCatalog \\ --conf spark.sql.catalog.my_catalog.uri=jdbc:mysql://test.1234567890.us-west-2.rds.amazonaws.com:3306/default \\ --conf spark.sql.catalog.my_catalog.jdbc.verifyServerCertificate=true \\ --conf spark.sql.catalog.my_catalog.jdbc.useSSL=true \\ --conf spark.sql.catalog.my_catalog.jdbc.user=admin \\ --conf spark.sql.catalog.my_catalog.jdbc.password=pass","title":"Spark"},{"location":"jdbc/#java-api","text":"Class.forName(\"com.mysql.cj.jdbc.Driver\"); // ensure JDBC driver is at runtime classpath Map<String, String> properties = new HashMap<>(); properties.put(CatalogProperties.CATALOG_IMPL, JdbcCatalog.class.getName()); properties.put(CatalogProperties.URI, \"jdbc:mysql://localhost:3306/test\"); properties.put(JdbcCatalog.PROPERTY_PREFIX + \"user\", \"admin\"); properties.put(JdbcCatalog.PROPERTY_PREFIX + \"password\", \"pass\"); properties.put(CatalogProperties.WAREHOUSE_LOCATION, \"s3://warehouse/path\"); Configuration hadoopConf = new Configuration(); // configs if you use HadoopFileIO JdbcCatalog catalog = CatalogUtil.buildIcebergCatalog(\"test_jdbc_catalog\", properties, hadoopConf);","title":"Java API"},{"location":"maintenance/","text":"Table Maintenance \u00b6 Note Maintenance operations require the Table instance. Please refer Java API quickstart page to refer how to load an existing table. Recommended Maintenance \u00b6 Expire Snapshots \u00b6 Each write to an Iceberg table creates a new snapshot , or version, of a table. Snapshots can be used for time-travel queries, or the table can be rolled back to any valid snapshot. Snapshots accumulate until they are expired by the expireSnapshots operation. Regularly expiring snapshots is recommended to delete data files that are no longer needed, and to keep the size of table metadata small. This example expires snapshots that are older than 1 day: Table table = ... long tsToExpire = System.currentTimeMillis() - (1000 * 60 * 60 * 24); // 1 day table.expireSnapshots() .expireOlderThan(tsToExpire) .commit(); See the ExpireSnapshots Javadoc to see more configuration options. There is also a Spark action that can run table expiration in parallel for large tables: Actions.forTable(table) .expireSnapshots() .expireOlderThan(tsToExpire) .execute(); Expiring old snapshots removes them from metadata, so they are no longer available for time travel queries. Note Data files are not deleted until they are no longer referenced by a snapshot that may be used for time travel or rollback. Regularly expiring snapshots deletes unused data files. Remove old metadata files \u00b6 Iceberg keeps track of table metadata using JSON files. Each change to a table produces a new metadata file to provide atomicity. Old metadata files are kept for history by default. Tables with frequent commits, like those written by streaming jobs, may need to regularly clean metadata files. To automatically clean metadata files, set write.metadata.delete-after-commit.enabled=true in table properties. This will keep some metadata files (up to write.metadata.previous-versions-max ) and will delete the oldest metadata file after each new one is created. Property Description write.metadata.delete-after-commit.enabled Whether to delete old metadata files after each table commit write.metadata.previous-versions-max The number of old metadata files to keep See table write properties for more details. Remove orphan files \u00b6 In Spark and other distributed processing engines, task or job failures can leave files that are not referenced by table metadata, and in some cases normal snapshot expiration may not be able to determine a file is no longer needed and delete it. To clean up these \u201corphan\u201d files under a table location, use the removeOrphanFiles action. Table table = ... Actions.forTable(table) .removeOrphanFiles() .execute(); See the RemoveOrphanFilesAction Javadoc to see more configuration options. This action may take a long time to finish if you have lots of files in data and metadata directories. It is recommended to execute this periodically, but you may not need to execute this often. Note It is dangerous to remove orphan files with a retention interval shorter than the time expected for any write to complete because it might corrupt the table if in-progress files are considered orphaned and are deleted. The default interval is 3 days. Note Iceberg uses the string representations of paths when determining which files need to be removed. On some file systems, the path can change over time, but it still represents the same file. For example, if you change authorities for an HDFS cluster, none of the old path urls used during creation will match those that appear in a current listing. This will lead to data loss when RemoveOrphanFiles is run . Please be sure the entries in your MetadataTables match those listed by the Hadoop FileSystem API to avoid unintentional deletion. Optional Maintenance \u00b6 Some tables require additional maintenance. For example, streaming queries may produce small data files that should be compacted into larger files . And some tables can benefit from rewriting manifest files to make locating data for queries much faster. Compact data files \u00b6 Iceberg tracks each data file in a table. More data files leads to more metadata stored in manifest files, and small data files causes an unnecessary amount of metadata and less efficient queries from file open costs. Iceberg can compact data files in parallel using Spark with the rewriteDataFiles action. This will combine small files into larger files to reduce metadata overhead and runtime file open cost. Table table = ... Actions.forTable(table).rewriteDataFiles() .filter(Expressions.equal(\"date\", \"2020-08-18\")) .targetSizeInBytes(500 * 1024 * 1024) // 500 MB .execute(); The files metadata table is useful for inspecting data file sizes and determining when to compact partitons. See the RewriteDataFilesAction Javadoc to see more configuration options. Rewrite manifests \u00b6 Iceberg uses metadata in its manifest list and manifest files speed up query planning and to prune unnecessary data files. The metadata tree functions as an index over a table\u2019s data. Manifests in the metadata tree are automatically compacted in the order they are added, which makes queries faster when the write pattern aligns with read filters. For example, writing hourly-partitioned data as it arrives is aligned with time range query filters. When a table\u2019s write pattern doesn\u2019t align with the query pattern, metadata can be rewritten to re-group data files into manifests using rewriteManifests or the rewriteManifests action (for parallel rewrites using Spark). This example rewrites small manifests and groups data files by the first partition field. Table table = ... table.rewriteManifests() .rewriteIf(file -> file.length() < 10 * 1024 * 1024) // 10 MB .clusterBy(file -> file.partition().get(0, Integer.class)) .commit(); See the RewriteManifestsAction Javadoc to see more configuration options.","title":"Maintenance"},{"location":"maintenance/#table-maintenance","text":"Note Maintenance operations require the Table instance. Please refer Java API quickstart page to refer how to load an existing table.","title":"Table Maintenance"},{"location":"maintenance/#recommended-maintenance","text":"","title":"Recommended Maintenance"},{"location":"maintenance/#expire-snapshots","text":"Each write to an Iceberg table creates a new snapshot , or version, of a table. Snapshots can be used for time-travel queries, or the table can be rolled back to any valid snapshot. Snapshots accumulate until they are expired by the expireSnapshots operation. Regularly expiring snapshots is recommended to delete data files that are no longer needed, and to keep the size of table metadata small. This example expires snapshots that are older than 1 day: Table table = ... long tsToExpire = System.currentTimeMillis() - (1000 * 60 * 60 * 24); // 1 day table.expireSnapshots() .expireOlderThan(tsToExpire) .commit(); See the ExpireSnapshots Javadoc to see more configuration options. There is also a Spark action that can run table expiration in parallel for large tables: Actions.forTable(table) .expireSnapshots() .expireOlderThan(tsToExpire) .execute(); Expiring old snapshots removes them from metadata, so they are no longer available for time travel queries. Note Data files are not deleted until they are no longer referenced by a snapshot that may be used for time travel or rollback. Regularly expiring snapshots deletes unused data files.","title":"Expire Snapshots"},{"location":"maintenance/#remove-old-metadata-files","text":"Iceberg keeps track of table metadata using JSON files. Each change to a table produces a new metadata file to provide atomicity. Old metadata files are kept for history by default. Tables with frequent commits, like those written by streaming jobs, may need to regularly clean metadata files. To automatically clean metadata files, set write.metadata.delete-after-commit.enabled=true in table properties. This will keep some metadata files (up to write.metadata.previous-versions-max ) and will delete the oldest metadata file after each new one is created. Property Description write.metadata.delete-after-commit.enabled Whether to delete old metadata files after each table commit write.metadata.previous-versions-max The number of old metadata files to keep See table write properties for more details.","title":"Remove old metadata files"},{"location":"maintenance/#remove-orphan-files","text":"In Spark and other distributed processing engines, task or job failures can leave files that are not referenced by table metadata, and in some cases normal snapshot expiration may not be able to determine a file is no longer needed and delete it. To clean up these \u201corphan\u201d files under a table location, use the removeOrphanFiles action. Table table = ... Actions.forTable(table) .removeOrphanFiles() .execute(); See the RemoveOrphanFilesAction Javadoc to see more configuration options. This action may take a long time to finish if you have lots of files in data and metadata directories. It is recommended to execute this periodically, but you may not need to execute this often. Note It is dangerous to remove orphan files with a retention interval shorter than the time expected for any write to complete because it might corrupt the table if in-progress files are considered orphaned and are deleted. The default interval is 3 days. Note Iceberg uses the string representations of paths when determining which files need to be removed. On some file systems, the path can change over time, but it still represents the same file. For example, if you change authorities for an HDFS cluster, none of the old path urls used during creation will match those that appear in a current listing. This will lead to data loss when RemoveOrphanFiles is run . Please be sure the entries in your MetadataTables match those listed by the Hadoop FileSystem API to avoid unintentional deletion.","title":"Remove orphan files"},{"location":"maintenance/#optional-maintenance","text":"Some tables require additional maintenance. For example, streaming queries may produce small data files that should be compacted into larger files . And some tables can benefit from rewriting manifest files to make locating data for queries much faster.","title":"Optional Maintenance"},{"location":"maintenance/#compact-data-files","text":"Iceberg tracks each data file in a table. More data files leads to more metadata stored in manifest files, and small data files causes an unnecessary amount of metadata and less efficient queries from file open costs. Iceberg can compact data files in parallel using Spark with the rewriteDataFiles action. This will combine small files into larger files to reduce metadata overhead and runtime file open cost. Table table = ... Actions.forTable(table).rewriteDataFiles() .filter(Expressions.equal(\"date\", \"2020-08-18\")) .targetSizeInBytes(500 * 1024 * 1024) // 500 MB .execute(); The files metadata table is useful for inspecting data file sizes and determining when to compact partitons. See the RewriteDataFilesAction Javadoc to see more configuration options.","title":"Compact data files"},{"location":"maintenance/#rewrite-manifests","text":"Iceberg uses metadata in its manifest list and manifest files speed up query planning and to prune unnecessary data files. The metadata tree functions as an index over a table\u2019s data. Manifests in the metadata tree are automatically compacted in the order they are added, which makes queries faster when the write pattern aligns with read filters. For example, writing hourly-partitioned data as it arrives is aligned with time range query filters. When a table\u2019s write pattern doesn\u2019t align with the query pattern, metadata can be rewritten to re-group data files into manifests using rewriteManifests or the rewriteManifests action (for parallel rewrites using Spark). This example rewrites small manifests and groups data files by the first partition field. Table table = ... table.rewriteManifests() .rewriteIf(file -> file.length() < 10 * 1024 * 1024) // 10 MB .clusterBy(file -> file.partition().get(0, Integer.class)) .commit(); See the RewriteManifestsAction Javadoc to see more configuration options.","title":"Rewrite manifests"},{"location":"nessie/","text":"Iceberg Nessie Integration \u00b6 Iceberg provides integration with Nessie through the iceberg-nessie module. This section describes how to use Iceberg with Nessie. Nessie provides several key features on top of Iceberg: multi-table transactions git-like operations (eg branches, tags, commits) hive-like metastore capabilities See Project Nessie for more information on Nessie. Nessie requires a server to run, see Getting Started to start a Nessie server. Enabling Nessie Catalog \u00b6 The iceberg-nessie module is bundled with Spark and Flink runtimes for all versions from 0.11.0 . To get started with Nessie and Iceberg simply add the Iceberg runtime to your process. Eg: spark-sql --packages org.apache.iceberg:iceberg-spark3-runtiume:0.12.1 . Spark SQL Extensions \u00b6 From spark, Nessie SQL extensions can be used to manage the Nessie repo as shown below. bin/spark-sql --packages \"org.apache.iceberg:iceberg-spark3-runtime:0.12.1,org.projectnessie:nessie-spark-extensions:0.9.2\" --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\" --conf <other settings> Please refer Nessie SQL extension document to learn more about it. Nessie Catalog \u00b6 One major feature introduced in release 0.11.0 is the ability to easily interact with a Custom Catalog from Spark and Flink. See Spark Configuration and Flink Configuration for instructions for adding a custom catalog to Iceberg. To use the Nessie Catalog the following properties are required: warehouse . Like most other catalogs the warehouse property is a file path to where this catalog should store tables. uri . This is the Nessie server base uri. Eg http://localhost:19120/api/v1 . ref (optional). This is the Nessie branch or tag you want to work in. To run directly in Java this looks like: Map<String, String> options = new HashMap<>(); options.put(\"warehouse\", \"/path/to/warehouse\"); options.put(\"ref\", \"main\"); options.put(\"uri\", \"https://localhost:19120/api/v1\"); Catalog nessieCatalog = CatalogUtil.loadCatalog(\"org.apache.iceberg.nessie.NessieCatalog\", \"nessie\", hadoopConfig, options); and in Spark: conf.set(\"spark.sql.catalog.nessie.warehouse\", \"/path/to/warehouse\"); conf.set(\"spark.sql.catalog.nessie.uri\", \"http://localhost:19120/api/v1\") conf.set(\"spark.sql.catalog.nessie.ref\", \"main\") conf.set(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\") conf.set(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\") conf.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\") This is how it looks in Flink via the Python API (additional details can be found here ): import os from pyflink.datastream import StreamExecutionEnvironment from pyflink.table import StreamTableEnvironment env = StreamExecutionEnvironment.get_execution_environment() iceberg_flink_runtime_jar = os.path.join(os.getcwd(), \"iceberg-flink-runtime-0.12.1.jar\") env.add_jars(\"file://{}\".format(iceberg_flink_runtime_jar)) table_env = StreamTableEnvironment.create(env) table_env.execute_sql(\"CREATE CATALOG nessie_catalog WITH (\" \"'type'='iceberg', \" \"'catalog-impl'='org.apache.iceberg.nessie.NessieCatalog', \" \"'uri'='http://localhost:19120/api/v1', \" \"'ref'='main', \" \"'warehouse'='/path/to/warehouse')\") There is nothing special above about the nessie name. A spark catalog can have any name, the important parts are the settings for the catalog-impl and the required config to start Nessie correctly. Once you have a Nessie catalog you have access to your entire Nessie repo. You can then perform create/delete/merge operations on branches and perform commits on branches. Each Iceberg table in a Nessie Catalog is identified by an arbitrary length namespace and table name (eg data.base.name.table ). These namespaces are implicit and don\u2019t need to be created separately. Any transaction on a Nessie enabled Iceberg table is a single commit in Nessie. Nessie commits can encompass an arbitrary number of actions on an arbitrary number of tables, however in Iceberg this will be limited to the set of single table transactions currently available. Further operations such as merges, viewing the commit log or diffs are performed by direct interaction with the NessieClient in java or by using the python client or cli. See Nessie CLI for more details on the CLI and Spark Guide for a more complete description of Nessie functionality. Nessie and Iceberg \u00b6 For most cases Nessie acts just like any other Catalog for Iceberg: providing a logical organization of a set of tables and providing atomicity to transactions. However, using Nessie opens up other interesting possibilities. When using Nessie with Iceberg every Iceberg transaction becomes a Nessie commit. This history can be listed, merged or cherry-picked across branches. Loosely coupled transactions \u00b6 By creating a branch and performing a set of operations on that branch you can approximate a multi-table transaction. A sequence of commits can be performed on the newly created branch and then merged back into the main branch atomically. This gives the appearance of a series of connected changes being exposed to the main branch simultaneously. While downstream consumers will see multiple transactions appear at once this isn\u2019t a true multi-table transaction on the database. It is effectively a fast-forward merge of multiple commits (in git language) and each operation from the branch is its own distinct transaction and commit. This is different from a real multi-table transaction where all changes would be in the same commit. This does allow multiple applications to take part in modifying a branch and for this distributed set of transactions to be exposed to the downstream users simultaneously. Experimentation \u00b6 Changes to a table can be tested in a branch before merging back into main. This is particularly useful when performing large changes like schema evolution or partition evolution. A partition evolution could be performed in a branch and you would be able to test out the change (eg performance benchmarks) before merging it. This provides great flexibility in performing on-line table modifications and testing without interrupting downstream use cases. If the changes are incorrect or not performant the branch can be dropped without being merged. Further use cases \u00b6 Please see the Nessie Documentation for further descriptions of Nessie features. Warning Regular table maintenance in Iceberg is complicated when using nessie. Please consult Management Services before performing any table maintenance . Example \u00b6 Please have a look at the Nessie Demos repo for different examples of Nessie and Iceberg in action together. Future Improvements \u00b6 Iceberg multi-table transactions. Changes to multiple Iceberg tables in the same transaction, isolation levels etc","title":"Nessie"},{"location":"nessie/#iceberg-nessie-integration","text":"Iceberg provides integration with Nessie through the iceberg-nessie module. This section describes how to use Iceberg with Nessie. Nessie provides several key features on top of Iceberg: multi-table transactions git-like operations (eg branches, tags, commits) hive-like metastore capabilities See Project Nessie for more information on Nessie. Nessie requires a server to run, see Getting Started to start a Nessie server.","title":"Iceberg Nessie Integration"},{"location":"nessie/#enabling-nessie-catalog","text":"The iceberg-nessie module is bundled with Spark and Flink runtimes for all versions from 0.11.0 . To get started with Nessie and Iceberg simply add the Iceberg runtime to your process. Eg: spark-sql --packages org.apache.iceberg:iceberg-spark3-runtiume:0.12.1 .","title":"Enabling Nessie Catalog"},{"location":"nessie/#spark-sql-extensions","text":"From spark, Nessie SQL extensions can be used to manage the Nessie repo as shown below. bin/spark-sql --packages \"org.apache.iceberg:iceberg-spark3-runtime:0.12.1,org.projectnessie:nessie-spark-extensions:0.9.2\" --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\" --conf <other settings> Please refer Nessie SQL extension document to learn more about it.","title":"Spark SQL Extensions"},{"location":"nessie/#nessie-catalog","text":"One major feature introduced in release 0.11.0 is the ability to easily interact with a Custom Catalog from Spark and Flink. See Spark Configuration and Flink Configuration for instructions for adding a custom catalog to Iceberg. To use the Nessie Catalog the following properties are required: warehouse . Like most other catalogs the warehouse property is a file path to where this catalog should store tables. uri . This is the Nessie server base uri. Eg http://localhost:19120/api/v1 . ref (optional). This is the Nessie branch or tag you want to work in. To run directly in Java this looks like: Map<String, String> options = new HashMap<>(); options.put(\"warehouse\", \"/path/to/warehouse\"); options.put(\"ref\", \"main\"); options.put(\"uri\", \"https://localhost:19120/api/v1\"); Catalog nessieCatalog = CatalogUtil.loadCatalog(\"org.apache.iceberg.nessie.NessieCatalog\", \"nessie\", hadoopConfig, options); and in Spark: conf.set(\"spark.sql.catalog.nessie.warehouse\", \"/path/to/warehouse\"); conf.set(\"spark.sql.catalog.nessie.uri\", \"http://localhost:19120/api/v1\") conf.set(\"spark.sql.catalog.nessie.ref\", \"main\") conf.set(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\") conf.set(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\") conf.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\") This is how it looks in Flink via the Python API (additional details can be found here ): import os from pyflink.datastream import StreamExecutionEnvironment from pyflink.table import StreamTableEnvironment env = StreamExecutionEnvironment.get_execution_environment() iceberg_flink_runtime_jar = os.path.join(os.getcwd(), \"iceberg-flink-runtime-0.12.1.jar\") env.add_jars(\"file://{}\".format(iceberg_flink_runtime_jar)) table_env = StreamTableEnvironment.create(env) table_env.execute_sql(\"CREATE CATALOG nessie_catalog WITH (\" \"'type'='iceberg', \" \"'catalog-impl'='org.apache.iceberg.nessie.NessieCatalog', \" \"'uri'='http://localhost:19120/api/v1', \" \"'ref'='main', \" \"'warehouse'='/path/to/warehouse')\") There is nothing special above about the nessie name. A spark catalog can have any name, the important parts are the settings for the catalog-impl and the required config to start Nessie correctly. Once you have a Nessie catalog you have access to your entire Nessie repo. You can then perform create/delete/merge operations on branches and perform commits on branches. Each Iceberg table in a Nessie Catalog is identified by an arbitrary length namespace and table name (eg data.base.name.table ). These namespaces are implicit and don\u2019t need to be created separately. Any transaction on a Nessie enabled Iceberg table is a single commit in Nessie. Nessie commits can encompass an arbitrary number of actions on an arbitrary number of tables, however in Iceberg this will be limited to the set of single table transactions currently available. Further operations such as merges, viewing the commit log or diffs are performed by direct interaction with the NessieClient in java or by using the python client or cli. See Nessie CLI for more details on the CLI and Spark Guide for a more complete description of Nessie functionality.","title":"Nessie Catalog"},{"location":"nessie/#nessie-and-iceberg","text":"For most cases Nessie acts just like any other Catalog for Iceberg: providing a logical organization of a set of tables and providing atomicity to transactions. However, using Nessie opens up other interesting possibilities. When using Nessie with Iceberg every Iceberg transaction becomes a Nessie commit. This history can be listed, merged or cherry-picked across branches.","title":"Nessie and Iceberg"},{"location":"nessie/#loosely-coupled-transactions","text":"By creating a branch and performing a set of operations on that branch you can approximate a multi-table transaction. A sequence of commits can be performed on the newly created branch and then merged back into the main branch atomically. This gives the appearance of a series of connected changes being exposed to the main branch simultaneously. While downstream consumers will see multiple transactions appear at once this isn\u2019t a true multi-table transaction on the database. It is effectively a fast-forward merge of multiple commits (in git language) and each operation from the branch is its own distinct transaction and commit. This is different from a real multi-table transaction where all changes would be in the same commit. This does allow multiple applications to take part in modifying a branch and for this distributed set of transactions to be exposed to the downstream users simultaneously.","title":"Loosely coupled transactions"},{"location":"nessie/#experimentation","text":"Changes to a table can be tested in a branch before merging back into main. This is particularly useful when performing large changes like schema evolution or partition evolution. A partition evolution could be performed in a branch and you would be able to test out the change (eg performance benchmarks) before merging it. This provides great flexibility in performing on-line table modifications and testing without interrupting downstream use cases. If the changes are incorrect or not performant the branch can be dropped without being merged.","title":"Experimentation"},{"location":"nessie/#further-use-cases","text":"Please see the Nessie Documentation for further descriptions of Nessie features. Warning Regular table maintenance in Iceberg is complicated when using nessie. Please consult Management Services before performing any table maintenance .","title":"Further use cases"},{"location":"nessie/#example","text":"Please have a look at the Nessie Demos repo for different examples of Nessie and Iceberg in action together.","title":"Example"},{"location":"nessie/#future-improvements","text":"Iceberg multi-table transactions. Changes to multiple Iceberg tables in the same transaction, isolation levels etc","title":"Future Improvements"},{"location":"partitioning/","text":"What is partitioning? \u00b6 Partitioning is a way to make queries faster by grouping similar rows together when writing. For example, queries for log entries from a logs table would usually include a time range, like this query for logs between 10 and 12 AM: SELECT level, message FROM logs WHERE event_time BETWEEN '2018-12-01 10:00:00' AND '2018-12-01 12:00:00' Configuring the logs table to partition by the date of event_time will group log events into files with the same event date. Iceberg keeps track of that date and will use it to skip files for other dates that don\u2019t have useful data. Iceberg can partition timestamps by year, month, day, and hour granularity. It can also use a categorical column, like level in this logs example, to store rows together and speed up queries. What does Iceberg do differently? \u00b6 Other tables formats like Hive support partitioning, but Iceberg supports hidden partitioning . Iceberg handles the tedious and error-prone task of producing partition values for rows in a table. Iceberg avoids reading unnecessary partitions automatically. Consumers don\u2019t need to know how the table is partitioned and add extra filters to their queries. Iceberg partition layouts can evolve as needed. Partitioning in Hive \u00b6 To demonstrate the difference, consider how Hive would handle a logs table. In Hive, partitions are explicit and appear as a column, so the logs table would have a column called event_date . When writing, an insert needs to supply the data for the event_date column: INSERT INTO logs PARTITION (event_date) SELECT level, message, event_time, format_time(event_time, 'YYYY-MM-dd') FROM unstructured_log_source Similarly, queries that search through the logs table must have an event_date filter in addition to an event_time filter. SELECT level, count(1) as count FROM logs WHERE event_time BETWEEN '2018-12-01 10:00:00' AND '2018-12-01 12:00:00' AND event_date = '2018-12-01' If the event_date filter were missing, Hive would scan through every file in the table because it doesn\u2019t know that the event_time column is related to the event_date column. Problems with Hive partitioning \u00b6 Hive must be given partition values. In the logs example, it doesn\u2019t know the relationship between event_time and event_date . This leads to several problems: Hive can\u2019t validate partition values \u2013 it is up to the writer to produce the correct value Using the wrong format, 2018-12-01 instead of 20181201 , produces silently incorrect results, not query failures Using the wrong source column, like processing_time , or time zone also causes incorrect results, not failures It is up to the user to write queries correctly Using the wrong format also leads to silently incorrect results Users that don\u2019t understand a table\u2019s physical layout get needlessly slow queries \u2013 Hive can\u2019t translate filters automatically Working queries are tied to the table\u2019s partitioning scheme, so partitioning configuration cannot be changed without breaking queries Iceberg\u2019s hidden partitioning \u00b6 Iceberg produces partition values by taking a column value and optionally transforming it. Iceberg is responsible for converting event_time into event_date , and keeps track of the relationship. Table partitioning is configured using these relationships. The logs table would be partitioned by date(event_time) and level . Because Iceberg doesn\u2019t require user-maintained partition columns, it can hide partitioning. Partition values are produced correctly every time and always used to speed up queries, when possible. Producers and consumers wouldn\u2019t even see event_date . Most importantly, queries no longer depend on a table\u2019s physical layout. With a separation between physical and logical, Iceberg tables can evolve partition schemes over time as data volume changes. Misconfigured tables can be fixed without an expensive migration. For details about all the supported hidden partition transformations, see the Partition Transforms section. For details about updating a table\u2019s partition spec, see the partition evolution section.","title":"Partitioning"},{"location":"partitioning/#what-is-partitioning","text":"Partitioning is a way to make queries faster by grouping similar rows together when writing. For example, queries for log entries from a logs table would usually include a time range, like this query for logs between 10 and 12 AM: SELECT level, message FROM logs WHERE event_time BETWEEN '2018-12-01 10:00:00' AND '2018-12-01 12:00:00' Configuring the logs table to partition by the date of event_time will group log events into files with the same event date. Iceberg keeps track of that date and will use it to skip files for other dates that don\u2019t have useful data. Iceberg can partition timestamps by year, month, day, and hour granularity. It can also use a categorical column, like level in this logs example, to store rows together and speed up queries.","title":"What is partitioning?"},{"location":"partitioning/#what-does-iceberg-do-differently","text":"Other tables formats like Hive support partitioning, but Iceberg supports hidden partitioning . Iceberg handles the tedious and error-prone task of producing partition values for rows in a table. Iceberg avoids reading unnecessary partitions automatically. Consumers don\u2019t need to know how the table is partitioned and add extra filters to their queries. Iceberg partition layouts can evolve as needed.","title":"What does Iceberg do differently?"},{"location":"partitioning/#partitioning-in-hive","text":"To demonstrate the difference, consider how Hive would handle a logs table. In Hive, partitions are explicit and appear as a column, so the logs table would have a column called event_date . When writing, an insert needs to supply the data for the event_date column: INSERT INTO logs PARTITION (event_date) SELECT level, message, event_time, format_time(event_time, 'YYYY-MM-dd') FROM unstructured_log_source Similarly, queries that search through the logs table must have an event_date filter in addition to an event_time filter. SELECT level, count(1) as count FROM logs WHERE event_time BETWEEN '2018-12-01 10:00:00' AND '2018-12-01 12:00:00' AND event_date = '2018-12-01' If the event_date filter were missing, Hive would scan through every file in the table because it doesn\u2019t know that the event_time column is related to the event_date column.","title":"Partitioning in Hive"},{"location":"partitioning/#problems-with-hive-partitioning","text":"Hive must be given partition values. In the logs example, it doesn\u2019t know the relationship between event_time and event_date . This leads to several problems: Hive can\u2019t validate partition values \u2013 it is up to the writer to produce the correct value Using the wrong format, 2018-12-01 instead of 20181201 , produces silently incorrect results, not query failures Using the wrong source column, like processing_time , or time zone also causes incorrect results, not failures It is up to the user to write queries correctly Using the wrong format also leads to silently incorrect results Users that don\u2019t understand a table\u2019s physical layout get needlessly slow queries \u2013 Hive can\u2019t translate filters automatically Working queries are tied to the table\u2019s partitioning scheme, so partitioning configuration cannot be changed without breaking queries","title":"Problems with Hive partitioning"},{"location":"partitioning/#icebergs-hidden-partitioning","text":"Iceberg produces partition values by taking a column value and optionally transforming it. Iceberg is responsible for converting event_time into event_date , and keeps track of the relationship. Table partitioning is configured using these relationships. The logs table would be partitioned by date(event_time) and level . Because Iceberg doesn\u2019t require user-maintained partition columns, it can hide partitioning. Partition values are produced correctly every time and always used to speed up queries, when possible. Producers and consumers wouldn\u2019t even see event_date . Most importantly, queries no longer depend on a table\u2019s physical layout. With a separation between physical and logical, Iceberg tables can evolve partition schemes over time as data volume changes. Misconfigured tables can be fixed without an expensive migration. For details about all the supported hidden partition transformations, see the Partition Transforms section. For details about updating a table\u2019s partition spec, see the partition evolution section.","title":"Iceberg's hidden partitioning"},{"location":"performance/","text":"Performance \u00b6 Iceberg is designed for huge tables and is used in production where a single table can contain tens of petabytes of data. Even multi-petabyte tables can be read from a single node, without needing a distributed SQL engine to sift through table metadata. Scan planning \u00b6 Scan planning is the process of finding the files in a table that are needed for a query. Planning in an Iceberg table fits on a single node because Iceberg\u2019s metadata can be used to prune metadata files that aren\u2019t needed, in addition to filtering data files that don\u2019t contain matching data. Fast scan planning from a single node enables: Lower latency SQL queries \u2013 by eliminating a distributed scan to plan a distributed scan Access from any client \u2013 stand-alone processes can read data directly from Iceberg tables Metadata filtering \u00b6 Iceberg uses two levels of metadata to track the files in a snapshot. Manifest files store a list of data files, along each data file\u2019s partition data and column-level stats A manifest list stores the snapshot\u2019s list of manifests, along with the range of values for each partition field For fast scan planning, Iceberg first filters manifests using the partition value ranges in the manifest list. Then, it reads each manifest to get data files. With this scheme, the manifest list acts as an index over the manifest files, making it possible to plan without reading all manifests. In addition to partition value ranges, a manifest list also stores the number of files added or deleted in a manifest to speed up operations like snapshot expiration. Data filtering \u00b6 Manifest files include a tuple of partition data and column-level stats for each data file. During planning, query predicates are automatically converted to predicates on the partition data and applied first to filter data files. Next, column-level value counts, null counts, lower bounds, and upper bounds are used to eliminate files that cannot match the query predicate. By using upper and lower bounds to filter data files at planning time, Iceberg uses clustered data to eliminate splits without running tasks. In some cases, this is a 10x performance improvement .","title":"Performance"},{"location":"performance/#performance","text":"Iceberg is designed for huge tables and is used in production where a single table can contain tens of petabytes of data. Even multi-petabyte tables can be read from a single node, without needing a distributed SQL engine to sift through table metadata.","title":"Performance"},{"location":"performance/#scan-planning","text":"Scan planning is the process of finding the files in a table that are needed for a query. Planning in an Iceberg table fits on a single node because Iceberg\u2019s metadata can be used to prune metadata files that aren\u2019t needed, in addition to filtering data files that don\u2019t contain matching data. Fast scan planning from a single node enables: Lower latency SQL queries \u2013 by eliminating a distributed scan to plan a distributed scan Access from any client \u2013 stand-alone processes can read data directly from Iceberg tables","title":"Scan planning"},{"location":"performance/#metadata-filtering","text":"Iceberg uses two levels of metadata to track the files in a snapshot. Manifest files store a list of data files, along each data file\u2019s partition data and column-level stats A manifest list stores the snapshot\u2019s list of manifests, along with the range of values for each partition field For fast scan planning, Iceberg first filters manifests using the partition value ranges in the manifest list. Then, it reads each manifest to get data files. With this scheme, the manifest list acts as an index over the manifest files, making it possible to plan without reading all manifests. In addition to partition value ranges, a manifest list also stores the number of files added or deleted in a manifest to speed up operations like snapshot expiration.","title":"Metadata filtering"},{"location":"performance/#data-filtering","text":"Manifest files include a tuple of partition data and column-level stats for each data file. During planning, query predicates are automatically converted to predicates on the partition data and applied first to filter data files. Next, column-level value counts, null counts, lower bounds, and upper bounds are used to eliminate files that cannot match the query predicate. By using upper and lower bounds to filter data files at planning time, Iceberg uses clustered data to eliminate splits without running tasks. In some cases, this is a 10x performance improvement .","title":"Data filtering"},{"location":"python-api-intro/","text":"Iceberg Python API \u00b6 Much of the python api conforms to the java api. You can get more info about the java api here . Catalog \u00b6 The Catalog interface, like java provides search and management operations for tables. To create a catalog: from iceberg.hive import HiveTables # instantiate Hive Tables conf = {\"hive.metastore.uris\": 'thrift://{hms_host}:{hms_port}'} tables = HiveTables(conf) and to create a table from a catalog: from iceberg.api.schema import Schema\\ from iceberg.api.types import TimestampType, DoubleType, StringType, NestedField from iceberg.api.partition_spec import PartitionSpecBuilder schema = Schema(NestedField.optional(1, \"DateTime\", TimestampType.with_timezone()), NestedField.optional(2, \"Bid\", DoubleType.get()), NestedField.optional(3, \"Ask\", DoubleType.get()), NestedField.optional(4, \"symbol\", StringType.get())) partition_spec = PartitionSpecBuilder(schema).add(1, 1000, \"DateTime_day\", \"day\").build() tables.create(schema, \"test.test_123\", partition) Tables \u00b6 The Table interface provides access to table metadata schema returns the current table Schema spec returns the current table PartitonSpec properties returns a map of key-value TableProperties currentSnapshot returns the current table Snapshot snapshots returns all valid snapshots for the table snapshot(id) returns a specific snapshot by ID location returns the table\u2019s base location Tables also provide refresh to update the table to the latest version. Scanning \u00b6 Iceberg table scans start by creating a TableScan object with newScan . scan = table.new_scan(); To configure a scan, call filter and select on the TableScan to get a new TableScan with those changes. filtered_scan = scan.filter(Expressions.equal(\"id\", 5)) String expressions can also be passed to the filter method. filtered_scan = scan.filter(\"id=5\") Schema projections can be applied against a TableScan by passing a list of column names. filtered_scan = scan.select([\"col_1\", \"col_2\", \"col_3\"]) Because some data types cannot be read using the python library, a convenience method for excluding columns from projection is provided. filtered_scan = scan.select_except([\"unsupported_col_1\", \"unsupported_col_2\"]) Calls to configuration methods create a new TableScan so that each TableScan is immutable. When a scan is configured, planFiles , planTasks , and Schema are used to return files, tasks, and the read projection. scan = table.new_scan() \\ .filter(\"id=5\") \\ .select([\"id\", \"data\"]) projection = scan.schema for task in scan.plan_tasks(): print(task) Types \u00b6 Iceberg data types are located in iceberg.api.types.types Primitives \u00b6 Primitive type instances are available from static methods in each type class. Types without parameters use get , and types like DecimalType use factory methods: IntegerType.get() # int DoubleType.get() # double DecimalType.of(9, 2) # decimal(9, 2) Nested types \u00b6 Structs, maps, and lists are created using factory methods in type classes. Like struct fields, map keys or values and list elements are tracked as nested fields. Nested fields track field IDs and nullability. Struct fields are created using NestedField.optional or NestedField.required . Map value and list element nullability is set in the map and list factory methods. # struct<1 id: int, 2 data: optional string> struct = StructType.of([NestedField.required(1, \"id\", IntegerType.get()), NestedField.optional(2, \"data\", StringType.get()]) ) # map<1 key: int, 2 value: optional string> map_var = MapType.of_optional(1, IntegerType.get(), 2, StringType.get()) # array<1 element: int> list_var = ListType.of_required(1, IntegerType.get()); Expressions \u00b6 Iceberg\u2019s Expressions are used to configure table scans. To create Expressions , use the factory methods in Expressions . Supported Predicate expressions are: is_null not_null equal not_equal less_than less_than_or_equal greater_than greater_than_or_equal Supported expression Operations are: and or not Constant expressions are: always_true always_false","title":"Python API Intro"},{"location":"python-api-intro/#iceberg-python-api","text":"Much of the python api conforms to the java api. You can get more info about the java api here .","title":"Iceberg Python API"},{"location":"python-api-intro/#catalog","text":"The Catalog interface, like java provides search and management operations for tables. To create a catalog: from iceberg.hive import HiveTables # instantiate Hive Tables conf = {\"hive.metastore.uris\": 'thrift://{hms_host}:{hms_port}'} tables = HiveTables(conf) and to create a table from a catalog: from iceberg.api.schema import Schema\\ from iceberg.api.types import TimestampType, DoubleType, StringType, NestedField from iceberg.api.partition_spec import PartitionSpecBuilder schema = Schema(NestedField.optional(1, \"DateTime\", TimestampType.with_timezone()), NestedField.optional(2, \"Bid\", DoubleType.get()), NestedField.optional(3, \"Ask\", DoubleType.get()), NestedField.optional(4, \"symbol\", StringType.get())) partition_spec = PartitionSpecBuilder(schema).add(1, 1000, \"DateTime_day\", \"day\").build() tables.create(schema, \"test.test_123\", partition)","title":"Catalog"},{"location":"python-api-intro/#tables","text":"The Table interface provides access to table metadata schema returns the current table Schema spec returns the current table PartitonSpec properties returns a map of key-value TableProperties currentSnapshot returns the current table Snapshot snapshots returns all valid snapshots for the table snapshot(id) returns a specific snapshot by ID location returns the table\u2019s base location Tables also provide refresh to update the table to the latest version.","title":"Tables"},{"location":"python-api-intro/#scanning","text":"Iceberg table scans start by creating a TableScan object with newScan . scan = table.new_scan(); To configure a scan, call filter and select on the TableScan to get a new TableScan with those changes. filtered_scan = scan.filter(Expressions.equal(\"id\", 5)) String expressions can also be passed to the filter method. filtered_scan = scan.filter(\"id=5\") Schema projections can be applied against a TableScan by passing a list of column names. filtered_scan = scan.select([\"col_1\", \"col_2\", \"col_3\"]) Because some data types cannot be read using the python library, a convenience method for excluding columns from projection is provided. filtered_scan = scan.select_except([\"unsupported_col_1\", \"unsupported_col_2\"]) Calls to configuration methods create a new TableScan so that each TableScan is immutable. When a scan is configured, planFiles , planTasks , and Schema are used to return files, tasks, and the read projection. scan = table.new_scan() \\ .filter(\"id=5\") \\ .select([\"id\", \"data\"]) projection = scan.schema for task in scan.plan_tasks(): print(task)","title":"Scanning"},{"location":"python-api-intro/#types","text":"Iceberg data types are located in iceberg.api.types.types","title":"Types"},{"location":"python-api-intro/#primitives","text":"Primitive type instances are available from static methods in each type class. Types without parameters use get , and types like DecimalType use factory methods: IntegerType.get() # int DoubleType.get() # double DecimalType.of(9, 2) # decimal(9, 2)","title":"Primitives"},{"location":"python-api-intro/#nested-types","text":"Structs, maps, and lists are created using factory methods in type classes. Like struct fields, map keys or values and list elements are tracked as nested fields. Nested fields track field IDs and nullability. Struct fields are created using NestedField.optional or NestedField.required . Map value and list element nullability is set in the map and list factory methods. # struct<1 id: int, 2 data: optional string> struct = StructType.of([NestedField.required(1, \"id\", IntegerType.get()), NestedField.optional(2, \"data\", StringType.get()]) ) # map<1 key: int, 2 value: optional string> map_var = MapType.of_optional(1, IntegerType.get(), 2, StringType.get()) # array<1 element: int> list_var = ListType.of_required(1, IntegerType.get());","title":"Nested types"},{"location":"python-api-intro/#expressions","text":"Iceberg\u2019s Expressions are used to configure table scans. To create Expressions , use the factory methods in Expressions . Supported Predicate expressions are: is_null not_null equal not_equal less_than less_than_or_equal greater_than greater_than_or_equal Supported expression Operations are: and or not Constant expressions are: always_true always_false","title":"Expressions"},{"location":"python-feature-support/","text":"Feature Support \u00b6 The goal is that the python library will provide a functional, performant subset of the java library. The initial focus has been on reading table metadata as well as providing the capability to both plan and execute a scan. Feature Comparison \u00b6 Metadata \u00b6 Operation Java Python Get Schema X X Get Snapshots X X Plan Scan X X Plan Scan for Snapshot X X Update Current Snapshot X Set Table Properties X Create Table X X Drop Table X X Alter Table X Read Support \u00b6 Pyarrow is used for reading parquet files, so read support is limited to what is currently supported in the pyarrow.parquet package. Primitive Types \u00b6 Data Type Java Python BooleanType X X DateType X X DecimalType X X FloatType X X IntegerType X X LongType X X TimeType X X TimestampType X X Nested Types \u00b6 Data Type Java Python ListType of primitives X X MapType of primitives X X StructType of primitives X X ListType of Nested Types X MapType of Nested Types X Write Support \u00b6 The python client does not currently support write capability","title":"Python Feature Support"},{"location":"python-feature-support/#feature-support","text":"The goal is that the python library will provide a functional, performant subset of the java library. The initial focus has been on reading table metadata as well as providing the capability to both plan and execute a scan.","title":"Feature Support"},{"location":"python-feature-support/#feature-comparison","text":"","title":"Feature Comparison"},{"location":"python-feature-support/#metadata","text":"Operation Java Python Get Schema X X Get Snapshots X X Plan Scan X X Plan Scan for Snapshot X X Update Current Snapshot X Set Table Properties X Create Table X X Drop Table X X Alter Table X","title":"Metadata"},{"location":"python-feature-support/#read-support","text":"Pyarrow is used for reading parquet files, so read support is limited to what is currently supported in the pyarrow.parquet package.","title":"Read Support"},{"location":"python-feature-support/#primitive-types","text":"Data Type Java Python BooleanType X X DateType X X DecimalType X X FloatType X X IntegerType X X LongType X X TimeType X X TimestampType X X","title":"Primitive Types"},{"location":"python-feature-support/#nested-types","text":"Data Type Java Python ListType of primitives X X MapType of primitives X X StructType of primitives X X ListType of Nested Types X MapType of Nested Types X","title":"Nested Types"},{"location":"python-feature-support/#write-support","text":"The python client does not currently support write capability","title":"Write Support"},{"location":"python-quickstart/","text":"Getting Started \u00b6 Installation \u00b6 Iceberg python is currently in development, for development and testing purposes the best way to install the library is to perform the following steps: git clone https://github.com/apache/iceberg.git cd iceberg/python pip install -e . Testing \u00b6 Testing is done using tox. The config can be found in tox.ini within the python directory of the iceberg project. # simply run tox from within the python dir tox Examples \u00b6 Inspect Table Metadata \u00b6 from iceberg.hive import HiveTables # instantiate Hive Tables conf = {\"hive.metastore.uris\": 'thrift://{hms_host}:{hms_port}'} tables = HiveTables(conf) # load table tbl = tables.load(\"iceberg_db.iceberg_test_table\") # inspect metadata print(tbl.schema()) print(tbl.spec()) print(tbl.location()) # get table level record count from pprint import pprint pprint(int(tbl.current_snapshot().summary.get(\"total-records\")))","title":"Python Quickstart"},{"location":"python-quickstart/#getting-started","text":"","title":"Getting Started"},{"location":"python-quickstart/#installation","text":"Iceberg python is currently in development, for development and testing purposes the best way to install the library is to perform the following steps: git clone https://github.com/apache/iceberg.git cd iceberg/python pip install -e .","title":"Installation"},{"location":"python-quickstart/#testing","text":"Testing is done using tox. The config can be found in tox.ini within the python directory of the iceberg project. # simply run tox from within the python dir tox","title":"Testing"},{"location":"python-quickstart/#examples","text":"","title":"Examples"},{"location":"python-quickstart/#inspect-table-metadata","text":"from iceberg.hive import HiveTables # instantiate Hive Tables conf = {\"hive.metastore.uris\": 'thrift://{hms_host}:{hms_port}'} tables = HiveTables(conf) # load table tbl = tables.load(\"iceberg_db.iceberg_test_table\") # inspect metadata print(tbl.schema()) print(tbl.spec()) print(tbl.location()) # get table level record count from pprint import pprint pprint(int(tbl.current_snapshot().summary.get(\"total-records\")))","title":"Inspect Table Metadata"},{"location":"releases/","text":"Downloads \u00b6 The latest version of Iceberg is 0.12.1 . 0.12.1 source tar.gz \u2013 signature \u2013 sha512 0.12.1 Spark 3.0 runtime Jar 0.12.1 Spark 2.4 runtime Jar 0.12.1 Flink runtime Jar 0.12.1 Hive runtime Jar To use Iceberg in Spark, download the runtime JAR and add it to the jars folder of your Spark install. Use iceberg-spark3-runtime for Spark 3, and iceberg-spark-runtime for Spark 2.4. To use Iceberg in Hive, download the iceberg-hive-runtime JAR and add it to Hive using ADD JAR . Gradle \u00b6 To add a dependency on Iceberg in Gradle, add the following to build.gradle : dependencies { compile 'org.apache.iceberg:iceberg-core:0.12.1' } You may also want to include iceberg-parquet for Parquet file support. Maven \u00b6 To add a dependency on Iceberg in Maven, add the following to your pom.xml : <dependencies> ... <dependency> <groupId>org.apache.iceberg</groupId> <artifactId>iceberg-core</artifactId> <version>0.12.1</version> </dependency> ... </dependencies> 0.12.1 Release Notes \u00b6 Apache Iceberg 0.12.1 was released on November 8th, 2021. Important bug fixes and changes: #3264 fixes validation failures that occurred after snapshot expiration when writing Flink CDC streams to Iceberg tables. #3264 fixes reading projected map columns from Parquet files written before Parquet 1.11.1. #3195 allows validating that commits that produce row-level deltas don\u2019t conflict with concurrently added files. Ensures users can maintain serializable isolation for update and delete operations, including merge operations. #3199 allows validating that commits that overwrite files don\u2019t conflict with concurrently added files. Ensures users can maintain serializable isolation for overwrite operations. #3135 fixes equality-deletes using DATE , TIMESTAMP , and TIME types. #3078 prevents the JDBC catalog from overwriting the jdbc.user property if any property called user exists in the environment. #3035 fixes drop namespace calls with the DyanmoDB catalog. #3273 fixes importing Avro files via add_files by correctly setting the number of records. #3332 fixes importing ORC files with float or double columns in add_files . A more exhaustive list of changes is available under the 0.12.1 release milestone . Past releases \u00b6 0.12.0 \u00b6 Apache Iceberg 0.12.0 was released on August 15, 2021. It consists of 395 commits authored by 74 contributors over a 139 day period. Git tag: 0.12.0 0.12.0 source tar.gz \u2013 signature \u2013 sha512 0.12.0 Spark 3.x runtime Jar 0.12.0 Spark 2.4 runtime Jar 0.12.0 Flink runtime Jar 0.12.0 Hive runtime Jar High-level features: Core Allow Iceberg schemas to specify one or more columns as row identifiers [ #2465 ]. Note that this is a prerequisite for supporting upserts in Flink. Added JDBC [ #1870 ] and DynamoDB [ #2688 ] catalog implementations. Added predicate pushdown for partitions and files metadata tables [ #2358 , #2926 ]. Added a new, more flexible compaction action for Spark that can support different strategies such as bin packing and sorting. [ #2501 , #2609 ]. Added the ability to upgrade to v2 or create a v2 table using the table property format-version=2 [ #2887 ]. Added support for nulls in StructLike collections [ #2929 ]. Added key_metadata field to manifest lists for encryption [ #2675 ]. Flink Added support for SQL primary keys [ #2410 ]. Hive Added the ability to set the catalog at the table level in the Hive Metastore. This makes it possible to write queries that reference tables from multiple catalogs [ #2129 ]. As a result of [ #2129 ], deprecated the configuration property iceberg.mr.catalog which was previously used to configure the Iceberg catalog in MapReduce and Hive [ #2565 ]. Added table-level JVM lock on commits[ #2547 ]. Added support for Hive\u2019s vectorized ORC reader [ #2613 ]. Spark Added SET and DROP IDENTIFIER FIELDS clauses to ALTER TABLE so people don\u2019t have to look up the DDL [ #2560 ]. Added support for ALTER TABLE REPLACE PARTITION FIELD DDL [ #2365 ]. Added support for micro-batch streaming reads for structured streaming in Spark3 [ #2660 ]. Improved the performance of importing a Hive table by not loading all partitions from Hive and instead pushing the partition filter to the Metastore [ #2777 ]. Added support for UPDATE statements in Spark [ #2193 , #2206 ]. Added support for Spark 3.1 [ #2512 ]. Added RemoveReachableFiles action [ #2415 ]. Added add_files stored procedure [ #2210 ]. Refactored Actions API and added a new entry point. Added support for Hadoop configuration overrides [ #2922 ]. Added support for the TIMESTAMP WITHOUT TIMEZONE type in Spark [ #2757 ]. Added validation that files referenced by row-level deletes are not concurrently rewritten [ #2308 ]. Important bug fixes: Core Fixed string bucketing with non-BMP characters [ #2849 ]. Fixed Parquet dictionary filtering with fixed-length byte arrays and decimals [ #2551 ]. Fixed a problem with the configuration of HiveCatalog [ #2550 ]. Fixed partition field IDs in table replacement [ #2906 ]. Hive Enabled dropping HMS tables even if the metadata on disk gets corrupted [ #2583 ]. Parquet Fixed Parquet row group filters when types are promoted from int to long or from float to double [ #2232 ] Spark Fixed MERGE INTO in Spark when used with SinglePartition partitioning [ #2584 ]. Fixed nested struct pruning in Spark [ #2877 ]. Fixed NaN handling for float and double metrics [ #2464 ]. Fixed Kryo serialization for data and delete files [ #2343 ]. Other notable changes: The Iceberg Community voted to approve version 2 of the Apache Iceberg Format Specification. The differences between version 1 and 2 of the specification are documented here . Bugfixes and stability improvements for NessieCatalog. Improvements and fixes for Iceberg\u2019s Python library. Added a vectorized reader for Apache Arrow [ #2286 ]. The following Iceberg dependencies were upgraded: Hive 2.3.8 [ #2110 ]. Avro 1.10.1 [ #1648 ]. Parquet 1.12.0 [ #2441 ]. 0.11.1 \u00b6 Git tag: 0.11.1 0.11.1 source tar.gz \u2013 signature \u2013 sha512 0.11.1 Spark 3.0 runtime Jar 0.11.1 Spark 2.4 runtime Jar 0.11.1 Flink runtime Jar 0.11.1 Hive runtime Jar Important bug fixes: #2367 prohibits deleting data files when tables are dropped if GC is disabled. #2196 fixes data loss after compaction when large files are split into multiple parts and only some parts are combined with other files. #2232 fixes row group filters with promoted types in Parquet. #2267 avoids listing non-Iceberg tables in Glue. #2254 fixes predicate pushdown for Date in Hive. #2126 fixes writing of Date, Decimal, Time, UUID types in Hive. #2241 fixes vectorized ORC reads with metadata columns in Spark. #2154 refreshes the relation cache in DELETE and MERGE operations in Spark. 0.11.0 \u00b6 Git tag: 0.11.0 0.11.0 source tar.gz \u2013 signature \u2013 sha512 0.11.0 Spark 3.0 runtime Jar 0.11.0 Spark 2.4 runtime Jar 0.11.0 Flink runtime Jar 0.11.0 Hive runtime Jar High-level features: Core API now supports partition spec and sort order evolution Spark 3 now supports the following SQL extensions: MERGE INTO (experimental) DELETE FROM (experimental) ALTER TABLE \u2026 ADD/DROP PARTITION ALTER TABLE \u2026 WRITE ORDERED BY Invoke stored procedures using CALL Flink now supports streaming reads, CDC writes (experimental), and filter pushdown AWS module is added to support better integration with AWS, with AWS Glue catalog support and dedicated S3 FileIO implementation Nessie module is added to support integration with project Nessie Important bug fixes: #1981 fixes bug that date and timestamp transforms were producing incorrect values for dates and times before 1970. Before the fix, negative values were incorrectly transformed by date and timestamp transforms to 1 larger than the correct value. For example, day(1969-12-31 10:00:00) produced 0 instead of -1. The fix is backwards compatible, which means predicate projection can still work with the incorrectly transformed partitions written using older versions. #2091 fixes ClassCastException for type promotion int to long and float to double during Parquet vectorized read. Now Arrow vector is created by looking at Parquet file schema instead of Iceberg schema for int and float fields. #1998 fixes bug in HiveTableOperation that unlock is not called if new metadata cannot be deleted. Now it is guaranteed that unlock is always called for Hive catalog users. #1979 fixes table listing failure in Hadoop catalog when user does not have permission to some tables. Now the tables with no permission are ignored in listing. #1798 fixes scan task failure when encountering duplicate entries of data files. Spark and Flink readers can now ignore duplicated entries in data files for each scan task. #1785 fixes invalidation of metadata tables in CachingCatalog . When a table is dropped, all the metadata tables associated with it are also invalidated in the cache. #1960 fixes bug that ORC writer does not read metrics config and always use the default. Now customized metrics config is respected. Other notable changes: NaN counts are now supported in metadata Shared catalog properties are added in core library to standardize catalog level configurations Spark and Flink now support dynamically loading customized Catalog and FileIO implementations Spark 2 now supports loading tables from other catalogs, like Spark 3 Spark 3 now supports catalog names in DataFrameReader when using Iceberg as a format Flink now uses the number of Iceberg read splits as its job parallelism to improve performance and save resource. Hive (experimental) now supports INSERT INTO, case insensitive query, projection pushdown, create DDL with schema and auto type conversion ORC now supports reading tinyint, smallint, char, varchar types Avro to Iceberg schema conversion now preserves field docs 0.10.0 \u00b6 Git tag: 0.10.0 0.10.0 source tar.gz \u2013 signature \u2013 sha512 0.10.0 Spark 3.0 runtime Jar 0.10.0 Spark 2.4 runtime Jar 0.10.0 Flink runtime Jar 0.10.0 Hive runtime Jar High-level features: Format v2 support for building row-level operations ( MERGE INTO ) in processing engines Note: format v2 is not yet finalized and does not have a forward-compatibility guarantee Flink integration for writing to Iceberg tables and reading from Iceberg tables (reading supports batch mode only) Hive integration for reading from Iceberg tables, with filter pushdown (experimental; configuration may change) Important bug fixes: #1706 fixes non-vectorized ORC reads in Spark that incorrectly skipped rows #1536 fixes ORC conversion of notIn and notEqual to match null values #1722 fixes Expressions.notNull returning an isNull predicate; API only, method was not used by processing engines #1736 fixes IllegalArgumentException in vectorized Spark reads with negative decimal values #1666 fixes file lengths returned by the ORC writer, using compressed size rather than uncompressed size #1674 removes catalog expiration in HiveCatalogs #1545 automatically refreshes tables in Spark when not caching table instances Other notable changes: The iceberg-hive module has been renamed to iceberg-hive-metastore to avoid confusion Spark 3 is based on 3.0.1 that includes the fix for SPARK-32168 Hadoop tables will recover from version hint corruption Tables can be configured with a required sort order Data file locations can be customized with a dynamically loaded LocationProvider ORC file imports can apply a name mapping for stats A more exhaustive list of changes is available under the 0.10.0 release milestone . 0.9.1 \u00b6 Git tag: 0.9.1 0.9.1 source tar.gz \u2013 signature \u2013 sha512 0.9.1 Spark 3.0 runtime Jar 0.9.1 Spark 2.4 runtime Jar 0.9.0 \u00b6 Git tag: 0.9.0 0.9.0 source tar.gz \u2013 signature \u2013 sha512 0.9.0 Spark 3.0 runtime Jar 0.9.0 Spark 2.4 runtime Jar 0.8.0 \u00b6 Git tag: apache-iceberg-0.8.0-incubating 0.8.0-incubating source tar.gz \u2013 signature \u2013 sha512 0.8.0-incubating Spark 2.4 runtime Jar 0.7.0 \u00b6 Git tag: apache-iceberg-0.7.0-incubating 0.7.0-incubating source tar.gz \u2013 signature \u2013 sha512 0.7.0-incubating Spark 2.4 runtime Jar","title":"Releases"},{"location":"releases/#downloads","text":"The latest version of Iceberg is 0.12.1 . 0.12.1 source tar.gz \u2013 signature \u2013 sha512 0.12.1 Spark 3.0 runtime Jar 0.12.1 Spark 2.4 runtime Jar 0.12.1 Flink runtime Jar 0.12.1 Hive runtime Jar To use Iceberg in Spark, download the runtime JAR and add it to the jars folder of your Spark install. Use iceberg-spark3-runtime for Spark 3, and iceberg-spark-runtime for Spark 2.4. To use Iceberg in Hive, download the iceberg-hive-runtime JAR and add it to Hive using ADD JAR .","title":"Downloads"},{"location":"releases/#gradle","text":"To add a dependency on Iceberg in Gradle, add the following to build.gradle : dependencies { compile 'org.apache.iceberg:iceberg-core:0.12.1' } You may also want to include iceberg-parquet for Parquet file support.","title":"Gradle"},{"location":"releases/#maven","text":"To add a dependency on Iceberg in Maven, add the following to your pom.xml : <dependencies> ... <dependency> <groupId>org.apache.iceberg</groupId> <artifactId>iceberg-core</artifactId> <version>0.12.1</version> </dependency> ... </dependencies>","title":"Maven"},{"location":"releases/#0121-release-notes","text":"Apache Iceberg 0.12.1 was released on November 8th, 2021. Important bug fixes and changes: #3264 fixes validation failures that occurred after snapshot expiration when writing Flink CDC streams to Iceberg tables. #3264 fixes reading projected map columns from Parquet files written before Parquet 1.11.1. #3195 allows validating that commits that produce row-level deltas don\u2019t conflict with concurrently added files. Ensures users can maintain serializable isolation for update and delete operations, including merge operations. #3199 allows validating that commits that overwrite files don\u2019t conflict with concurrently added files. Ensures users can maintain serializable isolation for overwrite operations. #3135 fixes equality-deletes using DATE , TIMESTAMP , and TIME types. #3078 prevents the JDBC catalog from overwriting the jdbc.user property if any property called user exists in the environment. #3035 fixes drop namespace calls with the DyanmoDB catalog. #3273 fixes importing Avro files via add_files by correctly setting the number of records. #3332 fixes importing ORC files with float or double columns in add_files . A more exhaustive list of changes is available under the 0.12.1 release milestone .","title":"0.12.1 Release Notes"},{"location":"releases/#past-releases","text":"","title":"Past releases"},{"location":"releases/#0120","text":"Apache Iceberg 0.12.0 was released on August 15, 2021. It consists of 395 commits authored by 74 contributors over a 139 day period. Git tag: 0.12.0 0.12.0 source tar.gz \u2013 signature \u2013 sha512 0.12.0 Spark 3.x runtime Jar 0.12.0 Spark 2.4 runtime Jar 0.12.0 Flink runtime Jar 0.12.0 Hive runtime Jar High-level features: Core Allow Iceberg schemas to specify one or more columns as row identifiers [ #2465 ]. Note that this is a prerequisite for supporting upserts in Flink. Added JDBC [ #1870 ] and DynamoDB [ #2688 ] catalog implementations. Added predicate pushdown for partitions and files metadata tables [ #2358 , #2926 ]. Added a new, more flexible compaction action for Spark that can support different strategies such as bin packing and sorting. [ #2501 , #2609 ]. Added the ability to upgrade to v2 or create a v2 table using the table property format-version=2 [ #2887 ]. Added support for nulls in StructLike collections [ #2929 ]. Added key_metadata field to manifest lists for encryption [ #2675 ]. Flink Added support for SQL primary keys [ #2410 ]. Hive Added the ability to set the catalog at the table level in the Hive Metastore. This makes it possible to write queries that reference tables from multiple catalogs [ #2129 ]. As a result of [ #2129 ], deprecated the configuration property iceberg.mr.catalog which was previously used to configure the Iceberg catalog in MapReduce and Hive [ #2565 ]. Added table-level JVM lock on commits[ #2547 ]. Added support for Hive\u2019s vectorized ORC reader [ #2613 ]. Spark Added SET and DROP IDENTIFIER FIELDS clauses to ALTER TABLE so people don\u2019t have to look up the DDL [ #2560 ]. Added support for ALTER TABLE REPLACE PARTITION FIELD DDL [ #2365 ]. Added support for micro-batch streaming reads for structured streaming in Spark3 [ #2660 ]. Improved the performance of importing a Hive table by not loading all partitions from Hive and instead pushing the partition filter to the Metastore [ #2777 ]. Added support for UPDATE statements in Spark [ #2193 , #2206 ]. Added support for Spark 3.1 [ #2512 ]. Added RemoveReachableFiles action [ #2415 ]. Added add_files stored procedure [ #2210 ]. Refactored Actions API and added a new entry point. Added support for Hadoop configuration overrides [ #2922 ]. Added support for the TIMESTAMP WITHOUT TIMEZONE type in Spark [ #2757 ]. Added validation that files referenced by row-level deletes are not concurrently rewritten [ #2308 ]. Important bug fixes: Core Fixed string bucketing with non-BMP characters [ #2849 ]. Fixed Parquet dictionary filtering with fixed-length byte arrays and decimals [ #2551 ]. Fixed a problem with the configuration of HiveCatalog [ #2550 ]. Fixed partition field IDs in table replacement [ #2906 ]. Hive Enabled dropping HMS tables even if the metadata on disk gets corrupted [ #2583 ]. Parquet Fixed Parquet row group filters when types are promoted from int to long or from float to double [ #2232 ] Spark Fixed MERGE INTO in Spark when used with SinglePartition partitioning [ #2584 ]. Fixed nested struct pruning in Spark [ #2877 ]. Fixed NaN handling for float and double metrics [ #2464 ]. Fixed Kryo serialization for data and delete files [ #2343 ]. Other notable changes: The Iceberg Community voted to approve version 2 of the Apache Iceberg Format Specification. The differences between version 1 and 2 of the specification are documented here . Bugfixes and stability improvements for NessieCatalog. Improvements and fixes for Iceberg\u2019s Python library. Added a vectorized reader for Apache Arrow [ #2286 ]. The following Iceberg dependencies were upgraded: Hive 2.3.8 [ #2110 ]. Avro 1.10.1 [ #1648 ]. Parquet 1.12.0 [ #2441 ].","title":"0.12.0"},{"location":"releases/#0111","text":"Git tag: 0.11.1 0.11.1 source tar.gz \u2013 signature \u2013 sha512 0.11.1 Spark 3.0 runtime Jar 0.11.1 Spark 2.4 runtime Jar 0.11.1 Flink runtime Jar 0.11.1 Hive runtime Jar Important bug fixes: #2367 prohibits deleting data files when tables are dropped if GC is disabled. #2196 fixes data loss after compaction when large files are split into multiple parts and only some parts are combined with other files. #2232 fixes row group filters with promoted types in Parquet. #2267 avoids listing non-Iceberg tables in Glue. #2254 fixes predicate pushdown for Date in Hive. #2126 fixes writing of Date, Decimal, Time, UUID types in Hive. #2241 fixes vectorized ORC reads with metadata columns in Spark. #2154 refreshes the relation cache in DELETE and MERGE operations in Spark.","title":"0.11.1"},{"location":"releases/#0110","text":"Git tag: 0.11.0 0.11.0 source tar.gz \u2013 signature \u2013 sha512 0.11.0 Spark 3.0 runtime Jar 0.11.0 Spark 2.4 runtime Jar 0.11.0 Flink runtime Jar 0.11.0 Hive runtime Jar High-level features: Core API now supports partition spec and sort order evolution Spark 3 now supports the following SQL extensions: MERGE INTO (experimental) DELETE FROM (experimental) ALTER TABLE \u2026 ADD/DROP PARTITION ALTER TABLE \u2026 WRITE ORDERED BY Invoke stored procedures using CALL Flink now supports streaming reads, CDC writes (experimental), and filter pushdown AWS module is added to support better integration with AWS, with AWS Glue catalog support and dedicated S3 FileIO implementation Nessie module is added to support integration with project Nessie Important bug fixes: #1981 fixes bug that date and timestamp transforms were producing incorrect values for dates and times before 1970. Before the fix, negative values were incorrectly transformed by date and timestamp transforms to 1 larger than the correct value. For example, day(1969-12-31 10:00:00) produced 0 instead of -1. The fix is backwards compatible, which means predicate projection can still work with the incorrectly transformed partitions written using older versions. #2091 fixes ClassCastException for type promotion int to long and float to double during Parquet vectorized read. Now Arrow vector is created by looking at Parquet file schema instead of Iceberg schema for int and float fields. #1998 fixes bug in HiveTableOperation that unlock is not called if new metadata cannot be deleted. Now it is guaranteed that unlock is always called for Hive catalog users. #1979 fixes table listing failure in Hadoop catalog when user does not have permission to some tables. Now the tables with no permission are ignored in listing. #1798 fixes scan task failure when encountering duplicate entries of data files. Spark and Flink readers can now ignore duplicated entries in data files for each scan task. #1785 fixes invalidation of metadata tables in CachingCatalog . When a table is dropped, all the metadata tables associated with it are also invalidated in the cache. #1960 fixes bug that ORC writer does not read metrics config and always use the default. Now customized metrics config is respected. Other notable changes: NaN counts are now supported in metadata Shared catalog properties are added in core library to standardize catalog level configurations Spark and Flink now support dynamically loading customized Catalog and FileIO implementations Spark 2 now supports loading tables from other catalogs, like Spark 3 Spark 3 now supports catalog names in DataFrameReader when using Iceberg as a format Flink now uses the number of Iceberg read splits as its job parallelism to improve performance and save resource. Hive (experimental) now supports INSERT INTO, case insensitive query, projection pushdown, create DDL with schema and auto type conversion ORC now supports reading tinyint, smallint, char, varchar types Avro to Iceberg schema conversion now preserves field docs","title":"0.11.0"},{"location":"releases/#0100","text":"Git tag: 0.10.0 0.10.0 source tar.gz \u2013 signature \u2013 sha512 0.10.0 Spark 3.0 runtime Jar 0.10.0 Spark 2.4 runtime Jar 0.10.0 Flink runtime Jar 0.10.0 Hive runtime Jar High-level features: Format v2 support for building row-level operations ( MERGE INTO ) in processing engines Note: format v2 is not yet finalized and does not have a forward-compatibility guarantee Flink integration for writing to Iceberg tables and reading from Iceberg tables (reading supports batch mode only) Hive integration for reading from Iceberg tables, with filter pushdown (experimental; configuration may change) Important bug fixes: #1706 fixes non-vectorized ORC reads in Spark that incorrectly skipped rows #1536 fixes ORC conversion of notIn and notEqual to match null values #1722 fixes Expressions.notNull returning an isNull predicate; API only, method was not used by processing engines #1736 fixes IllegalArgumentException in vectorized Spark reads with negative decimal values #1666 fixes file lengths returned by the ORC writer, using compressed size rather than uncompressed size #1674 removes catalog expiration in HiveCatalogs #1545 automatically refreshes tables in Spark when not caching table instances Other notable changes: The iceberg-hive module has been renamed to iceberg-hive-metastore to avoid confusion Spark 3 is based on 3.0.1 that includes the fix for SPARK-32168 Hadoop tables will recover from version hint corruption Tables can be configured with a required sort order Data file locations can be customized with a dynamically loaded LocationProvider ORC file imports can apply a name mapping for stats A more exhaustive list of changes is available under the 0.10.0 release milestone .","title":"0.10.0"},{"location":"releases/#091","text":"Git tag: 0.9.1 0.9.1 source tar.gz \u2013 signature \u2013 sha512 0.9.1 Spark 3.0 runtime Jar 0.9.1 Spark 2.4 runtime Jar","title":"0.9.1"},{"location":"releases/#090","text":"Git tag: 0.9.0 0.9.0 source tar.gz \u2013 signature \u2013 sha512 0.9.0 Spark 3.0 runtime Jar 0.9.0 Spark 2.4 runtime Jar","title":"0.9.0"},{"location":"releases/#080","text":"Git tag: apache-iceberg-0.8.0-incubating 0.8.0-incubating source tar.gz \u2013 signature \u2013 sha512 0.8.0-incubating Spark 2.4 runtime Jar","title":"0.8.0"},{"location":"releases/#070","text":"Git tag: apache-iceberg-0.7.0-incubating 0.7.0-incubating source tar.gz \u2013 signature \u2013 sha512 0.7.0-incubating Spark 2.4 runtime Jar","title":"0.7.0"},{"location":"reliability/","text":"Reliability \u00b6 Iceberg was designed to solve correctness problems that affect Hive tables running in S3. Hive tables track data files using both a central metastore for partitions and a file system for individual files. This makes atomic changes to a table\u2019s contents impossible, and eventually consistent stores like S3 may return incorrect results due to the use of listing files to reconstruct the state of a table. It also requires job planning to make many slow listing calls: O(n) with the number of partitions. Iceberg tracks the complete list of data files in each snapshot using a persistent tree structure. Every write or delete produces a new snapshot that reuses as much of the previous snapshot\u2019s metadata tree as possible to avoid high write volumes. Valid snapshots in an Iceberg table are stored in the table metadata file, along with a reference to the current snapshot. Commits replace the path of the current table metadata file using an atomic operation. This ensures that all updates to table data and metadata are atomic, and is the basis for serializable isolation . This results in improved reliability guarantees: Serializable isolation : All table changes occur in a linear history of atomic table updates Reliable reads : Readers always use a consistent snapshot of the table without holding a lock Version history and rollback : Table snapshots are kept as history and tables can roll back if a job produces bad data Safe file-level operations . By supporting atomic changes, Iceberg enables new use cases, like safely compacting small files and safely appending late data to tables This design also has performance benefits: O(1) RPCs to plan : Instead of listing O(n) directories in a table to plan a job, reading a snapshot requires O(1) RPC calls Distributed planning : File pruning and predicate push-down is distributed to jobs, removing the metastore as a bottleneck Finer granularity partitioning : Distributed planning and O(1) RPC calls remove the current barriers to finer-grained partitioning Concurrent write operations \u00b6 Iceberg supports multiple concurrent writes using optimistic concurrency. Each writer assumes that no other writers are operating and writes out new table metadata for an operation. Then, the writer attempts to commit by atomically swapping the new table metadata file for the existing metadata file. If the atomic swap fails because another writer has committed, the failed writer retries by writing a new metadata tree based on the the new current table state. Cost of retries \u00b6 Writers avoid expensive retry operations by structuring changes so that work can be reused across retries. For example, appends usually create a new manifest file for the appended data files, which can be added to the table without rewriting the manifest on every attempt. Retry validation \u00b6 Commits are structured as assumptions and actions. After a conflict, a writer checks that the assumptions are met by the current table state. If the assumptions are met, then it is safe to re-apply the actions and commit. For example, a compaction might rewrite file_a.avro and file_b.avro as merged.parquet . This is safe to commit as long as the table still contains both file_a.avro and file_b.avro . If either file was deleted by a conflicting commit, then the operation must fail. Otherwise, it is safe to remove the source files and add the merged file. Compatibility \u00b6 By avoiding file listing and rename operations, Iceberg tables are compatible with any object store. No consistent listing is required.","title":"Reliability"},{"location":"reliability/#reliability","text":"Iceberg was designed to solve correctness problems that affect Hive tables running in S3. Hive tables track data files using both a central metastore for partitions and a file system for individual files. This makes atomic changes to a table\u2019s contents impossible, and eventually consistent stores like S3 may return incorrect results due to the use of listing files to reconstruct the state of a table. It also requires job planning to make many slow listing calls: O(n) with the number of partitions. Iceberg tracks the complete list of data files in each snapshot using a persistent tree structure. Every write or delete produces a new snapshot that reuses as much of the previous snapshot\u2019s metadata tree as possible to avoid high write volumes. Valid snapshots in an Iceberg table are stored in the table metadata file, along with a reference to the current snapshot. Commits replace the path of the current table metadata file using an atomic operation. This ensures that all updates to table data and metadata are atomic, and is the basis for serializable isolation . This results in improved reliability guarantees: Serializable isolation : All table changes occur in a linear history of atomic table updates Reliable reads : Readers always use a consistent snapshot of the table without holding a lock Version history and rollback : Table snapshots are kept as history and tables can roll back if a job produces bad data Safe file-level operations . By supporting atomic changes, Iceberg enables new use cases, like safely compacting small files and safely appending late data to tables This design also has performance benefits: O(1) RPCs to plan : Instead of listing O(n) directories in a table to plan a job, reading a snapshot requires O(1) RPC calls Distributed planning : File pruning and predicate push-down is distributed to jobs, removing the metastore as a bottleneck Finer granularity partitioning : Distributed planning and O(1) RPC calls remove the current barriers to finer-grained partitioning","title":"Reliability"},{"location":"reliability/#concurrent-write-operations","text":"Iceberg supports multiple concurrent writes using optimistic concurrency. Each writer assumes that no other writers are operating and writes out new table metadata for an operation. Then, the writer attempts to commit by atomically swapping the new table metadata file for the existing metadata file. If the atomic swap fails because another writer has committed, the failed writer retries by writing a new metadata tree based on the the new current table state.","title":"Concurrent write operations"},{"location":"reliability/#cost-of-retries","text":"Writers avoid expensive retry operations by structuring changes so that work can be reused across retries. For example, appends usually create a new manifest file for the appended data files, which can be added to the table without rewriting the manifest on every attempt.","title":"Cost of retries"},{"location":"reliability/#retry-validation","text":"Commits are structured as assumptions and actions. After a conflict, a writer checks that the assumptions are met by the current table state. If the assumptions are met, then it is safe to re-apply the actions and commit. For example, a compaction might rewrite file_a.avro and file_b.avro as merged.parquet . This is safe to commit as long as the table still contains both file_a.avro and file_b.avro . If either file was deleted by a conflicting commit, then the operation must fail. Otherwise, it is safe to remove the source files and add the merged file.","title":"Retry validation"},{"location":"reliability/#compatibility","text":"By avoiding file listing and rename operations, Iceberg tables are compatible with any object store. No consistent listing is required.","title":"Compatibility"},{"location":"roadmap/","text":"Roadmap Overview \u00b6 This roadmap outlines projects that the Iceberg community is working on, their priority, and a rough size estimate. This is based on the latest community priority discussion . Each high-level item links to a Github project board that tracks the current status. Related design docs will be linked on the planning boards. Priority 1 \u00b6 API: Iceberg 1.0.0 [medium] Spark: Merge-on-read plans [large] Maintenance: Delete file compaction [medium] Flink: Upgrade to 1.13.2 (document compatibility) [medium] Python: Pythonic refactor [medium] Priority 2 \u00b6 ORC: Support delete files stored as ORC [small] Spark: DSv2 streaming improvements [small] Flink: Inline file compaction [small] Flink: Support UPSERT [small] Flink: FLIP-27 based Iceberg source [large] Views: Spec [medium] Spec: Z-ordering / Space-filling curves [medium] Spec: Snapshot tagging and branching [small] Spec: Secondary indexes [large] Spec v3: Encryption [large] Spec v3: Relative paths [large] Spec v3: Default field values [medium] Priority 3 \u00b6 Docs: versioned docs [medium] IO: Support Aliyun OSS/DLF [medium] IO: Support Dell ECS [medium] External \u00b6 PrestoDB: Iceberg PrestoDB Connector Trino: Iceberg Trino Connector","title":"Roadmap"},{"location":"roadmap/#roadmap-overview","text":"This roadmap outlines projects that the Iceberg community is working on, their priority, and a rough size estimate. This is based on the latest community priority discussion . Each high-level item links to a Github project board that tracks the current status. Related design docs will be linked on the planning boards.","title":"Roadmap Overview"},{"location":"roadmap/#priority-1","text":"API: Iceberg 1.0.0 [medium] Spark: Merge-on-read plans [large] Maintenance: Delete file compaction [medium] Flink: Upgrade to 1.13.2 (document compatibility) [medium] Python: Pythonic refactor [medium]","title":"Priority 1"},{"location":"roadmap/#priority-2","text":"ORC: Support delete files stored as ORC [small] Spark: DSv2 streaming improvements [small] Flink: Inline file compaction [small] Flink: Support UPSERT [small] Flink: FLIP-27 based Iceberg source [large] Views: Spec [medium] Spec: Z-ordering / Space-filling curves [medium] Spec: Snapshot tagging and branching [small] Spec: Secondary indexes [large] Spec v3: Encryption [large] Spec v3: Relative paths [large] Spec v3: Default field values [medium]","title":"Priority 2"},{"location":"roadmap/#priority-3","text":"Docs: versioned docs [medium] IO: Support Aliyun OSS/DLF [medium] IO: Support Dell ECS [medium]","title":"Priority 3"},{"location":"roadmap/#external","text":"PrestoDB: Iceberg PrestoDB Connector Trino: Iceberg Trino Connector","title":"External"},{"location":"schemas/","text":"Schemas \u00b6 Iceberg tables support the following types: Type Description Notes boolean True or false int 32-bit signed integers Can promote to long long 64-bit signed integers float 32-bit IEEE 754 floating point Can promote to double double 64-bit IEEE 754 floating point decimal(P,S) Fixed-point decimal; precision P, scale S Scale is fixed and precision must be 38 or less date Calendar date without timezone or time time Time of day without date, timezone Stored as microseconds timestamp Timestamp without timezone Stored as microseconds timestamptz Timestamp with timezone Stored as microseconds string Arbitrary-length character sequences Encoded with UTF-8 fixed(L) Fixed-length byte array of length L binary Arbitrary-length byte array struct<...> A record with named fields of any data type list<E> A list with elements of any data type map<K, V> A map with keys and values of any data type Iceberg tracks each field in a table schema using an ID that is never reused in a table. See correctness guarantees for more information.","title":"Schemas"},{"location":"schemas/#schemas","text":"Iceberg tables support the following types: Type Description Notes boolean True or false int 32-bit signed integers Can promote to long long 64-bit signed integers float 32-bit IEEE 754 floating point Can promote to double double 64-bit IEEE 754 floating point decimal(P,S) Fixed-point decimal; precision P, scale S Scale is fixed and precision must be 38 or less date Calendar date without timezone or time time Time of day without date, timezone Stored as microseconds timestamp Timestamp without timezone Stored as microseconds timestamptz Timestamp with timezone Stored as microseconds string Arbitrary-length character sequences Encoded with UTF-8 fixed(L) Fixed-length byte array of length L binary Arbitrary-length byte array struct<...> A record with named fields of any data type list<E> A list with elements of any data type map<K, V> A map with keys and values of any data type Iceberg tracks each field in a table schema using an ID that is never reused in a table. See correctness guarantees for more information.","title":"Schemas"},{"location":"security/","text":"Reporting Security Issues \u00b6 The Apache Iceberg Project uses the standard process outlined by the Apache Security Team for reporting vulnerabilities. Note that vulnerabilities should not be publicly disclosed until the project has responded. To report a possible security vulnerability, please email security@iceberg.apache.org . Verifying Signed Releases \u00b6 Please refer to the instructions on the Release Verification page.","title":"Security"},{"location":"security/#reporting-security-issues","text":"The Apache Iceberg Project uses the standard process outlined by the Apache Security Team for reporting vulnerabilities. Note that vulnerabilities should not be publicly disclosed until the project has responded. To report a possible security vulnerability, please email security@iceberg.apache.org .","title":"Reporting Security Issues"},{"location":"security/#verifying-signed-releases","text":"Please refer to the instructions on the Release Verification page.","title":"Verifying Signed Releases"},{"location":"snapshots/","text":"Snapshots \u00b6 Time travel \u00b6 Expiration \u00b6","title":"Snapshots"},{"location":"snapshots/#snapshots","text":"","title":"Snapshots"},{"location":"snapshots/#time-travel","text":"","title":"Time travel"},{"location":"snapshots/#expiration","text":"","title":"Expiration"},{"location":"spark-configuration/","text":"Spark Configuration \u00b6 Catalogs \u00b6 Spark 3.0 adds an API to plug in table catalogs that are used to load, create, and manage Iceberg tables. Spark catalogs are configured by setting Spark properties under spark.sql.catalog . This creates an Iceberg catalog named hive_prod that loads tables from a Hive metastore: spark.sql.catalog.hive_prod = org.apache.iceberg.spark.SparkCatalog spark.sql.catalog.hive_prod.type = hive spark.sql.catalog.hive_prod.uri = thrift://metastore-host:port # omit uri to use the same URI as Spark: hive.metastore.uris in hive-site.xml Iceberg also supports a directory-based catalog in HDFS that can be configured using type=hadoop : spark.sql.catalog.hadoop_prod = org.apache.iceberg.spark.SparkCatalog spark.sql.catalog.hadoop_prod.type = hadoop spark.sql.catalog.hadoop_prod.warehouse = hdfs://nn:8020/warehouse/path Note The Hive-based catalog only loads Iceberg tables. To load non-Iceberg tables in the same Hive metastore, use a session catalog . Catalog configuration \u00b6 A catalog is created and named by adding a property spark.sql.catalog.(catalog-name) with an implementation class for its value. Iceberg supplies two implementations: org.apache.iceberg.spark.SparkCatalog supports a Hive Metastore or a Hadoop warehouse as a catalog org.apache.iceberg.spark.SparkSessionCatalog adds support for Iceberg tables to Spark\u2019s built-in catalog, and delegates to the built-in catalog for non-Iceberg tables Both catalogs are configured using properties nested under the catalog name. Common configuration properties for Hive and Hadoop are: Property Values Description spark.sql.catalog. catalog-name .type hive or hadoop The underlying Iceberg catalog implementation, HiveCatalog , HadoopCatalog or left unset if using a custom catalog spark.sql.catalog. catalog-name .catalog-impl The underlying Iceberg catalog implementation. spark.sql.catalog. catalog-name .default-namespace default The default current namespace for the catalog spark.sql.catalog. catalog-name .uri thrift://host:port Metastore connect URI; default from hive-site.xml spark.sql.catalog. catalog-name .warehouse hdfs://nn:8020/warehouse/path Base path for the warehouse directory spark.sql.catalog. catalog-name .cache-enabled true or false Whether to enable catalog cache, default value is true Additional properties can be found in common catalog configuration . Using catalogs \u00b6 Catalog names are used in SQL queries to identify a table. In the examples above, hive_prod and hadoop_prod can be used to prefix database and table names that will be loaded from those catalogs. SELECT * FROM hive_prod.db.table -- load db.table from catalog hive_prod Spark 3 keeps track of the current catalog and namespace, which can be omitted from table names. USE hive_prod.db; SELECT * FROM table -- load db.table from catalog hive_prod To see the current catalog and namespace, run SHOW CURRENT NAMESPACE . Replacing the session catalog \u00b6 To add Iceberg table support to Spark\u2019s built-in catalog, configure spark_catalog to use Iceberg\u2019s SparkSessionCatalog . spark.sql.catalog.spark_catalog = org.apache.iceberg.spark.SparkSessionCatalog spark.sql.catalog.spark_catalog.type = hive Spark\u2019s built-in catalog supports existing v1 and v2 tables tracked in a Hive Metastore. This configures Spark to use Iceberg\u2019s SparkSessionCatalog as a wrapper around that session catalog. When a table is not an Iceberg table, the built-in catalog will be used to load it instead. This configuration can use same Hive Metastore for both Iceberg and non-Iceberg tables. Using catalog specific Hadoop configuration values \u00b6 Similar to configuring Hadoop properties by using spark.hadoop.* , it\u2019s possible to set per-catalog Hadoop configuration values when using Spark by adding the property for the catalog with the prefix spark.sql.catalog.(catalog-name).hadoop.* . These properties will take precedence over values configured globally using spark.hadoop.* and will only affect Iceberg tables. spark.sql.catalog.hadoop_prod.hadoop.fs.s3a.endpoint = http://aws-local:9000 Loading a custom catalog \u00b6 Spark supports loading a custom Iceberg Catalog implementation by specifying the catalog-impl property. Here is an example: spark.sql.catalog.custom_prod = org.apache.iceberg.spark.SparkCatalog spark.sql.catalog.custom_prod.catalog-impl = com.my.custom.CatalogImpl spark.sql.catalog.custom_prod.my-additional-catalog-config = my-value Catalogs in Spark 2.4 \u00b6 When using Iceberg 0.11.0 and later, Spark 2.4 can load tables from multiple Iceberg catalogs or from table locations. Catalogs in 2.4 are configured just like catalogs in 3.0, but only Iceberg catalogs are supported. SQL Extensions \u00b6 Iceberg 0.11.0 and later add an extension module to Spark to add new SQL commands, like CALL for stored procedures or ALTER TABLE ... WRITE ORDERED BY . Using those SQL commands requires adding Iceberg extensions to your Spark environment using the following Spark property: Spark extensions property Iceberg extensions implementation spark.sql.extensions org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions SQL extensions are not available for Spark 2.4. Runtime configuration \u00b6 Read options \u00b6 Spark read options are passed when configuring the DataFrameReader, like this: // time travel spark.read .option(\"snapshot-id\", 10963874102873L) .table(\"catalog.db.table\") Spark option Default Description snapshot-id (latest) Snapshot ID of the table snapshot to read as-of-timestamp (latest) A timestamp in milliseconds; the snapshot used will be the snapshot current at this time. split-size As per table property Overrides this table\u2019s read.split.target-size and read.split.metadata-target-size lookback As per table property Overrides this table\u2019s read.split.planning-lookback file-open-cost As per table property Overrides this table\u2019s read.split.open-file-cost vectorization-enabled As per table property Overrides this table\u2019s read.parquet.vectorization.enabled batch-size As per table property Overrides this table\u2019s read.parquet.vectorization.batch-size Write options \u00b6 Spark write options are passed when configuring the DataFrameWriter, like this: // write with Avro instead of Parquet df.write .option(\"write-format\", \"avro\") .option(\"snapshot-property.key\", \"value\") .insertInto(\"catalog.db.table\") Spark option Default Description write-format Table write.format.default File format to use for this write operation; parquet, avro, or orc target-file-size-bytes As per table property Overrides this table\u2019s write.target-file-size-bytes check-nullability true Sets the nullable check on fields snapshot-property. custom-key null Adds an entry with custom-key and corresponding value in the snapshot summary fanout-enabled false Overrides this table\u2019s write.spark.fanout.enabled check-ordering true Checks if input schema and table schema are same","title":"Configuration"},{"location":"spark-configuration/#spark-configuration","text":"","title":"Spark Configuration"},{"location":"spark-configuration/#catalogs","text":"Spark 3.0 adds an API to plug in table catalogs that are used to load, create, and manage Iceberg tables. Spark catalogs are configured by setting Spark properties under spark.sql.catalog . This creates an Iceberg catalog named hive_prod that loads tables from a Hive metastore: spark.sql.catalog.hive_prod = org.apache.iceberg.spark.SparkCatalog spark.sql.catalog.hive_prod.type = hive spark.sql.catalog.hive_prod.uri = thrift://metastore-host:port # omit uri to use the same URI as Spark: hive.metastore.uris in hive-site.xml Iceberg also supports a directory-based catalog in HDFS that can be configured using type=hadoop : spark.sql.catalog.hadoop_prod = org.apache.iceberg.spark.SparkCatalog spark.sql.catalog.hadoop_prod.type = hadoop spark.sql.catalog.hadoop_prod.warehouse = hdfs://nn:8020/warehouse/path Note The Hive-based catalog only loads Iceberg tables. To load non-Iceberg tables in the same Hive metastore, use a session catalog .","title":"Catalogs"},{"location":"spark-configuration/#catalog-configuration","text":"A catalog is created and named by adding a property spark.sql.catalog.(catalog-name) with an implementation class for its value. Iceberg supplies two implementations: org.apache.iceberg.spark.SparkCatalog supports a Hive Metastore or a Hadoop warehouse as a catalog org.apache.iceberg.spark.SparkSessionCatalog adds support for Iceberg tables to Spark\u2019s built-in catalog, and delegates to the built-in catalog for non-Iceberg tables Both catalogs are configured using properties nested under the catalog name. Common configuration properties for Hive and Hadoop are: Property Values Description spark.sql.catalog. catalog-name .type hive or hadoop The underlying Iceberg catalog implementation, HiveCatalog , HadoopCatalog or left unset if using a custom catalog spark.sql.catalog. catalog-name .catalog-impl The underlying Iceberg catalog implementation. spark.sql.catalog. catalog-name .default-namespace default The default current namespace for the catalog spark.sql.catalog. catalog-name .uri thrift://host:port Metastore connect URI; default from hive-site.xml spark.sql.catalog. catalog-name .warehouse hdfs://nn:8020/warehouse/path Base path for the warehouse directory spark.sql.catalog. catalog-name .cache-enabled true or false Whether to enable catalog cache, default value is true Additional properties can be found in common catalog configuration .","title":"Catalog configuration"},{"location":"spark-configuration/#using-catalogs","text":"Catalog names are used in SQL queries to identify a table. In the examples above, hive_prod and hadoop_prod can be used to prefix database and table names that will be loaded from those catalogs. SELECT * FROM hive_prod.db.table -- load db.table from catalog hive_prod Spark 3 keeps track of the current catalog and namespace, which can be omitted from table names. USE hive_prod.db; SELECT * FROM table -- load db.table from catalog hive_prod To see the current catalog and namespace, run SHOW CURRENT NAMESPACE .","title":"Using catalogs"},{"location":"spark-configuration/#replacing-the-session-catalog","text":"To add Iceberg table support to Spark\u2019s built-in catalog, configure spark_catalog to use Iceberg\u2019s SparkSessionCatalog . spark.sql.catalog.spark_catalog = org.apache.iceberg.spark.SparkSessionCatalog spark.sql.catalog.spark_catalog.type = hive Spark\u2019s built-in catalog supports existing v1 and v2 tables tracked in a Hive Metastore. This configures Spark to use Iceberg\u2019s SparkSessionCatalog as a wrapper around that session catalog. When a table is not an Iceberg table, the built-in catalog will be used to load it instead. This configuration can use same Hive Metastore for both Iceberg and non-Iceberg tables.","title":"Replacing the session catalog"},{"location":"spark-configuration/#using-catalog-specific-hadoop-configuration-values","text":"Similar to configuring Hadoop properties by using spark.hadoop.* , it\u2019s possible to set per-catalog Hadoop configuration values when using Spark by adding the property for the catalog with the prefix spark.sql.catalog.(catalog-name).hadoop.* . These properties will take precedence over values configured globally using spark.hadoop.* and will only affect Iceberg tables. spark.sql.catalog.hadoop_prod.hadoop.fs.s3a.endpoint = http://aws-local:9000","title":"Using catalog specific Hadoop configuration values"},{"location":"spark-configuration/#loading-a-custom-catalog","text":"Spark supports loading a custom Iceberg Catalog implementation by specifying the catalog-impl property. Here is an example: spark.sql.catalog.custom_prod = org.apache.iceberg.spark.SparkCatalog spark.sql.catalog.custom_prod.catalog-impl = com.my.custom.CatalogImpl spark.sql.catalog.custom_prod.my-additional-catalog-config = my-value","title":"Loading a custom catalog"},{"location":"spark-configuration/#catalogs-in-spark-24","text":"When using Iceberg 0.11.0 and later, Spark 2.4 can load tables from multiple Iceberg catalogs or from table locations. Catalogs in 2.4 are configured just like catalogs in 3.0, but only Iceberg catalogs are supported.","title":"Catalogs in Spark 2.4"},{"location":"spark-configuration/#sql-extensions","text":"Iceberg 0.11.0 and later add an extension module to Spark to add new SQL commands, like CALL for stored procedures or ALTER TABLE ... WRITE ORDERED BY . Using those SQL commands requires adding Iceberg extensions to your Spark environment using the following Spark property: Spark extensions property Iceberg extensions implementation spark.sql.extensions org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions SQL extensions are not available for Spark 2.4.","title":"SQL Extensions"},{"location":"spark-configuration/#runtime-configuration","text":"","title":"Runtime configuration"},{"location":"spark-configuration/#read-options","text":"Spark read options are passed when configuring the DataFrameReader, like this: // time travel spark.read .option(\"snapshot-id\", 10963874102873L) .table(\"catalog.db.table\") Spark option Default Description snapshot-id (latest) Snapshot ID of the table snapshot to read as-of-timestamp (latest) A timestamp in milliseconds; the snapshot used will be the snapshot current at this time. split-size As per table property Overrides this table\u2019s read.split.target-size and read.split.metadata-target-size lookback As per table property Overrides this table\u2019s read.split.planning-lookback file-open-cost As per table property Overrides this table\u2019s read.split.open-file-cost vectorization-enabled As per table property Overrides this table\u2019s read.parquet.vectorization.enabled batch-size As per table property Overrides this table\u2019s read.parquet.vectorization.batch-size","title":"Read options"},{"location":"spark-configuration/#write-options","text":"Spark write options are passed when configuring the DataFrameWriter, like this: // write with Avro instead of Parquet df.write .option(\"write-format\", \"avro\") .option(\"snapshot-property.key\", \"value\") .insertInto(\"catalog.db.table\") Spark option Default Description write-format Table write.format.default File format to use for this write operation; parquet, avro, or orc target-file-size-bytes As per table property Overrides this table\u2019s write.target-file-size-bytes check-nullability true Sets the nullable check on fields snapshot-property. custom-key null Adds an entry with custom-key and corresponding value in the snapshot summary fanout-enabled false Overrides this table\u2019s write.spark.fanout.enabled check-ordering true Checks if input schema and table schema are same","title":"Write options"},{"location":"spark-ddl/","text":"Spark DDL \u00b6 To use Iceberg in Spark, first configure Spark catalogs . Iceberg uses Apache Spark\u2019s DataSourceV2 API for data source and catalog implementations. Spark DSv2 is an evolving API with different levels of support in Spark versions. Spark 2.4 does not support SQL DDL. Note Spark 2.4 can\u2019t create Iceberg tables with DDL, instead use Spark 3.x or the Iceberg API . CREATE TABLE \u00b6 Spark 3.0 can create tables in any Iceberg catalog with the clause USING iceberg : CREATE TABLE prod.db.sample ( id bigint COMMENT 'unique id', data string) USING iceberg Iceberg will convert the column type in Spark to corresponding Iceberg type. Please check the section of type compatibility on creating table for details. Table create commands, including CTAS and RTAS, support the full range of Spark create clauses, including: PARTITION BY (partition-expressions) to configure partitioning LOCATION '(fully-qualified-uri)' to set the table location COMMENT 'table documentation' to set a table description TBLPROPERTIES ('key'='value', ...) to set table configuration Create commands may also set the default format with the USING clause. This is only supported for SparkCatalog because Spark handles the USING clause differently for the built-in catalog. PARTITIONED BY \u00b6 To create a partitioned table, use PARTITIONED BY : CREATE TABLE prod.db.sample ( id bigint, data string, category string) USING iceberg PARTITIONED BY (category) The PARTITIONED BY clause supports transform expressions to create hidden partitions . CREATE TABLE prod.db.sample ( id bigint, data string, category string, ts timestamp) USING iceberg PARTITIONED BY (bucket(16, id), days(ts), category) Supported transformations are: years(ts) : partition by year months(ts) : partition by month days(ts) or date(ts) : equivalent to dateint partitioning hours(ts) or date_hour(ts) : equivalent to dateint and hour partitioning bucket(N, col) : partition by hashed value mod N buckets truncate(L, col) : partition by value truncated to L Strings are truncated to the given length Integers and longs truncate to bins: truncate(10, i) produces partitions 0, 10, 20, 30, \u2026 CREATE TABLE ... AS SELECT \u00b6 Iceberg supports CTAS as an atomic operation when using a SparkCatalog . CTAS is supported, but is not atomic when using SparkSessionCatalog . CREATE TABLE prod.db.sample USING iceberg AS SELECT ... REPLACE TABLE ... AS SELECT \u00b6 Iceberg supports RTAS as an atomic operation when using a SparkCatalog . RTAS is supported, but is not atomic when using SparkSessionCatalog . Atomic table replacement creates a new snapshot with the results of the SELECT query, but keeps table history. REPLACE TABLE prod.db.sample USING iceberg AS SELECT ... CREATE OR REPLACE TABLE prod.db.sample USING iceberg AS SELECT ... The schema and partition spec will be replaced if changed. To avoid modifying the table\u2019s schema and partitioning, use INSERT OVERWRITE instead of REPLACE TABLE . The new table properties in the REPLACE TABLE command will be merged with any existing table properties. The existing table properties will be updated if changed else they are preserved. DROP TABLE \u00b6 To delete a table, run: DROP TABLE prod.db.sample ALTER TABLE \u00b6 Iceberg has full ALTER TABLE support in Spark 3, including: Renaming a table Setting or removing table properties Adding, deleting, and renaming columns Adding, deleting, and renaming nested fields Reordering top-level columns and nested struct fields Widening the type of int , float , and decimal fields Making required columns optional In addition, SQL extensions can be used to add support for partition evolution and setting a table\u2019s write order ALTER TABLE ... RENAME TO \u00b6 ALTER TABLE prod.db.sample RENAME TO prod.db.new_name ALTER TABLE ... SET TBLPROPERTIES \u00b6 ALTER TABLE prod.db.sample SET TBLPROPERTIES ( 'read.split.target-size'='268435456' ) Iceberg uses table properties to control table behavior. For a list of available properties, see Table configuration . UNSET is used to remove properties: ALTER TABLE prod.db.sample UNSET TBLPROPERTIES ('read.split.target-size') ALTER TABLE ... ADD COLUMN \u00b6 To add a column to Iceberg, use the ADD COLUMNS clause with ALTER TABLE : ALTER TABLE prod.db.sample ADD COLUMNS ( new_column string comment 'new_column docs' ) Multiple columns can be added at the same time, separated by commas. Nested columns should be identified using the full column name: -- create a struct column ALTER TABLE prod.db.sample ADD COLUMN point struct<x: double, y: double>; -- add a field to the struct ALTER TABLE prod.db.sample ADD COLUMN point.z double In Spark 2.4.4 and later, you can add columns in any position by adding FIRST or AFTER clauses: ALTER TABLE prod.db.sample ADD COLUMN new_column bigint AFTER other_column ALTER TABLE prod.db.sample ADD COLUMN nested.new_column bigint FIRST ALTER TABLE ... RENAME COLUMN \u00b6 Iceberg allows any field to be renamed. To rename a field, use RENAME COLUMN : ALTER TABLE prod.db.sample RENAME COLUMN data TO payload ALTER TABLE prod.db.sample RENAME COLUMN location.lat TO latitude Note that nested rename commands only rename the leaf field. The above command renames location.lat to location.latitude ALTER TABLE ... ALTER COLUMN \u00b6 Alter column is used to widen types, make a field optional, set comments, and reorder fields. Iceberg allows updating column types if the update is safe. Safe updates are: int to bigint float to double decimal(P,S) to decimal(P2,S) when P2 > P (scale cannot change) ALTER TABLE prod.db.sample ALTER COLUMN measurement TYPE double To add or remove columns from a struct, use ADD COLUMN or DROP COLUMN with a nested column name. Column comments can also be updated using ALTER COLUMN : ALTER TABLE prod.db.sample ALTER COLUMN measurement TYPE double COMMENT 'unit is bytes per second' ALTER TABLE prod.db.sample ALTER COLUMN measurement COMMENT 'unit is kilobytes per second' Iceberg allows reordering top-level columns or columns in a struct using FIRST and AFTER clauses: ALTER TABLE prod.db.sample ALTER COLUMN col FIRST ALTER TABLE prod.db.sample ALTER COLUMN nested.col AFTER other_col Nullability can be changed using SET NOT NULL and DROP NOT NULL : ALTER TABLE prod.db.sample ALTER COLUMN id DROP NOT NULL Note ALTER COLUMN is not used to update struct types. Use ADD COLUMN and DROP COLUMN to add or remove struct fields. ALTER TABLE ... DROP COLUMN \u00b6 To drop columns, use ALTER TABLE ... DROP COLUMN : ALTER TABLE prod.db.sample DROP COLUMN id ALTER TABLE prod.db.sample DROP COLUMN point.z ALTER TABLE SQL extensions \u00b6 These commands are available in Spark 3.x when using Iceberg SQL extensions . ALTER TABLE ... ADD PARTITION FIELD \u00b6 Iceberg supports adding new partition fields to a spec using ADD PARTITION FIELD : ALTER TABLE prod.db.sample ADD PARTITION FIELD catalog -- identity transform Partition transforms are also supported: ALTER TABLE prod.db.sample ADD PARTITION FIELD bucket(16, id) ALTER TABLE prod.db.sample ADD PARTITION FIELD truncate(data, 4) ALTER TABLE prod.db.sample ADD PARTITION FIELD years(ts) -- use optional AS keyword to specify a custom name for the partition field ALTER TABLE prod.db.sample ADD PARTITION FIELD bucket(16, id) AS shard Adding a partition field is a metadata operation and does not change any of the existing table data. New data will be written with the new partitioning, but existing data will remain in the old partition layout. Old data files will have null values for the new partition fields in metadata tables. Dynamic partition overwrite behavior will change when the table\u2019s partitioning changes because dynamic overwrite replaces partitions implicitly. To overwrite explicitly, use the new DataFrameWriterV2 API. Note To migrate from daily to hourly partitioning with transforms, it is not necessary to drop the daily partition field. Keeping the field ensures existing metadata table queries continue to work. Warning Dynamic partition overwrite behavior will change when partitioning changes For example, if you partition by days and move to partitioning by hours, overwrites will overwrite hourly partitions but not days anymore. ALTER TABLE ... DROP PARTITION FIELD \u00b6 Partition fields can be removed using DROP PARTITION FIELD : ALTER TABLE prod.db.sample DROP PARTITION FIELD catalog ALTER TABLE prod.db.sample DROP PARTITION FIELD bucket(16, id) ALTER TABLE prod.db.sample DROP PARTITION FIELD truncate(data, 4) ALTER TABLE prod.db.sample DROP PARTITION FIELD years(ts) ALTER TABLE prod.db.sample DROP PARTITION FIELD shard Note that although the partition is removed, the column will still exist in the table schema. Dropping a partition field is a metadata operation and does not change any of the existing table data. New data will be written with the new partitioning, but existing data will remain in the old partition layout. Warning Dynamic partition overwrite behavior will change when partitioning changes For example, if you partition by days and move to partitioning by hours, overwrites will overwrite hourly partitions but not days anymore. Warning Be careful when dropping a partition field because it will change the schema of metadata tables, like files , and may cause metadata queries to fail or produce different results. ALTER TABLE ... WRITE ORDERED BY \u00b6 Iceberg tables can be configured with a sort order that is used to automatically sort data that is written to the table in some engines. For example, MERGE INTO in Spark will use the table ordering. To set the write order for a table, use WRITE ORDERED BY : ALTER TABLE prod.db.sample WRITE ORDERED BY category, id -- use optional ASC/DEC keyword to specify sort order of each field (default ASC) ALTER TABLE prod.db.sample WRITE ORDERED BY category ASC, id DESC -- use optional NULLS FIRST/NULLS LAST keyword to specify null order of each field (default FIRST) ALTER TABLE prod.db.sample WRITE ORDERED BY category ASC NULLS LAST, id DESC NULLS FIRST Note Table write order does not guarantee data order for queries. It only affects how data is written to the table.","title":"DDL"},{"location":"spark-ddl/#spark-ddl","text":"To use Iceberg in Spark, first configure Spark catalogs . Iceberg uses Apache Spark\u2019s DataSourceV2 API for data source and catalog implementations. Spark DSv2 is an evolving API with different levels of support in Spark versions. Spark 2.4 does not support SQL DDL. Note Spark 2.4 can\u2019t create Iceberg tables with DDL, instead use Spark 3.x or the Iceberg API .","title":"Spark DDL"},{"location":"spark-ddl/#create-table","text":"Spark 3.0 can create tables in any Iceberg catalog with the clause USING iceberg : CREATE TABLE prod.db.sample ( id bigint COMMENT 'unique id', data string) USING iceberg Iceberg will convert the column type in Spark to corresponding Iceberg type. Please check the section of type compatibility on creating table for details. Table create commands, including CTAS and RTAS, support the full range of Spark create clauses, including: PARTITION BY (partition-expressions) to configure partitioning LOCATION '(fully-qualified-uri)' to set the table location COMMENT 'table documentation' to set a table description TBLPROPERTIES ('key'='value', ...) to set table configuration Create commands may also set the default format with the USING clause. This is only supported for SparkCatalog because Spark handles the USING clause differently for the built-in catalog.","title":"CREATE TABLE"},{"location":"spark-ddl/#partitioned-by","text":"To create a partitioned table, use PARTITIONED BY : CREATE TABLE prod.db.sample ( id bigint, data string, category string) USING iceberg PARTITIONED BY (category) The PARTITIONED BY clause supports transform expressions to create hidden partitions . CREATE TABLE prod.db.sample ( id bigint, data string, category string, ts timestamp) USING iceberg PARTITIONED BY (bucket(16, id), days(ts), category) Supported transformations are: years(ts) : partition by year months(ts) : partition by month days(ts) or date(ts) : equivalent to dateint partitioning hours(ts) or date_hour(ts) : equivalent to dateint and hour partitioning bucket(N, col) : partition by hashed value mod N buckets truncate(L, col) : partition by value truncated to L Strings are truncated to the given length Integers and longs truncate to bins: truncate(10, i) produces partitions 0, 10, 20, 30, \u2026","title":"PARTITIONED BY"},{"location":"spark-ddl/#create-table-as-select","text":"Iceberg supports CTAS as an atomic operation when using a SparkCatalog . CTAS is supported, but is not atomic when using SparkSessionCatalog . CREATE TABLE prod.db.sample USING iceberg AS SELECT ...","title":"CREATE TABLE ... AS SELECT"},{"location":"spark-ddl/#replace-table-as-select","text":"Iceberg supports RTAS as an atomic operation when using a SparkCatalog . RTAS is supported, but is not atomic when using SparkSessionCatalog . Atomic table replacement creates a new snapshot with the results of the SELECT query, but keeps table history. REPLACE TABLE prod.db.sample USING iceberg AS SELECT ... CREATE OR REPLACE TABLE prod.db.sample USING iceberg AS SELECT ... The schema and partition spec will be replaced if changed. To avoid modifying the table\u2019s schema and partitioning, use INSERT OVERWRITE instead of REPLACE TABLE . The new table properties in the REPLACE TABLE command will be merged with any existing table properties. The existing table properties will be updated if changed else they are preserved.","title":"REPLACE TABLE ... AS SELECT"},{"location":"spark-ddl/#drop-table","text":"To delete a table, run: DROP TABLE prod.db.sample","title":"DROP TABLE"},{"location":"spark-ddl/#alter-table","text":"Iceberg has full ALTER TABLE support in Spark 3, including: Renaming a table Setting or removing table properties Adding, deleting, and renaming columns Adding, deleting, and renaming nested fields Reordering top-level columns and nested struct fields Widening the type of int , float , and decimal fields Making required columns optional In addition, SQL extensions can be used to add support for partition evolution and setting a table\u2019s write order","title":"ALTER TABLE"},{"location":"spark-ddl/#alter-table-rename-to","text":"ALTER TABLE prod.db.sample RENAME TO prod.db.new_name","title":"ALTER TABLE ... RENAME TO"},{"location":"spark-ddl/#alter-table-set-tblproperties","text":"ALTER TABLE prod.db.sample SET TBLPROPERTIES ( 'read.split.target-size'='268435456' ) Iceberg uses table properties to control table behavior. For a list of available properties, see Table configuration . UNSET is used to remove properties: ALTER TABLE prod.db.sample UNSET TBLPROPERTIES ('read.split.target-size')","title":"ALTER TABLE ... SET TBLPROPERTIES"},{"location":"spark-ddl/#alter-table-add-column","text":"To add a column to Iceberg, use the ADD COLUMNS clause with ALTER TABLE : ALTER TABLE prod.db.sample ADD COLUMNS ( new_column string comment 'new_column docs' ) Multiple columns can be added at the same time, separated by commas. Nested columns should be identified using the full column name: -- create a struct column ALTER TABLE prod.db.sample ADD COLUMN point struct<x: double, y: double>; -- add a field to the struct ALTER TABLE prod.db.sample ADD COLUMN point.z double In Spark 2.4.4 and later, you can add columns in any position by adding FIRST or AFTER clauses: ALTER TABLE prod.db.sample ADD COLUMN new_column bigint AFTER other_column ALTER TABLE prod.db.sample ADD COLUMN nested.new_column bigint FIRST","title":"ALTER TABLE ... ADD COLUMN"},{"location":"spark-ddl/#alter-table-rename-column","text":"Iceberg allows any field to be renamed. To rename a field, use RENAME COLUMN : ALTER TABLE prod.db.sample RENAME COLUMN data TO payload ALTER TABLE prod.db.sample RENAME COLUMN location.lat TO latitude Note that nested rename commands only rename the leaf field. The above command renames location.lat to location.latitude","title":"ALTER TABLE ... RENAME COLUMN"},{"location":"spark-ddl/#alter-table-alter-column","text":"Alter column is used to widen types, make a field optional, set comments, and reorder fields. Iceberg allows updating column types if the update is safe. Safe updates are: int to bigint float to double decimal(P,S) to decimal(P2,S) when P2 > P (scale cannot change) ALTER TABLE prod.db.sample ALTER COLUMN measurement TYPE double To add or remove columns from a struct, use ADD COLUMN or DROP COLUMN with a nested column name. Column comments can also be updated using ALTER COLUMN : ALTER TABLE prod.db.sample ALTER COLUMN measurement TYPE double COMMENT 'unit is bytes per second' ALTER TABLE prod.db.sample ALTER COLUMN measurement COMMENT 'unit is kilobytes per second' Iceberg allows reordering top-level columns or columns in a struct using FIRST and AFTER clauses: ALTER TABLE prod.db.sample ALTER COLUMN col FIRST ALTER TABLE prod.db.sample ALTER COLUMN nested.col AFTER other_col Nullability can be changed using SET NOT NULL and DROP NOT NULL : ALTER TABLE prod.db.sample ALTER COLUMN id DROP NOT NULL Note ALTER COLUMN is not used to update struct types. Use ADD COLUMN and DROP COLUMN to add or remove struct fields.","title":"ALTER TABLE ... ALTER COLUMN"},{"location":"spark-ddl/#alter-table-drop-column","text":"To drop columns, use ALTER TABLE ... DROP COLUMN : ALTER TABLE prod.db.sample DROP COLUMN id ALTER TABLE prod.db.sample DROP COLUMN point.z","title":"ALTER TABLE ... DROP COLUMN"},{"location":"spark-ddl/#alter-table-sql-extensions","text":"These commands are available in Spark 3.x when using Iceberg SQL extensions .","title":"ALTER TABLE SQL extensions"},{"location":"spark-ddl/#alter-table-add-partition-field","text":"Iceberg supports adding new partition fields to a spec using ADD PARTITION FIELD : ALTER TABLE prod.db.sample ADD PARTITION FIELD catalog -- identity transform Partition transforms are also supported: ALTER TABLE prod.db.sample ADD PARTITION FIELD bucket(16, id) ALTER TABLE prod.db.sample ADD PARTITION FIELD truncate(data, 4) ALTER TABLE prod.db.sample ADD PARTITION FIELD years(ts) -- use optional AS keyword to specify a custom name for the partition field ALTER TABLE prod.db.sample ADD PARTITION FIELD bucket(16, id) AS shard Adding a partition field is a metadata operation and does not change any of the existing table data. New data will be written with the new partitioning, but existing data will remain in the old partition layout. Old data files will have null values for the new partition fields in metadata tables. Dynamic partition overwrite behavior will change when the table\u2019s partitioning changes because dynamic overwrite replaces partitions implicitly. To overwrite explicitly, use the new DataFrameWriterV2 API. Note To migrate from daily to hourly partitioning with transforms, it is not necessary to drop the daily partition field. Keeping the field ensures existing metadata table queries continue to work. Warning Dynamic partition overwrite behavior will change when partitioning changes For example, if you partition by days and move to partitioning by hours, overwrites will overwrite hourly partitions but not days anymore.","title":"ALTER TABLE ... ADD PARTITION FIELD"},{"location":"spark-ddl/#alter-table-drop-partition-field","text":"Partition fields can be removed using DROP PARTITION FIELD : ALTER TABLE prod.db.sample DROP PARTITION FIELD catalog ALTER TABLE prod.db.sample DROP PARTITION FIELD bucket(16, id) ALTER TABLE prod.db.sample DROP PARTITION FIELD truncate(data, 4) ALTER TABLE prod.db.sample DROP PARTITION FIELD years(ts) ALTER TABLE prod.db.sample DROP PARTITION FIELD shard Note that although the partition is removed, the column will still exist in the table schema. Dropping a partition field is a metadata operation and does not change any of the existing table data. New data will be written with the new partitioning, but existing data will remain in the old partition layout. Warning Dynamic partition overwrite behavior will change when partitioning changes For example, if you partition by days and move to partitioning by hours, overwrites will overwrite hourly partitions but not days anymore. Warning Be careful when dropping a partition field because it will change the schema of metadata tables, like files , and may cause metadata queries to fail or produce different results.","title":"ALTER TABLE ... DROP PARTITION FIELD"},{"location":"spark-ddl/#alter-table-write-ordered-by","text":"Iceberg tables can be configured with a sort order that is used to automatically sort data that is written to the table in some engines. For example, MERGE INTO in Spark will use the table ordering. To set the write order for a table, use WRITE ORDERED BY : ALTER TABLE prod.db.sample WRITE ORDERED BY category, id -- use optional ASC/DEC keyword to specify sort order of each field (default ASC) ALTER TABLE prod.db.sample WRITE ORDERED BY category ASC, id DESC -- use optional NULLS FIRST/NULLS LAST keyword to specify null order of each field (default FIRST) ALTER TABLE prod.db.sample WRITE ORDERED BY category ASC NULLS LAST, id DESC NULLS FIRST Note Table write order does not guarantee data order for queries. It only affects how data is written to the table.","title":"ALTER TABLE ... WRITE ORDERED BY"},{"location":"spark-procedures/","text":"Spark Procedures \u00b6 To use Iceberg in Spark, first configure Spark catalogs . Stored procedures are only available when using Iceberg SQL extensions in Spark 3.x. Usage \u00b6 Procedures can be used from any configured Iceberg catalog with CALL . All procedures are in the namespace system . CALL supports passing arguments by name (recommended) or by position. Mixing position and named arguments is not supported. Named arguments \u00b6 All procedure arguments are named. When passing arguments by name, arguments can be in any order and any optional argument can be omitted. CALL catalog_name.system.procedure_name(arg_name_2 => arg_2, arg_name_1 => arg_1) Positional arguments \u00b6 When passing arguments by position, only the ending arguments may be omitted if they are optional. CALL catalog_name.system.procedure_name(arg_1, arg_2, ... arg_n) Snapshot management \u00b6 rollback_to_snapshot \u00b6 Roll back a table to a specific snapshot ID. To roll back to a specific time, use rollback_to_timestamp . Note this procedure invalidates all cached Spark plans that reference the affected table. Usage \u00b6 Argument Name Required? Type Description table \u2714\ufe0f string Name of the table to update snapshot_id \u2714\ufe0f long Snapshot ID to rollback to Output \u00b6 Output Name Type Description previous_snapshot_id long The current snapshot ID before the rollback current_snapshot_id long The new current snapshot ID Example \u00b6 Roll back table db.sample to snapshot ID 1 : CALL catalog_name.system.rollback_to_snapshot('db.sample', 1) rollback_to_timestamp \u00b6 Roll back a table to the snapshot that was current at some time. Note this procedure invalidates all cached Spark plans that reference the affected table. Usage \u00b6 Argument Name Required? Type Description table \u2714\ufe0f string Name of the table to update timestamp \u2714\ufe0f timestamp A timestamp to rollback to Output \u00b6 Output Name Type Description previous_snapshot_id long The current snapshot ID before the rollback current_snapshot_id long The new current snapshot ID Example \u00b6 Roll back db.sample to one day CALL catalog_name.system.rollback_to_timestamp('db.sample', TIMESTAMP '2021-06-30 00:00:00.000') set_current_snapshot \u00b6 Sets the current snapshot ID for a table. Unlike rollback, the snapshot is not required to be an ancestor of the current table state. Note this procedure invalidates all cached Spark plans that reference the affected table. Usage \u00b6 Argument Name Required? Type Description table \u2714\ufe0f string Name of the table to update snapshot_id \u2714\ufe0f long Snapshot ID to set as current Output \u00b6 Output Name Type Description previous_snapshot_id long The current snapshot ID before the rollback current_snapshot_id long The new current snapshot ID Example \u00b6 Set the current snapshot for db.sample to 1: CALL catalog_name.system.set_current_snapshot('db.sample', 1) cherrypick_snapshot \u00b6 Cherry-picks changes from a snapshot into the current table state. Cherry-picking creates a new snapshot from an existing snapshot without altering or removing the original. Only append and dynamic overwrite snapshots can be cherry-picked. Note this procedure invalidates all cached Spark plans that reference the affected table. Usage \u00b6 Argument Name Required? Type Description table \u2714\ufe0f string Name of the table to update snapshot_id \u2714\ufe0f long The snapshot ID to cherry-pick Output \u00b6 Output Name Type Description source_snapshot_id long The table\u2019s current snapshot before the cherry-pick current_snapshot_id long The snapshot ID created by applying the cherry-pick Examples \u00b6 Cherry-pick snapshot 1 CALL catalog_name.system.cherrypick_snapshot('my_table', 1) Cherry-pick snapshot 1 with named args CALL catalog_name.system.cherrypick_snapshot(snapshot_id => 1, table => 'my_table' ) Metadata management \u00b6 Many maintenance actions can be performed using Iceberg stored procedures. expire_snapshots \u00b6 Each write/update/delete/upsert/compaction in Iceberg produces a new snapshot while keeping the old data and metadata around for snapshot isolation and time travel. The expire_snapshots procedure can be used to remove older snapshots and their files which are no longer needed. This procedure will remove old snapshots and data files which are uniquely required by those old snapshots. This means the expire_snapshots procedure will never remove files which are still required by a non-expired snapshot. Usage \u00b6 Argument Name Required? Type Description table \u2714\ufe0f string Name of the table to update older_than \ufe0f timestamp Timestamp before which snapshots will be removed (Default: 5 days ago) retain_last int Number of ancestor snapshots to preserve regardless of older_than (defaults to 1) Output \u00b6 Output Name Type Description deleted_data_files_count long Number of data files deleted by this operation deleted_manifest_files_count long Number of manifest files deleted by this operation deleted_manifest_lists_count long Number of manifest List files deleted by this operation Examples \u00b6 Remove snapshots older than one day, but retain the last 100 snapshots: CALL hive_prod.system.expire_snapshots('db.sample', TIMESTAMP '2021-06-30 00:00:00.000', 100) Erase all snapshots older than the current timestamp but retain the last 5 snapshots: CALL hive_prod.system.expire_snapshots(table => 'db.sample', older_than => now(), retain_last => 5) remove_orphan_files \u00b6 Used to remove files which are not referenced in any metadata files of an Iceberg table and can thus be considered \u201corphaned\u201d. Usage \u00b6 Argument Name Required? Type Description table \u2714\ufe0f string Name of the table to clean older_than \ufe0f timestamp Remove orphan files created before this timestamp (Defaults to 3 days ago) location string Directory to look for files in (defaults to the table\u2019s location) dry_run boolean When true, don\u2019t actually remove files (defaults to false) Output \u00b6 Output Name Type Description orphan_file_location String The path to each file determined to be an orphan by this command Examples \u00b6 List all the files that are candidates for removal by performing a dry run of the remove_orphan_files command on this table without actually removing them: CALL catalog_name.system.remove_orphan_files(table => 'db.sample', dry_run => true) Remove any files in the tablelocation/data folder which are not known to the table db.sample . CALL catalog_name.system.remove_orphan_files(table => 'db.sample', location => 'tablelocation/data') rewrite_manifests \u00b6 Rewrite manifests for a table to optimize scan planning. Data files in manifests are sorted by fields in the partition spec. This procedure runs in parallel using a Spark job. See the RewriteManifestsAction Javadoc to see more configuration options. Note this procedure invalidates all cached Spark plans that reference the affected table. Usage \u00b6 Argument Name Required? Type Description table \u2714\ufe0f string Name of the table to update use_caching \ufe0f boolean Use Spark caching during operation (defaults to true) Output \u00b6 Output Name Type Description rewritten_manifests_count int Number of manifests which were re-written by this command added_mainfests_count int Number of new manifest files which were written by this command Examples \u00b6 Rewrite the manifests in table db.sample and align manifest files with table partitioning. CALL catalog_name.system.rewrite_manifests('db.sample') Rewrite the manifests in table db.sample and disable the use of Spark caching. This could be done to avoid memory issues on executors. CALL catalog_name.system.rewrite_manifests('db.sample', false) Table migration \u00b6 The snapshot and migrate procedures help test and migrate existing Hive or Spark tables to Iceberg. snapshot \u00b6 Create a light-weight temporary copy of a table for testing, without changing the source table. The newly created table can be changed or written to without affecting the source table, but the snapshot uses the original table\u2019s data files. When inserts or overwrites run on the snapshot, new files are placed in the snapshot table\u2019s location rather than the original table location. When finished testing a snapshot table, clean it up by running DROP TABLE . Note Because tables created by snapshot are not the sole owners of their data files, they are prohibited from actions like expire_snapshots which would physically delete data files. Iceberg deletes, which only effect metadata, are still allowed. In addition, any operations which affect the original data files will disrupt the Snapshot\u2019s integrity. DELETE statements executed against the original Hive table will remove original data files and the snapshot table will no longer be able to access them. See migrate to replace an existing table with an Iceberg table. Usage \u00b6 Argument Name Required? Type Description source_table \u2714\ufe0f string Name of the table to snapshot table \u2714\ufe0f string Name of the new Iceberg table to create location string Table location for the new table (delegated to the catalog by default) properties \ufe0f map Properties to add to the newly created table Output \u00b6 Output Name Type Description imported_files_count long Number of files added to the new table Examples \u00b6 Make an isolated Iceberg table which references table db.sample named db.snap at the catalog\u2019s default location for db.snap . CALL catalog_name.system.snapshot('db.sample', 'db.snap') Migrate an isolated Iceberg table which references table db.sample named db.snap at a manually specified location /tmp/temptable/ . CALL catalog_name.system.snapshot('db.sample', 'db.snap', '/tmp/temptable/') migrate \u00b6 Replace a table with an Iceberg table, loaded with the source\u2019s data files. Table schema, partitioning, properties, and location will be copied from the source table. Migrate will fail if any table partition uses an unsupported format. Supported formats are Avro, Parquet, and ORC. Existing data files are added to the Iceberg table\u2019s metadata and can be read using a name-to-id mapping created from the original table schema. To leave the original table intact while testing, use snapshot to create new temporary table that shares source data files and schema. Usage \u00b6 Argument Name Required? Type Description table \u2714\ufe0f string Name of the table to migrate properties \ufe0f map Properties for the new Iceberg table Output \u00b6 Output Name Type Description migrated_files_count long Number of files appended to the Iceberg table Examples \u00b6 Migrate the table db.sample in Spark\u2019s default catalog to an Iceberg table and add a property \u2018foo\u2019 set to \u2018bar\u2019: CALL catalog_name.system.migrate('spark_catalog.db.sample', map('foo', 'bar')) Migrate db.sample in the current catalog to an Iceberg table without adding any additional properties: CALL catalog_name.system.migrate('db.sample') add_files \u00b6 Attempts to directly add files from a Hive or file based table into a given Iceberg table. Unlike migrate or snapshot, add_files can import files from a specific partition or partitions and does not create a new Iceberg table. This command will create metadata for the new files and will not move them. This procedure will not analyze the schema of the files to determine if they actually match the schema of the Iceberg table. Upon completion, the Iceberg table will then treat these files as if they are part of the set of files owned by Iceberg. This means any subsequent expire_snapshot calls will be able to physically delete the added files. This method should not be used if migrate or snapshot are possible. Usage \u00b6 Argument Name Required? Type Description table \u2714\ufe0f string Table which will have files added to source_table \u2714\ufe0f string Table where files should come from, paths are also possible in the form of `file_format`.`path` partition_filter \ufe0f map A map of partitions in the source table to import from Warning : Schema is not validated, adding files with different schema to the Iceberg table will cause issues. Warning : Files added by this method can be physically deleted by Iceberg operations Examples \u00b6 Add the files from table db.src_table , a Hive or Spark table registered in the session Catalog, to Iceberg table db.tbl . Only add files that exist within partitions where part_col_1 is equal to A . CALL spark_catalog.system.add_files( table => 'db.tbl', source_table => 'db.src_tbl', partition_filter => map('part_col_1', 'A') ) Add files from a parquet file based table at location path/to/table to the Iceberg table db.tbl . Add all files regardless of what partition they belong to. CALL spark_catalog.system.add_files( table => 'db.tbl', source_table => '`parquet`.`path/to/table`' ) Metadata information \u00b6 ancestors_of \u00b6 Report the live snapshot IDs of parents of a specified snapshot Usage \u00b6 Argument Name Required? Type Description table \u2714\ufe0f string Name of the table to report live snapshot IDs snapshot_id \ufe0f long Use a specified snapshot to get the live snapshot IDs of parents tip : Using snapshot_id Given snapshots history with roll back to B and addition of C\u2019 -> D\u2019 shell A -> B - > C -> D \\ -> C' -> (D') Not specifying the snapshot ID would return A -> B -> C\u2019 -> D\u2019, while providing the snapshot ID of D as an argument would return A-> B -> C -> D Output \u00b6 Output Name Type Description snapshot_id long the ancestor snapshot id timestamp long snapshot creation time Examples \u00b6 Get all the snapshot ancestors of current snapshots(default) CALL spark_catalog.system.ancestors_of('db.tbl') Get all the snapshot ancestors by a particular snapshot CALL spark_catalog.system.ancestors_of('db.tbl', 1) CALL spark_catalog.system.ancestors_of(snapshot_id => 1, table => 'db.tbl')","title":"Maintenance Procedures"},{"location":"spark-procedures/#spark-procedures","text":"To use Iceberg in Spark, first configure Spark catalogs . Stored procedures are only available when using Iceberg SQL extensions in Spark 3.x.","title":"Spark Procedures"},{"location":"spark-procedures/#usage","text":"Procedures can be used from any configured Iceberg catalog with CALL . All procedures are in the namespace system . CALL supports passing arguments by name (recommended) or by position. Mixing position and named arguments is not supported.","title":"Usage"},{"location":"spark-procedures/#named-arguments","text":"All procedure arguments are named. When passing arguments by name, arguments can be in any order and any optional argument can be omitted. CALL catalog_name.system.procedure_name(arg_name_2 => arg_2, arg_name_1 => arg_1)","title":"Named arguments"},{"location":"spark-procedures/#positional-arguments","text":"When passing arguments by position, only the ending arguments may be omitted if they are optional. CALL catalog_name.system.procedure_name(arg_1, arg_2, ... arg_n)","title":"Positional arguments"},{"location":"spark-procedures/#snapshot-management","text":"","title":"Snapshot management"},{"location":"spark-procedures/#rollback_to_snapshot","text":"Roll back a table to a specific snapshot ID. To roll back to a specific time, use rollback_to_timestamp . Note this procedure invalidates all cached Spark plans that reference the affected table.","title":"rollback_to_snapshot"},{"location":"spark-procedures/#usage_1","text":"Argument Name Required? Type Description table \u2714\ufe0f string Name of the table to update snapshot_id \u2714\ufe0f long Snapshot ID to rollback to","title":"Usage"},{"location":"spark-procedures/#output","text":"Output Name Type Description previous_snapshot_id long The current snapshot ID before the rollback current_snapshot_id long The new current snapshot ID","title":"Output"},{"location":"spark-procedures/#example","text":"Roll back table db.sample to snapshot ID 1 : CALL catalog_name.system.rollback_to_snapshot('db.sample', 1)","title":"Example"},{"location":"spark-procedures/#rollback_to_timestamp","text":"Roll back a table to the snapshot that was current at some time. Note this procedure invalidates all cached Spark plans that reference the affected table.","title":"rollback_to_timestamp"},{"location":"spark-procedures/#usage_2","text":"Argument Name Required? Type Description table \u2714\ufe0f string Name of the table to update timestamp \u2714\ufe0f timestamp A timestamp to rollback to","title":"Usage"},{"location":"spark-procedures/#output_1","text":"Output Name Type Description previous_snapshot_id long The current snapshot ID before the rollback current_snapshot_id long The new current snapshot ID","title":"Output"},{"location":"spark-procedures/#example_1","text":"Roll back db.sample to one day CALL catalog_name.system.rollback_to_timestamp('db.sample', TIMESTAMP '2021-06-30 00:00:00.000')","title":"Example"},{"location":"spark-procedures/#set_current_snapshot","text":"Sets the current snapshot ID for a table. Unlike rollback, the snapshot is not required to be an ancestor of the current table state. Note this procedure invalidates all cached Spark plans that reference the affected table.","title":"set_current_snapshot"},{"location":"spark-procedures/#usage_3","text":"Argument Name Required? Type Description table \u2714\ufe0f string Name of the table to update snapshot_id \u2714\ufe0f long Snapshot ID to set as current","title":"Usage"},{"location":"spark-procedures/#output_2","text":"Output Name Type Description previous_snapshot_id long The current snapshot ID before the rollback current_snapshot_id long The new current snapshot ID","title":"Output"},{"location":"spark-procedures/#example_2","text":"Set the current snapshot for db.sample to 1: CALL catalog_name.system.set_current_snapshot('db.sample', 1)","title":"Example"},{"location":"spark-procedures/#cherrypick_snapshot","text":"Cherry-picks changes from a snapshot into the current table state. Cherry-picking creates a new snapshot from an existing snapshot without altering or removing the original. Only append and dynamic overwrite snapshots can be cherry-picked. Note this procedure invalidates all cached Spark plans that reference the affected table.","title":"cherrypick_snapshot"},{"location":"spark-procedures/#usage_4","text":"Argument Name Required? Type Description table \u2714\ufe0f string Name of the table to update snapshot_id \u2714\ufe0f long The snapshot ID to cherry-pick","title":"Usage"},{"location":"spark-procedures/#output_3","text":"Output Name Type Description source_snapshot_id long The table\u2019s current snapshot before the cherry-pick current_snapshot_id long The snapshot ID created by applying the cherry-pick","title":"Output"},{"location":"spark-procedures/#examples","text":"Cherry-pick snapshot 1 CALL catalog_name.system.cherrypick_snapshot('my_table', 1) Cherry-pick snapshot 1 with named args CALL catalog_name.system.cherrypick_snapshot(snapshot_id => 1, table => 'my_table' )","title":"Examples"},{"location":"spark-procedures/#metadata-management","text":"Many maintenance actions can be performed using Iceberg stored procedures.","title":"Metadata management"},{"location":"spark-procedures/#expire_snapshots","text":"Each write/update/delete/upsert/compaction in Iceberg produces a new snapshot while keeping the old data and metadata around for snapshot isolation and time travel. The expire_snapshots procedure can be used to remove older snapshots and their files which are no longer needed. This procedure will remove old snapshots and data files which are uniquely required by those old snapshots. This means the expire_snapshots procedure will never remove files which are still required by a non-expired snapshot.","title":"expire_snapshots"},{"location":"spark-procedures/#usage_5","text":"Argument Name Required? Type Description table \u2714\ufe0f string Name of the table to update older_than \ufe0f timestamp Timestamp before which snapshots will be removed (Default: 5 days ago) retain_last int Number of ancestor snapshots to preserve regardless of older_than (defaults to 1)","title":"Usage"},{"location":"spark-procedures/#output_4","text":"Output Name Type Description deleted_data_files_count long Number of data files deleted by this operation deleted_manifest_files_count long Number of manifest files deleted by this operation deleted_manifest_lists_count long Number of manifest List files deleted by this operation","title":"Output"},{"location":"spark-procedures/#examples_1","text":"Remove snapshots older than one day, but retain the last 100 snapshots: CALL hive_prod.system.expire_snapshots('db.sample', TIMESTAMP '2021-06-30 00:00:00.000', 100) Erase all snapshots older than the current timestamp but retain the last 5 snapshots: CALL hive_prod.system.expire_snapshots(table => 'db.sample', older_than => now(), retain_last => 5)","title":"Examples"},{"location":"spark-procedures/#remove_orphan_files","text":"Used to remove files which are not referenced in any metadata files of an Iceberg table and can thus be considered \u201corphaned\u201d.","title":"remove_orphan_files"},{"location":"spark-procedures/#usage_6","text":"Argument Name Required? Type Description table \u2714\ufe0f string Name of the table to clean older_than \ufe0f timestamp Remove orphan files created before this timestamp (Defaults to 3 days ago) location string Directory to look for files in (defaults to the table\u2019s location) dry_run boolean When true, don\u2019t actually remove files (defaults to false)","title":"Usage"},{"location":"spark-procedures/#output_5","text":"Output Name Type Description orphan_file_location String The path to each file determined to be an orphan by this command","title":"Output"},{"location":"spark-procedures/#examples_2","text":"List all the files that are candidates for removal by performing a dry run of the remove_orphan_files command on this table without actually removing them: CALL catalog_name.system.remove_orphan_files(table => 'db.sample', dry_run => true) Remove any files in the tablelocation/data folder which are not known to the table db.sample . CALL catalog_name.system.remove_orphan_files(table => 'db.sample', location => 'tablelocation/data')","title":"Examples"},{"location":"spark-procedures/#rewrite_manifests","text":"Rewrite manifests for a table to optimize scan planning. Data files in manifests are sorted by fields in the partition spec. This procedure runs in parallel using a Spark job. See the RewriteManifestsAction Javadoc to see more configuration options. Note this procedure invalidates all cached Spark plans that reference the affected table.","title":"rewrite_manifests"},{"location":"spark-procedures/#usage_7","text":"Argument Name Required? Type Description table \u2714\ufe0f string Name of the table to update use_caching \ufe0f boolean Use Spark caching during operation (defaults to true)","title":"Usage"},{"location":"spark-procedures/#output_6","text":"Output Name Type Description rewritten_manifests_count int Number of manifests which were re-written by this command added_mainfests_count int Number of new manifest files which were written by this command","title":"Output"},{"location":"spark-procedures/#examples_3","text":"Rewrite the manifests in table db.sample and align manifest files with table partitioning. CALL catalog_name.system.rewrite_manifests('db.sample') Rewrite the manifests in table db.sample and disable the use of Spark caching. This could be done to avoid memory issues on executors. CALL catalog_name.system.rewrite_manifests('db.sample', false)","title":"Examples"},{"location":"spark-procedures/#table-migration","text":"The snapshot and migrate procedures help test and migrate existing Hive or Spark tables to Iceberg.","title":"Table migration"},{"location":"spark-procedures/#snapshot","text":"Create a light-weight temporary copy of a table for testing, without changing the source table. The newly created table can be changed or written to without affecting the source table, but the snapshot uses the original table\u2019s data files. When inserts or overwrites run on the snapshot, new files are placed in the snapshot table\u2019s location rather than the original table location. When finished testing a snapshot table, clean it up by running DROP TABLE . Note Because tables created by snapshot are not the sole owners of their data files, they are prohibited from actions like expire_snapshots which would physically delete data files. Iceberg deletes, which only effect metadata, are still allowed. In addition, any operations which affect the original data files will disrupt the Snapshot\u2019s integrity. DELETE statements executed against the original Hive table will remove original data files and the snapshot table will no longer be able to access them. See migrate to replace an existing table with an Iceberg table.","title":"snapshot"},{"location":"spark-procedures/#usage_8","text":"Argument Name Required? Type Description source_table \u2714\ufe0f string Name of the table to snapshot table \u2714\ufe0f string Name of the new Iceberg table to create location string Table location for the new table (delegated to the catalog by default) properties \ufe0f map Properties to add to the newly created table","title":"Usage"},{"location":"spark-procedures/#output_7","text":"Output Name Type Description imported_files_count long Number of files added to the new table","title":"Output"},{"location":"spark-procedures/#examples_4","text":"Make an isolated Iceberg table which references table db.sample named db.snap at the catalog\u2019s default location for db.snap . CALL catalog_name.system.snapshot('db.sample', 'db.snap') Migrate an isolated Iceberg table which references table db.sample named db.snap at a manually specified location /tmp/temptable/ . CALL catalog_name.system.snapshot('db.sample', 'db.snap', '/tmp/temptable/')","title":"Examples"},{"location":"spark-procedures/#migrate","text":"Replace a table with an Iceberg table, loaded with the source\u2019s data files. Table schema, partitioning, properties, and location will be copied from the source table. Migrate will fail if any table partition uses an unsupported format. Supported formats are Avro, Parquet, and ORC. Existing data files are added to the Iceberg table\u2019s metadata and can be read using a name-to-id mapping created from the original table schema. To leave the original table intact while testing, use snapshot to create new temporary table that shares source data files and schema.","title":"migrate"},{"location":"spark-procedures/#usage_9","text":"Argument Name Required? Type Description table \u2714\ufe0f string Name of the table to migrate properties \ufe0f map Properties for the new Iceberg table","title":"Usage"},{"location":"spark-procedures/#output_8","text":"Output Name Type Description migrated_files_count long Number of files appended to the Iceberg table","title":"Output"},{"location":"spark-procedures/#examples_5","text":"Migrate the table db.sample in Spark\u2019s default catalog to an Iceberg table and add a property \u2018foo\u2019 set to \u2018bar\u2019: CALL catalog_name.system.migrate('spark_catalog.db.sample', map('foo', 'bar')) Migrate db.sample in the current catalog to an Iceberg table without adding any additional properties: CALL catalog_name.system.migrate('db.sample')","title":"Examples"},{"location":"spark-procedures/#add_files","text":"Attempts to directly add files from a Hive or file based table into a given Iceberg table. Unlike migrate or snapshot, add_files can import files from a specific partition or partitions and does not create a new Iceberg table. This command will create metadata for the new files and will not move them. This procedure will not analyze the schema of the files to determine if they actually match the schema of the Iceberg table. Upon completion, the Iceberg table will then treat these files as if they are part of the set of files owned by Iceberg. This means any subsequent expire_snapshot calls will be able to physically delete the added files. This method should not be used if migrate or snapshot are possible.","title":"add_files"},{"location":"spark-procedures/#usage_10","text":"Argument Name Required? Type Description table \u2714\ufe0f string Table which will have files added to source_table \u2714\ufe0f string Table where files should come from, paths are also possible in the form of `file_format`.`path` partition_filter \ufe0f map A map of partitions in the source table to import from Warning : Schema is not validated, adding files with different schema to the Iceberg table will cause issues. Warning : Files added by this method can be physically deleted by Iceberg operations","title":"Usage"},{"location":"spark-procedures/#examples_6","text":"Add the files from table db.src_table , a Hive or Spark table registered in the session Catalog, to Iceberg table db.tbl . Only add files that exist within partitions where part_col_1 is equal to A . CALL spark_catalog.system.add_files( table => 'db.tbl', source_table => 'db.src_tbl', partition_filter => map('part_col_1', 'A') ) Add files from a parquet file based table at location path/to/table to the Iceberg table db.tbl . Add all files regardless of what partition they belong to. CALL spark_catalog.system.add_files( table => 'db.tbl', source_table => '`parquet`.`path/to/table`' )","title":"Examples"},{"location":"spark-procedures/#metadata-information","text":"","title":"Metadata information"},{"location":"spark-procedures/#ancestors_of","text":"Report the live snapshot IDs of parents of a specified snapshot","title":"ancestors_of"},{"location":"spark-procedures/#usage_11","text":"Argument Name Required? Type Description table \u2714\ufe0f string Name of the table to report live snapshot IDs snapshot_id \ufe0f long Use a specified snapshot to get the live snapshot IDs of parents tip : Using snapshot_id Given snapshots history with roll back to B and addition of C\u2019 -> D\u2019 shell A -> B - > C -> D \\ -> C' -> (D') Not specifying the snapshot ID would return A -> B -> C\u2019 -> D\u2019, while providing the snapshot ID of D as an argument would return A-> B -> C -> D","title":"Usage"},{"location":"spark-procedures/#output_9","text":"Output Name Type Description snapshot_id long the ancestor snapshot id timestamp long snapshot creation time","title":"Output"},{"location":"spark-procedures/#examples_7","text":"Get all the snapshot ancestors of current snapshots(default) CALL spark_catalog.system.ancestors_of('db.tbl') Get all the snapshot ancestors by a particular snapshot CALL spark_catalog.system.ancestors_of('db.tbl', 1) CALL spark_catalog.system.ancestors_of(snapshot_id => 1, table => 'db.tbl')","title":"Examples"},{"location":"spark-queries/","text":"Spark Queries \u00b6 To use Iceberg in Spark, first configure Spark catalogs . Iceberg uses Apache Spark\u2019s DataSourceV2 API for data source and catalog implementations. Spark DSv2 is an evolving API with different levels of support in Spark versions: Feature support Spark 3.0 Spark 2.4 Notes SELECT \u2714\ufe0f DataFrame reads \u2714\ufe0f \u2714\ufe0f Metadata table SELECT \u2714\ufe0f History metadata table \u2714\ufe0f \u2714\ufe0f Snapshots metadata table \u2714\ufe0f \u2714\ufe0f Files metadata table \u2714\ufe0f \u2714\ufe0f Manifests metadata table \u2714\ufe0f \u2714\ufe0f Querying with SQL \u00b6 In Spark 3, tables use identifiers that include a catalog name . SELECT * FROM prod.db.table -- catalog: prod, namespace: db, table: table Metadata tables, like history and snapshots , can use the Iceberg table name as a namespace. For example, to read from the files metadata table for prod.db.table , run: SELECT * FROM prod.db.table.files Querying with DataFrames \u00b6 To load a table as a DataFrame, use table : val df = spark.table(\"prod.db.table\") Catalogs with DataFrameReader \u00b6 Iceberg 0.11.0 adds multi-catalog support to DataFrameReader in both Spark 3.x and 2.4. Paths and table names can be loaded with Spark\u2019s DataFrameReader interface. How tables are loaded depends on how the identifier is specified. When using spark.read.format(\"iceberg\").path(table) or spark.table(table) the table variable can take a number of forms as listed below: file:/path/to/table : loads a HadoopTable at given path tablename : loads currentCatalog.currentNamespace.tablename catalog.tablename : loads tablename from the specified catalog. namespace.tablename : loads namespace.tablename from current catalog catalog.namespace.tablename : loads namespace.tablename from the specified catalog. namespace1.namespace2.tablename : loads namespace1.namespace2.tablename from current catalog The above list is in order of priority. For example: a matching catalog will take priority over any namespace resolution. Time travel \u00b6 To select a specific table snapshot or the snapshot at some time, Iceberg supports two Spark read options: snapshot-id selects a specific table snapshot as-of-timestamp selects the current snapshot at a timestamp, in milliseconds // time travel to October 26, 1986 at 01:21:00 spark.read .option(\"as-of-timestamp\", \"499162860000\") .format(\"iceberg\") .load(\"path/to/table\") // time travel to snapshot with ID 10963874102873L spark.read .option(\"snapshot-id\", 10963874102873L) .format(\"iceberg\") .load(\"path/to/table\") Note Spark does not currently support using option with table in DataFrameReader commands. All options will be silently ignored. Do not use table when attempting to time-travel or use other options. Options will be supported with table in Spark 3.1 - SPARK-32592 . Time travel is not yet supported by Spark\u2019s SQL syntax. Spark 2.4 \u00b6 Spark 2.4 requires using the DataFrame reader with iceberg as a format, because 2.4 does not support direct SQL queries: // named metastore table spark.read.format(\"iceberg\").load(\"catalog.db.table\") // Hadoop path table spark.read.format(\"iceberg\").load(\"hdfs://nn:8020/path/to/table\") Spark 2.4 with SQL \u00b6 To run SQL SELECT statements on Iceberg tables in 2.4, register the DataFrame as a temporary table: val df = spark.read.format(\"iceberg\").load(\"db.table\") df.createOrReplaceTempView(\"table\") spark.sql(\"\"\"select count(1) from table\"\"\").show() Inspecting tables \u00b6 To inspect a table\u2019s history, snapshots, and other metadata, Iceberg supports metadata tables. Metadata tables are identified by adding the metadata table name after the original table name. For example, history for db.table is read using db.table.history . Note As of Spark 3.0, the format of the table name for inspection ( catalog.database.table.metadata ) doesn\u2019t work with Spark\u2019s default catalog ( spark_catalog ). If you\u2019ve replaced the default catalog, you may want to use DataFrameReader API to inspect the table. History \u00b6 To show table history, run: SELECT * FROM prod.db.table.history +-------------------------+---------------------+---------------------+---------------------+ | made_current_at | snapshot_id | parent_id | is_current_ancestor | +-------------------------+---------------------+---------------------+---------------------+ | 2019-02-08 03:29:51.215 | 5781947118336215154 | NULL | true | | 2019-02-08 03:47:55.948 | 5179299526185056830 | 5781947118336215154 | true | | 2019-02-09 16:24:30.13 | 296410040247533544 | 5179299526185056830 | false | | 2019-02-09 16:32:47.336 | 2999875608062437330 | 5179299526185056830 | true | | 2019-02-09 19:42:03.919 | 8924558786060583479 | 2999875608062437330 | true | | 2019-02-09 19:49:16.343 | 6536733823181975045 | 8924558786060583479 | true | +-------------------------+---------------------+---------------------+---------------------+ Note This shows a commit that was rolled back. The example has two snapshots with the same parent, and one is not an ancestor of the current table state. Snapshots \u00b6 To show the valid snapshots for a table, run: SELECT * FROM prod.db.table.snapshots +-------------------------+----------------+-----------+-----------+----------------------------------------------------+-------------------------------------------------------+ | committed_at | snapshot_id | parent_id | operation | manifest_list | summary | +-------------------------+----------------+-----------+-----------+----------------------------------------------------+-------------------------------------------------------+ | 2019-02-08 03:29:51.215 | 57897183625154 | null | append | s3://.../table/metadata/snap-57897183625154-1.avro | { added-records -> 2478404, total-records -> 2478404, | | | | | | | added-data-files -> 438, total-data-files -> 438, | | | | | | | spark.app.id -> application_1520379288616_155055 } | | ... | ... | ... | ... | ... | ... | +-------------------------+----------------+-----------+-----------+----------------------------------------------------+-------------------------------------------------------+ You can also join snapshots to table history. For example, this query will show table history, with the application ID that wrote each snapshot: select h.made_current_at, s.operation, h.snapshot_id, h.is_current_ancestor, s.summary['spark.app.id'] from prod.db.table.history h join prod.db.table.snapshots s on h.snapshot_id = s.snapshot_id order by made_current_at +-------------------------+-----------+----------------+---------------------+----------------------------------+ | made_current_at | operation | snapshot_id | is_current_ancestor | summary[spark.app.id] | +-------------------------+-----------+----------------+---------------------+----------------------------------+ | 2019-02-08 03:29:51.215 | append | 57897183625154 | true | application_1520379288616_155055 | | 2019-02-09 16:24:30.13 | delete | 29641004024753 | false | application_1520379288616_151109 | | 2019-02-09 16:32:47.336 | append | 57897183625154 | true | application_1520379288616_155055 | | 2019-02-08 03:47:55.948 | overwrite | 51792995261850 | true | application_1520379288616_152431 | +-------------------------+-----------+----------------+---------------------+----------------------------------+ Files \u00b6 To show a table\u2019s data files and each file\u2019s metadata, run: SELECT * FROM prod.db.table.files +-------------------------------------------------------------------------+-------------+--------------+--------------------+--------------------+------------------+-------------------+------------------+-----------------+-----------------+--------------+---------------+ | file_path | file_format | record_count | file_size_in_bytes | column_sizes | value_counts | null_value_counts | nan_value_counts | lower_bounds | upper_bounds | key_metadata | split_offsets | +-------------------------------------------------------------------------+-------------+--------------+--------------------+--------------------+------------------+-------------------+------------------+-----------------+-----------------+--------------+---------------+ | s3:/.../table/data/00000-3-8d6d60e8-d427-4809-bcf0-f5d45a4aad96.parquet | PARQUET | 1 | 597 | [1 -> 90, 2 -> 62] | [1 -> 1, 2 -> 1] | [1 -> 0, 2 -> 0] | [] | [1 -> , 2 -> c] | [1 -> , 2 -> c] | null | [4] | | s3:/.../table/data/00001-4-8d6d60e8-d427-4809-bcf0-f5d45a4aad96.parquet | PARQUET | 1 | 597 | [1 -> 90, 2 -> 62] | [1 -> 1, 2 -> 1] | [1 -> 0, 2 -> 0] | [] | [1 -> , 2 -> b] | [1 -> , 2 -> b] | null | [4] | | s3:/.../table/data/00002-5-8d6d60e8-d427-4809-bcf0-f5d45a4aad96.parquet | PARQUET | 1 | 597 | [1 -> 90, 2 -> 62] | [1 -> 1, 2 -> 1] | [1 -> 0, 2 -> 0] | [] | [1 -> , 2 -> a] | [1 -> , 2 -> a] | null | [4] | +-------------------------------------------------------------------------+-------------+--------------+--------------------+--------------------+------------------+-------------------+------------------+-----------------+-----------------+--------------+---------------+ Manifests \u00b6 To show a table\u2019s file manifests and each file\u2019s metadata, run: SELECT * FROM prod.db.table.manifests +----------------------------------------------------------------------+--------+-------------------+---------------------+------------------------+---------------------------+--------------------------+--------------------------------------+ | path | length | partition_spec_id | added_snapshot_id | added_data_files_count | existing_data_files_count | deleted_data_files_count | partition_summaries | +----------------------------------------------------------------------+--------+-------------------+---------------------+------------------------+---------------------------+--------------------------+--------------------------------------+ | s3://.../table/metadata/45b5290b-ee61-4788-b324-b1e2735c0e10-m0.avro | 4479 | 0 | 6668963634911763636 | 8 | 0 | 0 | [[false,null,2019-05-13,2019-05-15]] | +----------------------------------------------------------------------+--------+-------------------+---------------------+------------------------+---------------------------+--------------------------+--------------------------------------+ Note: 1. Fields within partition_summaries column of the manifests table correspond to field_summary structs within manifest list , with the following order: - contains_null - contains_nan - lower_bound - upper_bound 2. contains_nan could return null, which indicates that this information is not available from files\u2019 metadata. This usually occurs when reading from V1 table, where contains_nan is not populated. Inspecting with DataFrames \u00b6 Metadata tables can be loaded in Spark 2.4 or Spark 3 using the DataFrameReader API: // named metastore table spark.read.format(\"iceberg\").load(\"db.table.files\").show(truncate = false) // Hadoop path table spark.read.format(\"iceberg\").load(\"hdfs://nn:8020/path/to/table#files\").show(truncate = false)","title":"Queries"},{"location":"spark-queries/#spark-queries","text":"To use Iceberg in Spark, first configure Spark catalogs . Iceberg uses Apache Spark\u2019s DataSourceV2 API for data source and catalog implementations. Spark DSv2 is an evolving API with different levels of support in Spark versions: Feature support Spark 3.0 Spark 2.4 Notes SELECT \u2714\ufe0f DataFrame reads \u2714\ufe0f \u2714\ufe0f Metadata table SELECT \u2714\ufe0f History metadata table \u2714\ufe0f \u2714\ufe0f Snapshots metadata table \u2714\ufe0f \u2714\ufe0f Files metadata table \u2714\ufe0f \u2714\ufe0f Manifests metadata table \u2714\ufe0f \u2714\ufe0f","title":"Spark Queries"},{"location":"spark-queries/#querying-with-sql","text":"In Spark 3, tables use identifiers that include a catalog name . SELECT * FROM prod.db.table -- catalog: prod, namespace: db, table: table Metadata tables, like history and snapshots , can use the Iceberg table name as a namespace. For example, to read from the files metadata table for prod.db.table , run: SELECT * FROM prod.db.table.files","title":"Querying with SQL"},{"location":"spark-queries/#querying-with-dataframes","text":"To load a table as a DataFrame, use table : val df = spark.table(\"prod.db.table\")","title":"Querying with DataFrames"},{"location":"spark-queries/#catalogs-with-dataframereader","text":"Iceberg 0.11.0 adds multi-catalog support to DataFrameReader in both Spark 3.x and 2.4. Paths and table names can be loaded with Spark\u2019s DataFrameReader interface. How tables are loaded depends on how the identifier is specified. When using spark.read.format(\"iceberg\").path(table) or spark.table(table) the table variable can take a number of forms as listed below: file:/path/to/table : loads a HadoopTable at given path tablename : loads currentCatalog.currentNamespace.tablename catalog.tablename : loads tablename from the specified catalog. namespace.tablename : loads namespace.tablename from current catalog catalog.namespace.tablename : loads namespace.tablename from the specified catalog. namespace1.namespace2.tablename : loads namespace1.namespace2.tablename from current catalog The above list is in order of priority. For example: a matching catalog will take priority over any namespace resolution.","title":"Catalogs with DataFrameReader"},{"location":"spark-queries/#time-travel","text":"To select a specific table snapshot or the snapshot at some time, Iceberg supports two Spark read options: snapshot-id selects a specific table snapshot as-of-timestamp selects the current snapshot at a timestamp, in milliseconds // time travel to October 26, 1986 at 01:21:00 spark.read .option(\"as-of-timestamp\", \"499162860000\") .format(\"iceberg\") .load(\"path/to/table\") // time travel to snapshot with ID 10963874102873L spark.read .option(\"snapshot-id\", 10963874102873L) .format(\"iceberg\") .load(\"path/to/table\") Note Spark does not currently support using option with table in DataFrameReader commands. All options will be silently ignored. Do not use table when attempting to time-travel or use other options. Options will be supported with table in Spark 3.1 - SPARK-32592 . Time travel is not yet supported by Spark\u2019s SQL syntax.","title":"Time travel"},{"location":"spark-queries/#spark-24","text":"Spark 2.4 requires using the DataFrame reader with iceberg as a format, because 2.4 does not support direct SQL queries: // named metastore table spark.read.format(\"iceberg\").load(\"catalog.db.table\") // Hadoop path table spark.read.format(\"iceberg\").load(\"hdfs://nn:8020/path/to/table\")","title":"Spark 2.4"},{"location":"spark-queries/#spark-24-with-sql","text":"To run SQL SELECT statements on Iceberg tables in 2.4, register the DataFrame as a temporary table: val df = spark.read.format(\"iceberg\").load(\"db.table\") df.createOrReplaceTempView(\"table\") spark.sql(\"\"\"select count(1) from table\"\"\").show()","title":"Spark 2.4 with SQL"},{"location":"spark-queries/#inspecting-tables","text":"To inspect a table\u2019s history, snapshots, and other metadata, Iceberg supports metadata tables. Metadata tables are identified by adding the metadata table name after the original table name. For example, history for db.table is read using db.table.history . Note As of Spark 3.0, the format of the table name for inspection ( catalog.database.table.metadata ) doesn\u2019t work with Spark\u2019s default catalog ( spark_catalog ). If you\u2019ve replaced the default catalog, you may want to use DataFrameReader API to inspect the table.","title":"Inspecting tables"},{"location":"spark-queries/#history","text":"To show table history, run: SELECT * FROM prod.db.table.history +-------------------------+---------------------+---------------------+---------------------+ | made_current_at | snapshot_id | parent_id | is_current_ancestor | +-------------------------+---------------------+---------------------+---------------------+ | 2019-02-08 03:29:51.215 | 5781947118336215154 | NULL | true | | 2019-02-08 03:47:55.948 | 5179299526185056830 | 5781947118336215154 | true | | 2019-02-09 16:24:30.13 | 296410040247533544 | 5179299526185056830 | false | | 2019-02-09 16:32:47.336 | 2999875608062437330 | 5179299526185056830 | true | | 2019-02-09 19:42:03.919 | 8924558786060583479 | 2999875608062437330 | true | | 2019-02-09 19:49:16.343 | 6536733823181975045 | 8924558786060583479 | true | +-------------------------+---------------------+---------------------+---------------------+ Note This shows a commit that was rolled back. The example has two snapshots with the same parent, and one is not an ancestor of the current table state.","title":"History"},{"location":"spark-queries/#snapshots","text":"To show the valid snapshots for a table, run: SELECT * FROM prod.db.table.snapshots +-------------------------+----------------+-----------+-----------+----------------------------------------------------+-------------------------------------------------------+ | committed_at | snapshot_id | parent_id | operation | manifest_list | summary | +-------------------------+----------------+-----------+-----------+----------------------------------------------------+-------------------------------------------------------+ | 2019-02-08 03:29:51.215 | 57897183625154 | null | append | s3://.../table/metadata/snap-57897183625154-1.avro | { added-records -> 2478404, total-records -> 2478404, | | | | | | | added-data-files -> 438, total-data-files -> 438, | | | | | | | spark.app.id -> application_1520379288616_155055 } | | ... | ... | ... | ... | ... | ... | +-------------------------+----------------+-----------+-----------+----------------------------------------------------+-------------------------------------------------------+ You can also join snapshots to table history. For example, this query will show table history, with the application ID that wrote each snapshot: select h.made_current_at, s.operation, h.snapshot_id, h.is_current_ancestor, s.summary['spark.app.id'] from prod.db.table.history h join prod.db.table.snapshots s on h.snapshot_id = s.snapshot_id order by made_current_at +-------------------------+-----------+----------------+---------------------+----------------------------------+ | made_current_at | operation | snapshot_id | is_current_ancestor | summary[spark.app.id] | +-------------------------+-----------+----------------+---------------------+----------------------------------+ | 2019-02-08 03:29:51.215 | append | 57897183625154 | true | application_1520379288616_155055 | | 2019-02-09 16:24:30.13 | delete | 29641004024753 | false | application_1520379288616_151109 | | 2019-02-09 16:32:47.336 | append | 57897183625154 | true | application_1520379288616_155055 | | 2019-02-08 03:47:55.948 | overwrite | 51792995261850 | true | application_1520379288616_152431 | +-------------------------+-----------+----------------+---------------------+----------------------------------+","title":"Snapshots"},{"location":"spark-queries/#files","text":"To show a table\u2019s data files and each file\u2019s metadata, run: SELECT * FROM prod.db.table.files +-------------------------------------------------------------------------+-------------+--------------+--------------------+--------------------+------------------+-------------------+------------------+-----------------+-----------------+--------------+---------------+ | file_path | file_format | record_count | file_size_in_bytes | column_sizes | value_counts | null_value_counts | nan_value_counts | lower_bounds | upper_bounds | key_metadata | split_offsets | +-------------------------------------------------------------------------+-------------+--------------+--------------------+--------------------+------------------+-------------------+------------------+-----------------+-----------------+--------------+---------------+ | s3:/.../table/data/00000-3-8d6d60e8-d427-4809-bcf0-f5d45a4aad96.parquet | PARQUET | 1 | 597 | [1 -> 90, 2 -> 62] | [1 -> 1, 2 -> 1] | [1 -> 0, 2 -> 0] | [] | [1 -> , 2 -> c] | [1 -> , 2 -> c] | null | [4] | | s3:/.../table/data/00001-4-8d6d60e8-d427-4809-bcf0-f5d45a4aad96.parquet | PARQUET | 1 | 597 | [1 -> 90, 2 -> 62] | [1 -> 1, 2 -> 1] | [1 -> 0, 2 -> 0] | [] | [1 -> , 2 -> b] | [1 -> , 2 -> b] | null | [4] | | s3:/.../table/data/00002-5-8d6d60e8-d427-4809-bcf0-f5d45a4aad96.parquet | PARQUET | 1 | 597 | [1 -> 90, 2 -> 62] | [1 -> 1, 2 -> 1] | [1 -> 0, 2 -> 0] | [] | [1 -> , 2 -> a] | [1 -> , 2 -> a] | null | [4] | +-------------------------------------------------------------------------+-------------+--------------+--------------------+--------------------+------------------+-------------------+------------------+-----------------+-----------------+--------------+---------------+","title":"Files"},{"location":"spark-queries/#manifests","text":"To show a table\u2019s file manifests and each file\u2019s metadata, run: SELECT * FROM prod.db.table.manifests +----------------------------------------------------------------------+--------+-------------------+---------------------+------------------------+---------------------------+--------------------------+--------------------------------------+ | path | length | partition_spec_id | added_snapshot_id | added_data_files_count | existing_data_files_count | deleted_data_files_count | partition_summaries | +----------------------------------------------------------------------+--------+-------------------+---------------------+------------------------+---------------------------+--------------------------+--------------------------------------+ | s3://.../table/metadata/45b5290b-ee61-4788-b324-b1e2735c0e10-m0.avro | 4479 | 0 | 6668963634911763636 | 8 | 0 | 0 | [[false,null,2019-05-13,2019-05-15]] | +----------------------------------------------------------------------+--------+-------------------+---------------------+------------------------+---------------------------+--------------------------+--------------------------------------+ Note: 1. Fields within partition_summaries column of the manifests table correspond to field_summary structs within manifest list , with the following order: - contains_null - contains_nan - lower_bound - upper_bound 2. contains_nan could return null, which indicates that this information is not available from files\u2019 metadata. This usually occurs when reading from V1 table, where contains_nan is not populated.","title":"Manifests"},{"location":"spark-queries/#inspecting-with-dataframes","text":"Metadata tables can be loaded in Spark 2.4 or Spark 3 using the DataFrameReader API: // named metastore table spark.read.format(\"iceberg\").load(\"db.table.files\").show(truncate = false) // Hadoop path table spark.read.format(\"iceberg\").load(\"hdfs://nn:8020/path/to/table#files\").show(truncate = false)","title":"Inspecting with DataFrames"},{"location":"spark-structured-streaming/","text":"Spark Structured Streaming \u00b6 Iceberg uses Apache Spark\u2019s DataSourceV2 API for data source and catalog implementations. Spark DSv2 is an evolving API with different levels of support in Spark versions. As of Spark 3.0, DataFrame reads and writes are supported. Feature support Spark 3.0 Spark 2.4 Notes DataFrame write \u2714 \u2714 Streaming Writes \u00b6 To write values from streaming query to Iceberg table, use DataStreamWriter : val tableIdentifier: String = ... data.writeStream .format(\"iceberg\") .outputMode(\"append\") .trigger(Trigger.ProcessingTime(1, TimeUnit.MINUTES)) .option(\"path\", tableIdentifier) .option(\"checkpointLocation\", checkpointPath) .start() The tableIdentifier can be: The fully-qualified path to a HDFS table, like hdfs://nn:8020/path/to/table A table name if the table is tracked by a catalog, like database.table_name Iceberg doesn\u2019t support \u201ccontinuous processing\u201d, as it doesn\u2019t provide the interface to \u201ccommit\u201d the output. Iceberg supports append and complete output modes: append : appends the rows of every micro-batch to the table complete : replaces the table contents every micro-batch The table should be created in prior to start the streaming query. Refer SQL create table on Spark page to see how to create the Iceberg table. Writing against partitioned table \u00b6 Iceberg requires the data to be sorted according to the partition spec per task (Spark partition) in prior to write against partitioned table. For batch queries you\u2019re encouraged to do explicit sort to fulfill the requirement (see here ), but the approach would bring additional latency as repartition and sort are considered as heavy operations for streaming workload. To avoid additional latency, you can enable fanout writer to eliminate the requirement. val tableIdentifier: String = ... data.writeStream .format(\"iceberg\") .outputMode(\"append\") .trigger(Trigger.ProcessingTime(1, TimeUnit.MINUTES)) .option(\"path\", tableIdentifier) .option(\"fanout-enabled\", \"true\") .option(\"checkpointLocation\", checkpointPath) .start() Fanout writer opens the files per partition value and doesn\u2019t close these files till write task is finished. This functionality is discouraged for batch query, as explicit sort against output rows isn\u2019t expensive for batch workload. Maintenance for streaming tables \u00b6 Streaming queries can create new table versions quickly, which creates lots of table metadata to track those versions. Maintaining metadata by tuning the rate of commits, expiring old snapshots, and automatically cleaning up metadata files is highly recommended. Tune the rate of commits \u00b6 Having high rate of commits would produce lots of data files, manifests, and snapshots which leads the table hard to maintain. We encourage having trigger interval 1 minute at minimum, and increase the interval if needed. The triggers section in Structured Streaming Programming Guide documents how to configure the interval. Expire old snapshots \u00b6 Each micro-batch written to a table produces a new snapshot, which are tracked in table metadata until they are expired to remove the metadata and any data files that are no longer needed. Snapshots accumulate quickly with frequent commits, so it is highly recommended that tables written by streaming queries are regularly maintained . Compacting data files \u00b6 The amount of data written in a micro batch is typically small, which can cause the table metadata to track lots of small files. Compacting small files into larger files reduces the metadata needed by the table, and increases query efficiency. Rewrite manifests \u00b6 To optimize write latency on streaming workload, Iceberg may write the new snapshot with a \u201cfast\u201d append that does not automatically compact manifests. This could lead lots of small manifest files. Manifests can be rewritten to optimize queries and to compact .","title":"Structured Streaming"},{"location":"spark-structured-streaming/#spark-structured-streaming","text":"Iceberg uses Apache Spark\u2019s DataSourceV2 API for data source and catalog implementations. Spark DSv2 is an evolving API with different levels of support in Spark versions. As of Spark 3.0, DataFrame reads and writes are supported. Feature support Spark 3.0 Spark 2.4 Notes DataFrame write \u2714 \u2714","title":"Spark Structured Streaming"},{"location":"spark-structured-streaming/#streaming-writes","text":"To write values from streaming query to Iceberg table, use DataStreamWriter : val tableIdentifier: String = ... data.writeStream .format(\"iceberg\") .outputMode(\"append\") .trigger(Trigger.ProcessingTime(1, TimeUnit.MINUTES)) .option(\"path\", tableIdentifier) .option(\"checkpointLocation\", checkpointPath) .start() The tableIdentifier can be: The fully-qualified path to a HDFS table, like hdfs://nn:8020/path/to/table A table name if the table is tracked by a catalog, like database.table_name Iceberg doesn\u2019t support \u201ccontinuous processing\u201d, as it doesn\u2019t provide the interface to \u201ccommit\u201d the output. Iceberg supports append and complete output modes: append : appends the rows of every micro-batch to the table complete : replaces the table contents every micro-batch The table should be created in prior to start the streaming query. Refer SQL create table on Spark page to see how to create the Iceberg table.","title":"Streaming Writes"},{"location":"spark-structured-streaming/#writing-against-partitioned-table","text":"Iceberg requires the data to be sorted according to the partition spec per task (Spark partition) in prior to write against partitioned table. For batch queries you\u2019re encouraged to do explicit sort to fulfill the requirement (see here ), but the approach would bring additional latency as repartition and sort are considered as heavy operations for streaming workload. To avoid additional latency, you can enable fanout writer to eliminate the requirement. val tableIdentifier: String = ... data.writeStream .format(\"iceberg\") .outputMode(\"append\") .trigger(Trigger.ProcessingTime(1, TimeUnit.MINUTES)) .option(\"path\", tableIdentifier) .option(\"fanout-enabled\", \"true\") .option(\"checkpointLocation\", checkpointPath) .start() Fanout writer opens the files per partition value and doesn\u2019t close these files till write task is finished. This functionality is discouraged for batch query, as explicit sort against output rows isn\u2019t expensive for batch workload.","title":"Writing against partitioned table"},{"location":"spark-structured-streaming/#maintenance-for-streaming-tables","text":"Streaming queries can create new table versions quickly, which creates lots of table metadata to track those versions. Maintaining metadata by tuning the rate of commits, expiring old snapshots, and automatically cleaning up metadata files is highly recommended.","title":"Maintenance for streaming tables"},{"location":"spark-structured-streaming/#tune-the-rate-of-commits","text":"Having high rate of commits would produce lots of data files, manifests, and snapshots which leads the table hard to maintain. We encourage having trigger interval 1 minute at minimum, and increase the interval if needed. The triggers section in Structured Streaming Programming Guide documents how to configure the interval.","title":"Tune the rate of commits"},{"location":"spark-structured-streaming/#expire-old-snapshots","text":"Each micro-batch written to a table produces a new snapshot, which are tracked in table metadata until they are expired to remove the metadata and any data files that are no longer needed. Snapshots accumulate quickly with frequent commits, so it is highly recommended that tables written by streaming queries are regularly maintained .","title":"Expire old snapshots"},{"location":"spark-structured-streaming/#compacting-data-files","text":"The amount of data written in a micro batch is typically small, which can cause the table metadata to track lots of small files. Compacting small files into larger files reduces the metadata needed by the table, and increases query efficiency.","title":"Compacting data files"},{"location":"spark-structured-streaming/#rewrite-manifests","text":"To optimize write latency on streaming workload, Iceberg may write the new snapshot with a \u201cfast\u201d append that does not automatically compact manifests. This could lead lots of small manifest files. Manifests can be rewritten to optimize queries and to compact .","title":"Rewrite manifests"},{"location":"spark-writes/","text":"Spark Writes \u00b6 To use Iceberg in Spark, first configure Spark catalogs . Some plans are only available when using Iceberg SQL extensions in Spark 3.x. Iceberg uses Apache Spark\u2019s DataSourceV2 API for data source and catalog implementations. Spark DSv2 is an evolving API with different levels of support in Spark versions: Feature support Spark 3.0 Spark 2.4 Notes SQL insert into \u2714\ufe0f SQL merge into \u2714\ufe0f \u26a0 Requires Iceberg Spark extensions SQL insert overwrite \u2714\ufe0f SQL delete from \u2714\ufe0f \u26a0 Row-level delete requires Spark extensions SQL update \u2714\ufe0f \u26a0 Requires Iceberg Spark extensions DataFrame append \u2714\ufe0f \u2714\ufe0f DataFrame overwrite \u2714\ufe0f \u2714\ufe0f \u26a0 Behavior changed in Spark 3.0 DataFrame CTAS and RTAS \u2714\ufe0f Writing with SQL \u00b6 Spark 3 supports SQL INSERT INTO , MERGE INTO , and INSERT OVERWRITE , as well as the new DataFrameWriterV2 API. INSERT INTO \u00b6 To append new data to a table, use INSERT INTO . INSERT INTO prod.db.table VALUES (1, 'a'), (2, 'b') INSERT INTO prod.db.table SELECT ... MERGE INTO \u00b6 Spark 3 added support for MERGE INTO queries that can express row-level updates. Iceberg supports MERGE INTO by rewriting data files that contain rows that need to be updated in an overwrite commit. MERGE INTO is recommended instead of INSERT OVERWRITE because Iceberg can replace only the affected data files, and because the data overwritten by a dynamic overwrite may change if the table\u2019s partitioning changes. MERGE INTO syntax \u00b6 MERGE INTO updates a table, called the target table, using a set of updates from another query, called the source . The update for a row in the target table is found using the ON clause that is like a join condition. MERGE INTO prod.db.target t -- a target table USING (SELECT ...) s -- the source updates ON t.id = s.id -- condition to find updates for target rows WHEN ... -- updates Updates to rows in the target table are listed using WHEN MATCHED ... THEN ... . Multiple MATCHED clauses can be added with conditions that determine when each match should be applied. The first matching expression is used. WHEN MATCHED AND s.op = 'delete' THEN DELETE WHEN MATCHED AND t.count IS NULL AND s.op = 'increment' THEN UPDATE SET t.count = 0 WHEN MATCHED AND s.op = 'increment' THEN UPDATE SET t.count = t.count + 1 Source rows (updates) that do not match can be inserted: WHEN NOT MATCHED THEN INSERT * Inserts also support additional conditions: WHEN NOT MATCHED AND s.event_time > still_valid_threshold THEN INSERT (id, count) VALUES (s.id, 1) Only one record in the source data can update any given row of the target table, or else an error will be thrown. INSERT OVERWRITE \u00b6 INSERT OVERWRITE can replace data in the table with the result of a query. Overwrites are atomic operations for Iceberg tables. The partitions that will be replaced by INSERT OVERWRITE depends on Spark\u2019s partition overwrite mode and the partitioning of a table. MERGE INTO can rewrite only affected data files and has more easily understood behavior, so it is recommended instead of INSERT OVERWRITE . Warning Spark 3.0.0 has a correctness bug that affects dynamic INSERT OVERWRITE with hidden partitioning, SPARK-32168 . For tables with hidden partitions , make sure you use Spark 3.0.1. Overwrite behavior \u00b6 Spark\u2019s default overwrite mode is static , but dynamic overwrite mode is recommended when writing to Iceberg tables. Static overwrite mode determines which partitions to overwrite in a table by converting the PARTITION clause to a filter, but the PARTITION clause can only reference table columns. Dynamic overwrite mode is configured by setting spark.sql.sources.partitionOverwriteMode=dynamic . To demonstrate the behavior of dynamic and static overwrites, consider a logs table defined by the following DDL: CREATE TABLE prod.my_app.logs ( uuid string NOT NULL, level string NOT NULL, ts timestamp NOT NULL, message string) USING iceberg PARTITIONED BY (level, hours(ts)) Dynamic overwrite \u00b6 When Spark\u2019s overwrite mode is dynamic, partitions that have rows produced by the SELECT query will be replaced. For example, this query removes duplicate log events from the example logs table. INSERT OVERWRITE prod.my_app.logs SELECT uuid, first(level), first(ts), first(message) FROM prod.my_app.logs WHERE cast(ts as date) = '2020-07-01' GROUP BY uuid In dynamic mode, this will replace any partition with rows in the SELECT result. Because the date of all rows is restricted to 1 July, only hours of that day will be replaced. Static overwrite \u00b6 When Spark\u2019s overwrite mode is static, the PARTITION clause is converted to a filter that is used to delete from the table. If the PARTITION clause is omitted, all partitions will be replaced. Because there is no PARTITION clause in the query above, it will drop all existing rows in the table when run in static mode, but will only write the logs from 1 July. To overwrite just the partitions that were loaded, add a PARTITION clause that aligns with the SELECT query filter: INSERT OVERWRITE prod.my_app.logs PARTITION (level = 'INFO') SELECT uuid, first(level), first(ts), first(message) FROM prod.my_app.logs WHERE level = 'INFO' GROUP BY uuid Note that this mode cannot replace hourly partitions like the dynamic example query because the PARTITION clause can only reference table columns, not hidden partitions. DELETE FROM \u00b6 Spark 3 added support for DELETE FROM queries to remove data from tables. Delete queries accept a filter to match rows to delete. DELETE FROM prod.db.table WHERE ts >= '2020-05-01 00:00:00' and ts < '2020-06-01 00:00:00' DELETE FROM prod.db.all_events WHERE session_time < (SELECT min(session_time) FROM prod.db.good_events) DELETE FROM prod.db.orders AS t1 WHERE EXISTS (SELECT oid FROM prod.db.returned_orders WHERE t1.oid = oid) If the delete filter matches entire partitions of the table, Iceberg will perform a metadata-only delete. If the filter matches individual rows of a table, then Iceberg will rewrite only the affected data files. UPDATE \u00b6 Spark 3.1 added support for UPDATE queries that update matching rows in tables. Update queries accept a filter to match rows to update. UPDATE prod.db.table SET c1 = 'update_c1', c2 = 'update_c2' WHERE ts >= '2020-05-01 00:00:00' and ts < '2020-06-01 00:00:00' UPDATE prod.db.all_events SET session_time = 0, ignored = true WHERE session_time < (SELECT min(session_time) FROM prod.db.good_events) UPDATE prod.db.orders AS t1 SET order_status = 'returned' WHERE EXISTS (SELECT oid FROM prod.db.returned_orders WHERE t1.oid = oid) For more complex row-level updates based on incoming data, see the section on MERGE INTO . Writing with DataFrames \u00b6 Spark 3 introduced the new DataFrameWriterV2 API for writing to tables using data frames. The v2 API is recommended for several reasons: CTAS, RTAS, and overwrite by filter are supported All operations consistently write columns to a table by name Hidden partition expressions are supported in partitionedBy Overwrite behavior is explicit, either dynamic or by a user-supplied filter The behavior of each operation corresponds to SQL statements df.writeTo(t).create() is equivalent to CREATE TABLE AS SELECT df.writeTo(t).replace() is equivalent to REPLACE TABLE AS SELECT df.writeTo(t).append() is equivalent to INSERT INTO df.writeTo(t).overwritePartitions() is equivalent to dynamic INSERT OVERWRITE The v1 DataFrame write API is still supported, but is not recommended. Warning When writing with the v1 DataFrame API in Spark 3, use saveAsTable or insertInto to load tables with a catalog. Using format(\"iceberg\") loads an isolated table reference that will not automatically refresh tables used by queries. Appending data \u00b6 To append a dataframe to an Iceberg table, use append : val data: DataFrame = ... data.writeTo(\"prod.db.table\").append() Spark 2.4 \u00b6 In Spark 2.4, use the v1 API with append mode and iceberg format: data.write .format(\"iceberg\") .mode(\"append\") .save(\"db.table\") Overwriting data \u00b6 To overwrite partitions dynamically, use overwritePartitions() : val data: DataFrame = ... data.writeTo(\"prod.db.table\").overwritePartitions() To explicitly overwrite partitions, use overwrite to supply a filter: data.writeTo(\"prod.db.table\").overwrite($\"level\" === \"INFO\") Spark 2.4 \u00b6 In Spark 2.4, overwrite values in an Iceberg table with overwrite mode and iceberg format: data.write .format(\"iceberg\") .mode(\"overwrite\") .save(\"db.table\") Warning The behavior of overwrite mode changed between Spark 2.4 and Spark 3 . The behavior of DataFrameWriter overwrite mode was undefined in Spark 2.4, but is required to overwrite the entire table in Spark 3. Because of this new requirement, the Iceberg source\u2019s behavior changed in Spark 3. In Spark 2.4, the behavior was to dynamically overwrite partitions. To use the Spark 2.4 behavior, add option overwrite-mode=dynamic . Creating tables \u00b6 To run a CTAS or RTAS, use create , replace , or createOrReplace operations: val data: DataFrame = ... data.writeTo(\"prod.db.table\").create() Create and replace operations support table configuration methods, like partitionedBy and tableProperty : data.writeTo(\"prod.db.table\") .tableProperty(\"write.format.default\", \"orc\") .partitionBy($\"level\", days($\"ts\")) .createOrReplace() Writing to partitioned tables \u00b6 Iceberg requires the data to be sorted according to the partition spec per task (Spark partition) in prior to write against partitioned table. This applies both Writing with SQL and Writing with DataFrames. Note Explicit sort is necessary because Spark doesn\u2019t allow Iceberg to request a sort before writing as of Spark 3.0. SPARK-23889 is filed to enable Iceberg to require specific distribution & sort order to Spark. Note Both global sort ( orderBy / sort ) and local sort ( sortWithinPartitions ) work for the requirement. Let\u2019s go through writing the data against below sample table: CREATE TABLE prod.db.sample ( id bigint, data string, category string, ts timestamp) USING iceberg PARTITIONED BY (days(ts), category) To write data to the sample table, your data needs to be sorted by days(ts), category . If you\u2019re inserting data with SQL statement, you can use ORDER BY to achieve it, like below: INSERT INTO prod.db.sample SELECT id, data, category, ts FROM another_table ORDER BY ts, category If you\u2019re inserting data with DataFrame, you can use either orderBy / sort to trigger global sort, or sortWithinPartitions to trigger local sort. Local sort for example: data.sortWithinPartitions(\"ts\", \"category\") .writeTo(\"prod.db.sample\") .append() You can simply add the original column to the sort condition for the most partition transformations, except bucket . For bucket partition transformation, you need to register the Iceberg transform function in Spark to specify it during sort. Let\u2019s go through another sample table having bucket partition: CREATE TABLE prod.db.sample ( id bigint, data string, category string, ts timestamp) USING iceberg PARTITIONED BY (bucket(16, id)) You need to register the function to deal with bucket, like below: import org.apache.iceberg.spark.IcebergSpark import org.apache.spark.sql.types.DataTypes IcebergSpark.registerBucketUDF(spark, \"iceberg_bucket16\", DataTypes.LongType, 16) Note Explicit registration of the function is necessary because Spark doesn\u2019t allow Iceberg to provide functions. SPARK-27658 is filed to enable Iceberg to provide functions which can be used in query. Here we just registered the bucket function as iceberg_bucket16 , which can be used in sort clause. If you\u2019re inserting data with SQL statement, you can use the function like below: INSERT INTO prod.db.sample SELECT id, data, category, ts FROM another_table ORDER BY iceberg_bucket16(id) If you\u2019re inserting data with DataFrame, you can use the function like below: data.sortWithinPartitions(expr(\"iceberg_bucket16(id)\")) .writeTo(\"prod.db.sample\") .append() Type compatibility \u00b6 Spark and Iceberg support different set of types. Iceberg does the type conversion automatically, but not for all combinations, so you may want to understand the type conversion in Iceberg in prior to design the types of columns in your tables. Spark type to Iceberg type \u00b6 This type conversion table describes how Spark types are converted to the Iceberg types. The conversion applies on both creating Iceberg table and writing to Iceberg table via Spark. Spark Iceberg Notes boolean boolean short integer byte integer integer integer long long float float double double date date timestamp timestamp with timezone char string varchar string string string binary binary decimal decimal struct struct array list map map Note The table is based on representing conversion during creating table. In fact, broader supports are applied on write. Here\u2019re some points on write: Iceberg numeric types ( integer , long , float , double , decimal ) support promotion during writes. e.g. You can write Spark types short , byte , integer , long to Iceberg type long . You can write to Iceberg fixed type using Spark binary type. Note that assertion on the length will be performed. Iceberg type to Spark type \u00b6 This type conversion table describes how Iceberg types are converted to the Spark types. The conversion applies on reading from Iceberg table via Spark. Iceberg Spark Note boolean boolean integer integer long long float float double double date date time Not supported timestamp with timezone timestamp timestamp without timezone Not supported string string uuid string fixed binary binary binary decimal decimal struct struct list array map map","title":"Writes"},{"location":"spark-writes/#spark-writes","text":"To use Iceberg in Spark, first configure Spark catalogs . Some plans are only available when using Iceberg SQL extensions in Spark 3.x. Iceberg uses Apache Spark\u2019s DataSourceV2 API for data source and catalog implementations. Spark DSv2 is an evolving API with different levels of support in Spark versions: Feature support Spark 3.0 Spark 2.4 Notes SQL insert into \u2714\ufe0f SQL merge into \u2714\ufe0f \u26a0 Requires Iceberg Spark extensions SQL insert overwrite \u2714\ufe0f SQL delete from \u2714\ufe0f \u26a0 Row-level delete requires Spark extensions SQL update \u2714\ufe0f \u26a0 Requires Iceberg Spark extensions DataFrame append \u2714\ufe0f \u2714\ufe0f DataFrame overwrite \u2714\ufe0f \u2714\ufe0f \u26a0 Behavior changed in Spark 3.0 DataFrame CTAS and RTAS \u2714\ufe0f","title":"Spark Writes"},{"location":"spark-writes/#writing-with-sql","text":"Spark 3 supports SQL INSERT INTO , MERGE INTO , and INSERT OVERWRITE , as well as the new DataFrameWriterV2 API.","title":"Writing with SQL"},{"location":"spark-writes/#insert-into","text":"To append new data to a table, use INSERT INTO . INSERT INTO prod.db.table VALUES (1, 'a'), (2, 'b') INSERT INTO prod.db.table SELECT ...","title":"INSERT INTO"},{"location":"spark-writes/#merge-into","text":"Spark 3 added support for MERGE INTO queries that can express row-level updates. Iceberg supports MERGE INTO by rewriting data files that contain rows that need to be updated in an overwrite commit. MERGE INTO is recommended instead of INSERT OVERWRITE because Iceberg can replace only the affected data files, and because the data overwritten by a dynamic overwrite may change if the table\u2019s partitioning changes.","title":"MERGE INTO"},{"location":"spark-writes/#merge-into-syntax","text":"MERGE INTO updates a table, called the target table, using a set of updates from another query, called the source . The update for a row in the target table is found using the ON clause that is like a join condition. MERGE INTO prod.db.target t -- a target table USING (SELECT ...) s -- the source updates ON t.id = s.id -- condition to find updates for target rows WHEN ... -- updates Updates to rows in the target table are listed using WHEN MATCHED ... THEN ... . Multiple MATCHED clauses can be added with conditions that determine when each match should be applied. The first matching expression is used. WHEN MATCHED AND s.op = 'delete' THEN DELETE WHEN MATCHED AND t.count IS NULL AND s.op = 'increment' THEN UPDATE SET t.count = 0 WHEN MATCHED AND s.op = 'increment' THEN UPDATE SET t.count = t.count + 1 Source rows (updates) that do not match can be inserted: WHEN NOT MATCHED THEN INSERT * Inserts also support additional conditions: WHEN NOT MATCHED AND s.event_time > still_valid_threshold THEN INSERT (id, count) VALUES (s.id, 1) Only one record in the source data can update any given row of the target table, or else an error will be thrown.","title":"MERGE INTO syntax"},{"location":"spark-writes/#insert-overwrite","text":"INSERT OVERWRITE can replace data in the table with the result of a query. Overwrites are atomic operations for Iceberg tables. The partitions that will be replaced by INSERT OVERWRITE depends on Spark\u2019s partition overwrite mode and the partitioning of a table. MERGE INTO can rewrite only affected data files and has more easily understood behavior, so it is recommended instead of INSERT OVERWRITE . Warning Spark 3.0.0 has a correctness bug that affects dynamic INSERT OVERWRITE with hidden partitioning, SPARK-32168 . For tables with hidden partitions , make sure you use Spark 3.0.1.","title":"INSERT OVERWRITE"},{"location":"spark-writes/#overwrite-behavior","text":"Spark\u2019s default overwrite mode is static , but dynamic overwrite mode is recommended when writing to Iceberg tables. Static overwrite mode determines which partitions to overwrite in a table by converting the PARTITION clause to a filter, but the PARTITION clause can only reference table columns. Dynamic overwrite mode is configured by setting spark.sql.sources.partitionOverwriteMode=dynamic . To demonstrate the behavior of dynamic and static overwrites, consider a logs table defined by the following DDL: CREATE TABLE prod.my_app.logs ( uuid string NOT NULL, level string NOT NULL, ts timestamp NOT NULL, message string) USING iceberg PARTITIONED BY (level, hours(ts))","title":"Overwrite behavior"},{"location":"spark-writes/#dynamic-overwrite","text":"When Spark\u2019s overwrite mode is dynamic, partitions that have rows produced by the SELECT query will be replaced. For example, this query removes duplicate log events from the example logs table. INSERT OVERWRITE prod.my_app.logs SELECT uuid, first(level), first(ts), first(message) FROM prod.my_app.logs WHERE cast(ts as date) = '2020-07-01' GROUP BY uuid In dynamic mode, this will replace any partition with rows in the SELECT result. Because the date of all rows is restricted to 1 July, only hours of that day will be replaced.","title":"Dynamic overwrite"},{"location":"spark-writes/#static-overwrite","text":"When Spark\u2019s overwrite mode is static, the PARTITION clause is converted to a filter that is used to delete from the table. If the PARTITION clause is omitted, all partitions will be replaced. Because there is no PARTITION clause in the query above, it will drop all existing rows in the table when run in static mode, but will only write the logs from 1 July. To overwrite just the partitions that were loaded, add a PARTITION clause that aligns with the SELECT query filter: INSERT OVERWRITE prod.my_app.logs PARTITION (level = 'INFO') SELECT uuid, first(level), first(ts), first(message) FROM prod.my_app.logs WHERE level = 'INFO' GROUP BY uuid Note that this mode cannot replace hourly partitions like the dynamic example query because the PARTITION clause can only reference table columns, not hidden partitions.","title":"Static overwrite"},{"location":"spark-writes/#delete-from","text":"Spark 3 added support for DELETE FROM queries to remove data from tables. Delete queries accept a filter to match rows to delete. DELETE FROM prod.db.table WHERE ts >= '2020-05-01 00:00:00' and ts < '2020-06-01 00:00:00' DELETE FROM prod.db.all_events WHERE session_time < (SELECT min(session_time) FROM prod.db.good_events) DELETE FROM prod.db.orders AS t1 WHERE EXISTS (SELECT oid FROM prod.db.returned_orders WHERE t1.oid = oid) If the delete filter matches entire partitions of the table, Iceberg will perform a metadata-only delete. If the filter matches individual rows of a table, then Iceberg will rewrite only the affected data files.","title":"DELETE FROM"},{"location":"spark-writes/#update","text":"Spark 3.1 added support for UPDATE queries that update matching rows in tables. Update queries accept a filter to match rows to update. UPDATE prod.db.table SET c1 = 'update_c1', c2 = 'update_c2' WHERE ts >= '2020-05-01 00:00:00' and ts < '2020-06-01 00:00:00' UPDATE prod.db.all_events SET session_time = 0, ignored = true WHERE session_time < (SELECT min(session_time) FROM prod.db.good_events) UPDATE prod.db.orders AS t1 SET order_status = 'returned' WHERE EXISTS (SELECT oid FROM prod.db.returned_orders WHERE t1.oid = oid) For more complex row-level updates based on incoming data, see the section on MERGE INTO .","title":"UPDATE"},{"location":"spark-writes/#writing-with-dataframes","text":"Spark 3 introduced the new DataFrameWriterV2 API for writing to tables using data frames. The v2 API is recommended for several reasons: CTAS, RTAS, and overwrite by filter are supported All operations consistently write columns to a table by name Hidden partition expressions are supported in partitionedBy Overwrite behavior is explicit, either dynamic or by a user-supplied filter The behavior of each operation corresponds to SQL statements df.writeTo(t).create() is equivalent to CREATE TABLE AS SELECT df.writeTo(t).replace() is equivalent to REPLACE TABLE AS SELECT df.writeTo(t).append() is equivalent to INSERT INTO df.writeTo(t).overwritePartitions() is equivalent to dynamic INSERT OVERWRITE The v1 DataFrame write API is still supported, but is not recommended. Warning When writing with the v1 DataFrame API in Spark 3, use saveAsTable or insertInto to load tables with a catalog. Using format(\"iceberg\") loads an isolated table reference that will not automatically refresh tables used by queries.","title":"Writing with DataFrames"},{"location":"spark-writes/#appending-data","text":"To append a dataframe to an Iceberg table, use append : val data: DataFrame = ... data.writeTo(\"prod.db.table\").append()","title":"Appending data"},{"location":"spark-writes/#spark-24","text":"In Spark 2.4, use the v1 API with append mode and iceberg format: data.write .format(\"iceberg\") .mode(\"append\") .save(\"db.table\")","title":"Spark 2.4"},{"location":"spark-writes/#overwriting-data","text":"To overwrite partitions dynamically, use overwritePartitions() : val data: DataFrame = ... data.writeTo(\"prod.db.table\").overwritePartitions() To explicitly overwrite partitions, use overwrite to supply a filter: data.writeTo(\"prod.db.table\").overwrite($\"level\" === \"INFO\")","title":"Overwriting data"},{"location":"spark-writes/#spark-24_1","text":"In Spark 2.4, overwrite values in an Iceberg table with overwrite mode and iceberg format: data.write .format(\"iceberg\") .mode(\"overwrite\") .save(\"db.table\") Warning The behavior of overwrite mode changed between Spark 2.4 and Spark 3 . The behavior of DataFrameWriter overwrite mode was undefined in Spark 2.4, but is required to overwrite the entire table in Spark 3. Because of this new requirement, the Iceberg source\u2019s behavior changed in Spark 3. In Spark 2.4, the behavior was to dynamically overwrite partitions. To use the Spark 2.4 behavior, add option overwrite-mode=dynamic .","title":"Spark 2.4"},{"location":"spark-writes/#creating-tables","text":"To run a CTAS or RTAS, use create , replace , or createOrReplace operations: val data: DataFrame = ... data.writeTo(\"prod.db.table\").create() Create and replace operations support table configuration methods, like partitionedBy and tableProperty : data.writeTo(\"prod.db.table\") .tableProperty(\"write.format.default\", \"orc\") .partitionBy($\"level\", days($\"ts\")) .createOrReplace()","title":"Creating tables"},{"location":"spark-writes/#writing-to-partitioned-tables","text":"Iceberg requires the data to be sorted according to the partition spec per task (Spark partition) in prior to write against partitioned table. This applies both Writing with SQL and Writing with DataFrames. Note Explicit sort is necessary because Spark doesn\u2019t allow Iceberg to request a sort before writing as of Spark 3.0. SPARK-23889 is filed to enable Iceberg to require specific distribution & sort order to Spark. Note Both global sort ( orderBy / sort ) and local sort ( sortWithinPartitions ) work for the requirement. Let\u2019s go through writing the data against below sample table: CREATE TABLE prod.db.sample ( id bigint, data string, category string, ts timestamp) USING iceberg PARTITIONED BY (days(ts), category) To write data to the sample table, your data needs to be sorted by days(ts), category . If you\u2019re inserting data with SQL statement, you can use ORDER BY to achieve it, like below: INSERT INTO prod.db.sample SELECT id, data, category, ts FROM another_table ORDER BY ts, category If you\u2019re inserting data with DataFrame, you can use either orderBy / sort to trigger global sort, or sortWithinPartitions to trigger local sort. Local sort for example: data.sortWithinPartitions(\"ts\", \"category\") .writeTo(\"prod.db.sample\") .append() You can simply add the original column to the sort condition for the most partition transformations, except bucket . For bucket partition transformation, you need to register the Iceberg transform function in Spark to specify it during sort. Let\u2019s go through another sample table having bucket partition: CREATE TABLE prod.db.sample ( id bigint, data string, category string, ts timestamp) USING iceberg PARTITIONED BY (bucket(16, id)) You need to register the function to deal with bucket, like below: import org.apache.iceberg.spark.IcebergSpark import org.apache.spark.sql.types.DataTypes IcebergSpark.registerBucketUDF(spark, \"iceberg_bucket16\", DataTypes.LongType, 16) Note Explicit registration of the function is necessary because Spark doesn\u2019t allow Iceberg to provide functions. SPARK-27658 is filed to enable Iceberg to provide functions which can be used in query. Here we just registered the bucket function as iceberg_bucket16 , which can be used in sort clause. If you\u2019re inserting data with SQL statement, you can use the function like below: INSERT INTO prod.db.sample SELECT id, data, category, ts FROM another_table ORDER BY iceberg_bucket16(id) If you\u2019re inserting data with DataFrame, you can use the function like below: data.sortWithinPartitions(expr(\"iceberg_bucket16(id)\")) .writeTo(\"prod.db.sample\") .append()","title":"Writing to partitioned tables"},{"location":"spark-writes/#type-compatibility","text":"Spark and Iceberg support different set of types. Iceberg does the type conversion automatically, but not for all combinations, so you may want to understand the type conversion in Iceberg in prior to design the types of columns in your tables.","title":"Type compatibility"},{"location":"spark-writes/#spark-type-to-iceberg-type","text":"This type conversion table describes how Spark types are converted to the Iceberg types. The conversion applies on both creating Iceberg table and writing to Iceberg table via Spark. Spark Iceberg Notes boolean boolean short integer byte integer integer integer long long float float double double date date timestamp timestamp with timezone char string varchar string string string binary binary decimal decimal struct struct array list map map Note The table is based on representing conversion during creating table. In fact, broader supports are applied on write. Here\u2019re some points on write: Iceberg numeric types ( integer , long , float , double , decimal ) support promotion during writes. e.g. You can write Spark types short , byte , integer , long to Iceberg type long . You can write to Iceberg fixed type using Spark binary type. Note that assertion on the length will be performed.","title":"Spark type to Iceberg type"},{"location":"spark-writes/#iceberg-type-to-spark-type","text":"This type conversion table describes how Iceberg types are converted to the Spark types. The conversion applies on reading from Iceberg table via Spark. Iceberg Spark Note boolean boolean integer integer long long float float double double date date time Not supported timestamp with timezone timestamp timestamp without timezone Not supported string string uuid string fixed binary binary binary decimal decimal struct struct list array map map","title":"Iceberg type to Spark type"},{"location":"spec/","text":"Iceberg Table Spec \u00b6 This is a specification for the Iceberg table format that is designed to manage a large, slow-changing collection of files in a distributed file system or key-value store as a table. Format Versioning \u00b6 Versions 1 and 2 of the Iceberg spec are complete and adopted by the community. The format version number is incremented when new features are added that will break forward-compatibility\u2014that is, when older readers would not read newer table features correctly. Tables may continue to be written with an older version of the spec to ensure compatibility by not using features that are not yet implemented by processing engines. Version 1: Analytic Data Tables \u00b6 Version 1 of the Iceberg spec defines how to manage large analytic tables using immutable file formats: Parquet, Avro, and ORC. All version 1 data and metadata files are valid after upgrading a table to version 2. Appendix E documents how to default version 2 fields when reading version 1 metadata. Version 2: Row-level Deletes \u00b6 Version 2 of the Iceberg spec adds row-level updates and deletes for analytic tables with immutable files. The primary change in version 2 adds delete files to encode that rows that are deleted in existing data files. This version can be used to delete or replace individual rows in immutable data files without rewriting the files. In addition to row-level deletes, version 2 makes some requirements stricter for writers. The full set of changes are listed in Appendix E . Goals \u00b6 Serializable isolation \u2013 Reads will be isolated from concurrent writes and always use a committed snapshot of a table\u2019s data. Writes will support removing and adding files in a single operation and are never partially visible. Readers will not acquire locks. Speed \u2013 Operations will use O(1) remote calls to plan the files for a scan and not O(n) where n grows with the size of the table, like the number of partitions or files. Scale \u2013 Job planning will be handled primarily by clients and not bottleneck on a central metadata store. Metadata will include information needed for cost-based optimization. Evolution \u2013 Tables will support full schema and partition spec evolution. Schema evolution supports safe column add, drop, reorder and rename, including in nested structures. Dependable types \u2013 Tables will provide well-defined and dependable support for a core set of types. Storage separation \u2013 Partitioning will be table configuration. Reads will be planned using predicates on data values, not partition values. Tables will support evolving partition schemes. Formats \u2013 Underlying data file formats will support identical schema evolution rules and types. Both read- and write-optimized formats will be available. Overview \u00b6 This table format tracks individual data files in a table instead of directories. This allows writers to create data files in-place and only adds files to the table in an explicit commit. Table state is maintained in metadata files. All changes to table state create a new metadata file and replace the old metadata with an atomic swap. The table metadata file tracks the table schema, partitioning config, custom properties, and snapshots of the table contents. A snapshot represents the state of a table at some time and is used to access the complete set of data files in the table. Data files in snapshots are tracked by one or more manifest files that contain a row for each data file in the table, the file\u2019s partition data, and its metrics. The data in a snapshot is the union of all files in its manifests. Manifest files are reused across snapshots to avoid rewriting metadata that is slow-changing. Manifests can track data files with any subset of a table and are not associated with partitions. The manifests that make up a snapshot are stored in a manifest list file. Each manifest list stores metadata about manifests, including partition stats and data file counts. These stats are used to avoid reading manifests that are not required for an operation. Optimistic Concurrency \u00b6 An atomic swap of one table metadata file for another provides the basis for serializable isolation. Readers use the snapshot that was current when they load the table metadata and are not affected by changes until they refresh and pick up a new metadata location. Writers create table metadata files optimistically, assuming that the current version will not be changed before the writer\u2019s commit. Once a writer has created an update, it commits by swapping the table\u2019s metadata file pointer from the base version to the new version. If the snapshot on which an update is based is no longer current, the writer must retry the update based on the new current version. Some operations support retry by re-applying metadata changes and committing, under well-defined conditions. For example, a change that rewrites files can be applied to a new table snapshot if all of the rewritten files are still in the table. The conditions required by a write to successfully commit determines the isolation level. Writers can select what to validate and can make different isolation guarantees. Sequence Numbers \u00b6 The relative age of data and delete files relies on a sequence number that is assigned to every successful commit. When a snapshot is created for a commit, it is optimistically assigned the next sequence number, and it is written into the snapshot\u2019s metadata. If the commit fails and must be retried, the sequence number is reassigned and written into new snapshot metadata. All manifests, data files, and delete files created for a snapshot inherit the snapshot\u2019s sequence number. Manifest file metadata in the manifest list stores a manifest\u2019s sequence number. New data and metadata file entries are written with null in place of a sequence number, which is replaced with the manifest\u2019s sequence number at read time. When a data or delete file is written to a new manifest (as \u201cexisting\u201d), the inherited sequence number is written to ensure it does not change after it is first inherited. Inheriting the sequence number from manifest metadata allows writing a new manifest once and reusing it in commit retries. To change a sequence number for a retry, only the manifest list must be rewritten \u2013 which would be rewritten anyway with the latest set of manifests. Row-level Deletes \u00b6 Row-level deletes are stored in delete files. There are two ways to encode a row-level delete: Position deletes mark a row deleted by data file path and the row position in the data file Equality deletes mark a row deleted by one or more column values, like id = 5 Like data files, delete files are tracked by partition. In general, a delete file must be applied to older data files with the same partition; see Scan Planning for details. Column metrics can be used to determine whether a delete file\u2019s rows overlap the contents of a data file or a scan range. File System Operations \u00b6 Iceberg only requires that file systems support the following operations: In-place write \u2013 Files are not moved or altered once they are written. Seekable reads \u2013 Data file formats require seek support. Deletes \u2013 Tables delete files that are no longer used. These requirements are compatible with object stores, like S3. Tables do not require random-access writes. Once written, data and metadata files are immutable until they are deleted. Tables do not require rename, except for tables that use atomic rename to implement the commit operation for new metadata files. Specification \u00b6 Terms \u00b6 Schema \u2013 Names and types of fields in a table. Partition spec \u2013 A definition of how partition values are derived from data fields. Snapshot \u2013 The state of a table at some point in time, including the set of all data files. Manifest list \u2013 A file that lists manifest files; one per snapshot. Manifest \u2013 A file that lists data or delete files; a subset of a snapshot. Data file \u2013 A file that contains rows of a table. Delete file \u2013 A file that encodes rows of a table that are deleted by position or data values. Writer requirements \u00b6 Some tables in this spec have columns that specify requirements for v1 and v2 tables. These requirements are intended for writers when adding metadata files to a table with the given version. Requirement Write behavior (blank) The field should be omitted optional The field can be written required The field must be written Readers should be more permissive because v1 metadata files are allowed in v2 tables so that tables can be upgraded to v2 without rewriting the metadata tree. For manifest list and manifest files, this table shows the expected v2 read behavior: v1 v2 v2 read behavior optional Read the field as optional required Read the field as optional ; it may be missing in v1 files optional Ignore the field optional optional Read the field as optional optional required Read the field as optional ; it may be missing in v1 files required Ignore the field required optional Read the field as optional required required Fill in a default or throw an exception if the field is missing Readers may be more strict for metadata JSON files because the JSON files are not reused and will always match the table version. Required v2 fields that were not present in v1 or optional in v1 may be handled as required fields. For example, a v2 table that is missing last-sequence-number can throw an exception. Schemas and Data Types \u00b6 A table\u2019s schema is a list of named columns. All data types are either primitives or nested types, which are maps, lists, or structs. A table schema is also a struct type. For the representations of these types in Avro, ORC, and Parquet file formats, see Appendix A. Nested Types \u00b6 A struct is a tuple of typed values. Each field in the tuple is named and has an integer id that is unique in the table schema. Each field can be either optional or required, meaning that values can (or cannot) be null. Fields may be any type. Fields may have an optional comment or doc string. A list is a collection of values with some element type. The element field has an integer id that is unique in the table schema. Elements can be either optional or required. Element types may be any type. A map is a collection of key-value pairs with a key type and a value type. Both the key field and value field each have an integer id that is unique in the table schema. Map keys are required and map values can be either optional or required. Both map keys and map values may be any type, including nested types. Primitive Types \u00b6 Primitive type Description Requirements boolean True or false int 32-bit signed integers Can promote to long long 64-bit signed integers float 32-bit IEEE 754 floating point Can promote to double double 64-bit IEEE 754 floating point decimal(P,S) Fixed-point decimal; precision P, scale S Scale is fixed [1], precision must be 38 or less date Calendar date without timezone or time time Time of day without date, timezone Microsecond precision [2] timestamp Timestamp without timezone Microsecond precision [2] timestamptz Timestamp with timezone Stored as UTC [2] string Arbitrary-length character sequences Encoded with UTF-8 [3] uuid Universally unique identifiers Should use 16-byte fixed fixed(L) Fixed-length byte array of length L binary Arbitrary-length byte array Notes: Decimal scale is fixed and cannot be changed by schema evolution. Precision can only be widened. All time and timestamp values are stored with microsecond precision. Timestamps with time zone represent a point in time: values are stored as UTC and do not retain a source time zone ( 2017-11-16 17:10:34 PST is stored/retrieved as 2017-11-17 01:10:34 UTC and these values are considered identical). Timestamps without time zone represent a date and time of day regardless of zone: the time value is independent of zone adjustments ( 2017-11-16 17:10:34 is always retrieved as 2017-11-16 17:10:34 ). Timestamp values are stored as a long that encodes microseconds from the unix epoch. Character strings must be stored as UTF-8 encoded byte arrays. For details on how to serialize a schema to JSON, see Appendix C. Schema Evolution \u00b6 Schemas may be evolved by type promotion or adding, deleting, renaming, or reordering fields in structs (both nested structs and the top-level schema\u2019s struct). Evolution applies changes to the table\u2019s current schema to produce a new schema that is identified by a unique schema ID, is added to the table\u2019s list of schemas, and is set as the table\u2019s current schema. Valid type promotions are: int to long float to double decimal(P, S) to decimal(P', S) if P' > P \u2013 widen the precision of decimal types. Any struct, including a top-level schema, can evolve through deleting fields, adding new fields, renaming existing fields, reordering existing fields, or promoting a primitive using the valid type promotions. Adding a new field assigns a new ID for that field and for any nested fields. Renaming an existing field must change the name, but not the field ID. Deleting a field removes it from the current schema. Field deletion cannot be rolled back unless the field was nullable or if the current snapshot has not changed. Grouping a subset of a struct\u2019s fields into a nested struct is not allowed, nor is moving fields from a nested struct into its immediate parent struct ( struct<a, b, c> \u2194 struct<a, struct<b, c>> ). Evolving primitive types to structs is not allowed, nor is evolving a single-field struct to a primitive ( map<string, int> \u2194 map<string, struct<int>> ). Column Projection \u00b6 Columns in Iceberg data files are selected by field id. The table schema\u2019s column names and order may change after a data file is written, and projection must be done using field ids. If a field id is missing from a data file, its value for each row should be null . For example, a file may be written with schema 1: a int, 2: b string, 3: c double and read using projection schema 3: measurement, 2: name, 4: a . This must select file columns c (renamed to measurement ), b (now called name ), and a column of null values called a ; in that order. Identifier Field IDs \u00b6 A schema can optionally track the set of primitive fields that identify rows in a table, using the property identifier-field-ids (see JSON encoding in Appendix C). Two rows are the \u201csame\u201d\u2014that is, the rows represent the same entity\u2014if the identifier fields are equal. However, uniqueness of rows by this identifier is not guaranteed or required by Iceberg and it is the responsibility of processing engines or data providers to enforce. Identifier fields may be nested in structs but cannot be nested within maps or lists. Float, double, and optional fields cannot be used as identifier fields and a nested field cannot be used as an identifier field if it is nested in an optional struct, to avoid null values in identifiers. Reserved Field IDs \u00b6 Iceberg tables must not use field ids greater than 2147483447 ( Integer.MAX_VALUE - 200 ). This id range is reserved for metadata columns that can be used in user data schemas, like the _file column that holds the file path in which a row was stored. The set of metadata columns is: Field id, name Type Description 2147483646 _file string Path of the file in which a row is stored 2147483645 _pos long Ordinal position of a row in the source data file 2147483644 _deleted boolean Whether the row has been deleted 2147483643 _spec_id int Spec ID used to track the file containing a row 2147483642 _partition struct Partition to which a row belongs 2147483546 file_path string Path of a file, used in position-based delete files 2147483545 pos long Ordinal position of a row, used in position-based delete files 2147483544 row struct<...> Deleted row values, used in position-based delete files Partitioning \u00b6 Data files are stored in manifests with a tuple of partition values that are used in scans to filter out files that cannot contain records that match the scan\u2019s filter predicate. Partition values for a data file must be the same for all records stored in the data file. (Manifests store data files from any partition, as long as the partition spec is the same for the data files.) Tables are configured with a partition spec that defines how to produce a tuple of partition values from a record. A partition spec has a list of fields that consist of: A source column id from the table\u2019s schema A partition field id that is used to identify a partition field and is unique within a partition spec. In v2 table metadata, it is unique across all partition specs. A transform that is applied to the source column to produce a partition value A partition name The source column, selected by id, must be a primitive type and cannot be contained in a map or list, but may be nested in a struct. For details on how to serialize a partition spec to JSON, see Appendix C. Partition specs capture the transform from table data to partition values. This is used to transform predicates to partition predicates, in addition to transforming data values. Deriving partition predicates from column predicates on the table data is used to separate the logical queries from physical storage: the partitioning can change and the correct partition filters are always derived from column predicates. This simplifies queries because users don\u2019t have to supply both logical predicates and partition predicates. For more information, see Scan Planning below. Partition Transforms \u00b6 Transform name Description Source types Result type identity Source value, unmodified Any Source type bucket[N] Hash of value, mod N (see below) int , long , decimal , date , time , timestamp , timestamptz , string , uuid , fixed , binary int truncate[W] Value truncated to width W (see below) int , long , decimal , string Source type year Extract a date or timestamp year, as years from 1970 date , timestamp , timestamptz int month Extract a date or timestamp month, as months from 1970-01-01 date , timestamp , timestamptz int day Extract a date or timestamp day, as days from 1970-01-01 date , timestamp , timestamptz date hour Extract a timestamp hour, as hours from 1970-01-01 00:00:00 timestamp , timestamptz int void Always produces null Any Source type or int All transforms must return null for a null input value. The void transform may be used to replace the transform in an existing partition field so that the field is effectively dropped in v1 tables. See partition evolution below. Bucket Transform Details \u00b6 Bucket partition transforms use a 32-bit hash of the source value. The 32-bit hash implementation is the 32-bit Murmur3 hash, x86 variant, seeded with 0. Transforms are parameterized by a number of buckets [1], N . The hash mod N must produce a positive value by first discarding the sign bit of the hash value. In pseudo-code, the function is: def bucket_N(x) = (murmur3_x86_32_hash(x) & Integer.MAX_VALUE) % N Notes: Changing the number of buckets as a table grows is possible by evolving the partition spec. For hash function details by type, see Appendix B. Truncate Transform Details \u00b6 Type Config Truncate specification Examples int W , width v - (v % W) remainders must be positive [1] W=10 : 1 \uffeb 0 , -1 \uffeb -10 long W , width v - (v % W) remainders must be positive [1] W=10 : 1 \uffeb 0 , -1 \uffeb -10 decimal W , width (no scale) scaled_W = decimal(W, scale(v)) v - (v % scaled_W) [1, 2] W=50 , s=2 : 10.65 \uffeb 10.50 string L , length Substring of length L : v.substring(0, L) L=3 : iceberg \uffeb ice Notes: The remainder, v % W , must be positive. For languages where % can produce negative values, the correct truncate function is: v - (((v % W) + W) % W) The width, W , used to truncate decimal values is applied using the scale of the decimal column to avoid additional (and potentially conflicting) parameters. Partition Evolution \u00b6 Table partitioning can be evolved by adding, removing, renaming, or reordering partition spec fields. Changing a partition spec produces a new spec identified by a unique spec ID that is added to the table\u2019s list of partition specs and may be set as the table\u2019s default spec. When evolving a spec, changes should not cause partition field IDs to change because the partition field IDs are used as the partition tuple field IDs in manifest files. In v2, partition field IDs must be explicitly tracked for each partition field. New IDs are assigned based on the last assigned partition ID in table metadata. In v1, partition field IDs were not tracked, but were assigned sequentially starting at 1000 in the reference implementation. This assignment caused problems when reading metadata tables based on manifest files from multiple specs because partition fields with the same ID may contain different data types. For compatibility with old versions, the following rules are recommended for partition evolution in v1 tables: Do not reorder partition fields Do not drop partition fields; instead replace the field\u2019s transform with the void transform Only add partition fields at the end of the previous partition spec Sorting \u00b6 Users can sort their data within partitions by columns to gain performance. The information on how the data is sorted can be declared per data or delete file, by a sort order . A sort order is defined by an sort order id and a list of sort fields. The order of the sort fields within the list defines the order in which the sort is applied to the data. Each sort field consists of: A source column id from the table\u2019s schema A transform that is used to produce values to be sorted on from the source column. This is the same transform as described in partition transforms . A sort direction , that can only be either asc or desc A null order that describes the order of null values when sorted. Can only be either nulls-first or nulls-last Order id 0 is reserved for the unsorted order. Sorting floating-point numbers should produce the following behavior: -NaN < -Infinity < -value < -0 < 0 < value < Infinity < NaN . This aligns with the implementation of Java floating-point types comparisons. A data or delete file is associated with a sort order by the sort order\u2019s id within a manifest . Therefore, the table must declare all the sort orders for lookup. A table could also be configured with a default sort order id, indicating how the new data should be sorted by default. Writers should use this default sort order to sort the data on write, but are not required to if the default order is prohibitively expensive, as it would be for streaming writes. Manifests \u00b6 A manifest is an immutable Avro file that lists data files or delete files, along with each file\u2019s partition data tuple, metrics, and tracking information. One or more manifest files are used to store a snapshot , which tracks all of the files in a table at some point in time. Manifests are tracked by a manifest list for each table snapshot. A manifest is a valid Iceberg data file: files must use valid Iceberg formats, schemas, and column projection. A manifest may store either data files or delete files, but not both because manifests that contain delete files are scanned first during job planning. Whether a manifest is a data manifest or a delete manifest is stored in manifest metadata. A manifest stores files for a single partition spec. When a table\u2019s partition spec changes, old files remain in the older manifest and newer files are written to a new manifest. This is required because a manifest file\u2019s schema is based on its partition spec (see below). The partition spec of each manifest is also used to transform predicates on the table\u2019s data rows into predicates on partition values that are used during job planning to select files from a manifest. A manifest file must store the partition spec and other metadata as properties in the Avro file\u2019s key-value metadata: v1 v2 Key Value required required schema JSON representation of the table schema at the time the manifest was written optional required schema-id ID of the schema used to write the manifest as a string required required partition-spec JSON fields representation of the partition spec used to write the manifest optional required partition-spec-id ID of the partition spec used to write the manifest as a string optional required format-version Table format version number of the manifest as a string required content Type of content files tracked by the manifest: \u201cdata\u201d or \u201cdeletes\u201d The schema of a manifest file is a struct called manifest_entry with the following fields: v1 v2 Field id, name Type Description required required 0 status int with meaning: 0: EXISTING 1: ADDED 2: DELETED Used to track additions and deletions required optional 1 snapshot_id long Snapshot id where the file was added, or deleted if status is 2. Inherited when null. optional 3 sequence_number long Sequence number when the file was added. Inherited when null. required required 2 data_file data_file struct (see below) File path, partition tuple, metrics, \u2026 data_file is a struct with the following fields: v1 v2 Field id, name Type Description required 134 content int with meaning: 0: DATA , 1: POSITION DELETES , 2: EQUALITY DELETES Type of content stored by the data file: data, equality deletes, or position deletes (all v1 files are data files) required required 100 file_path string Full URI for the file with FS scheme required required 101 file_format string String file format name, avro, orc or parquet required required 102 partition struct<...> Partition data tuple, schema based on the partition spec output using partition field ids for the struct field ids required required 103 record_count long Number of records in this file required required 104 file_size_in_bytes long Total file size in bytes required 105 block_size_in_bytes long Deprecated. Always write a default in v1. Do not write in v2. optional 106 file_ordinal int Deprecated. Do not write. optional 107 sort_columns list<112: int> Deprecated. Do not write. optional optional 108 column_sizes map<117: int, 118: long> Map from column id to the total size on disk of all regions that store the column. Does not include bytes necessary to read other columns, like footers. Leave null for row-oriented formats (Avro) optional optional 109 value_counts map<119: int, 120: long> Map from column id to number of values in the column (including null and NaN values) optional optional 110 null_value_counts map<121: int, 122: long> Map from column id to number of null values in the column optional optional 137 nan_value_counts map<138: int, 139: long> Map from column id to number of NaN values in the column optional optional 111 distinct_counts map<123: int, 124: long> Map from column id to number of distinct values in the column; distinct counts must be derived using values in the file by counting or using sketches, but not using methods like merging existing distinct counts optional optional 125 lower_bounds map<126: int, 127: binary> Map from column id to lower bound in the column serialized as binary [1]. Each value must be less than or equal to all non-null, non-NaN values in the column for the file [2] optional optional 128 upper_bounds map<129: int, 130: binary> Map from column id to upper bound in the column serialized as binary [1]. Each value must be greater than or equal to all non-null, non-Nan values in the column for the file [2] optional optional 131 key_metadata binary Implementation-specific key metadata for encryption optional optional 132 split_offsets list<133: long> Split offsets for the data file. For example, all row group offsets in a Parquet file. Must be sorted ascending optional 135 equality_ids list<136: int> Field ids used to determine row equality in equality delete files. Required when content=2 and should be null otherwise. Fields with ids listed in this column must be present in the delete file optional optional 140 sort_order_id int ID representing sort order for this file [3]. Notes: Single-value serialization for lower and upper bounds is detailed in Appendix D. For float and double , the value -0.0 must precede +0.0 , as in the IEEE 754 totalOrder predicate. If sort order ID is missing or unknown, then the order is assumed to be unsorted. Only data files and equality delete files should be written with a non-null order id. Position deletes are required to be sorted by file and position, not a table order, and should set sort order id to null. Readers must ignore sort order id for position delete files. The partition struct stores the tuple of partition values for each file. Its type is derived from the partition fields of the partition spec used to write the manifest file. In v2, the partition struct\u2019s field ids must match the ids from the partition spec. The column metrics maps are used when filtering to select both data and delete files. For delete files, the metrics must store bounds and counts for all deleted rows, or must be omitted. Storing metrics for deleted rows ensures that the values can be used during job planning to find delete files that must be merged during a scan. Manifest Entry Fields \u00b6 The manifest entry fields are used to keep track of the snapshot in which files were added or logically deleted. The data_file struct is nested inside of the manifest entry so that it can be easily passed to job planning without the manifest entry fields. When a file is added to the dataset, it\u2019s manifest entry should store the snapshot ID in which the file was added and set status to 1 (added). When a file is replaced or deleted from the dataset, it\u2019s manifest entry fields store the snapshot ID in which the file was deleted and status 2 (deleted). The file may be deleted from the file system when the snapshot in which it was deleted is garbage collected, assuming that older snapshots have also been garbage collected [1]. Iceberg v2 adds a sequence number to the entry and makes the snapshot id optional. Both fields, sequence_number and snapshot_id , are inherited from manifest metadata when null . That is, if the field is null for an entry, then the entry must inherit its value from the manifest file\u2019s metadata, stored in the manifest list [2]. Notes: Technically, data files can be deleted when the last snapshot that contains the file as \u201clive\u201d data is garbage collected. But this is harder to detect and requires finding the diff of multiple snapshots. It is easier to track what files are deleted in a snapshot and delete them when that snapshot expires. Manifest list files are required in v2, so that the sequence_number and snapshot_id to inherit are always available. Sequence Number Inheritance \u00b6 Manifests track the sequence number when a data or delete file was added to the table. When adding new file, its sequence number is set to null because the snapshot\u2019s sequence number is not assigned until the snapshot is successfully committed. When reading, sequence numbers are inherited by replacing null with the manifest\u2019s sequence number from the manifest list. When writing an existing file to a new manifest, the sequence number must be non-null and set to the sequence number that was inherited. Inheriting sequence numbers through the metadata tree allows writing a new manifest without a known sequence number, so that a manifest can be written once and reused in commit retries. To change a sequence number for a retry, only the manifest list must be rewritten. When reading v1 manifests with no sequence number column, sequence numbers for all files must default to 0. Snapshots \u00b6 A snapshot consists of the following fields: v1 v2 Field Description required required snapshot-id A unique long ID optional optional parent-snapshot-id The snapshot ID of the snapshot\u2019s parent. Omitted for any snapshot with no parent required sequence-number A monotonically increasing long that tracks the order of changes to a table required required timestamp-ms A timestamp when the snapshot was created, used for garbage collection and table inspection optional required manifest-list The location of a manifest list for this snapshot that tracks manifest files with additional meadata optional manifests A list of manifest file locations. Must be omitted if manifest-list is present optional required summary A string map that summarizes the snapshot changes, including operation (see below) optional optional schema-id ID of the table\u2019s current schema when the snapshot was created The snapshot summary\u2019s operation field is used by some operations, like snapshot expiration, to skip processing certain snapshots. Possible operation values are: append \u2013 Only data files were added and no files were removed. replace \u2013 Data and delete files were added and removed without changing table data; i.e., compaction, changing the data file format, or relocating data files. overwrite \u2013 Data and delete files were added and removed in a logical overwrite operation. delete \u2013 Data files were removed and their contents logically deleted and/or delete files were added to delete rows. Data and delete files for a snapshot can be stored in more than one manifest. This enables: Appends can add a new manifest to minimize the amount of data written, instead of adding new records by rewriting and appending to an existing manifest. (This is called a \u201cfast append\u201d.) Tables can use multiple partition specs. A table\u2019s partition configuration can evolve if, for example, its data volume changes. Each manifest uses a single partition spec, and queries do not need to change because partition filters are derived from data predicates. Large tables can be split across multiple manifests so that implementations can parallelize job planning or reduce the cost of rewriting a manifest. Manifests for a snapshot are tracked by a manifest list. Valid snapshots are stored as a list in table metadata. For serialization, see Appendix C. Manifest Lists \u00b6 Snapshots are embedded in table metadata, but the list of manifests for a snapshot are stored in a separate manifest list file. A new manifest list is written for each attempt to commit a snapshot because the list of manifests always changes to produce a new snapshot. When a manifest list is written, the (optimistic) sequence number of the snapshot is written for all new manifest files tracked by the list. A manifest list includes summary metadata that can be used to avoid scanning all of the manifests in a snapshot when planning a table scan. This includes the number of added, existing, and deleted files, and a summary of values for each field of the partition spec used to write the manifest. A manifest list is a valid Iceberg data file: files must use valid Iceberg formats, schemas, and column projection. Manifest list files store manifest_file , a struct with the following fields: v1 v2 Field id, name Type Description required required 500 manifest_path string Location of the manifest file required required 501 manifest_length long Length of the manifest file required required 502 partition_spec_id int ID of a partition spec used to write the manifest; must be listed in table metadata partition-specs required 517 content int with meaning: 0: data , 1: deletes The type of files tracked by the manifest, either data or delete files; 0 for all v1 manifests required 515 sequence_number long The sequence number when the manifest was added to the table; use 0 when reading v1 manifest lists required 516 min_sequence_number long The minimum sequence number of all data or delete files in the manifest; use 0 when reading v1 manifest lists required required 503 added_snapshot_id long ID of the snapshot where the manifest file was added optional required 504 added_files_count int Number of entries in the manifest that have status ADDED (1), when null this is assumed to be non-zero optional required 505 existing_files_count int Number of entries in the manifest that have status EXISTING (0), when null this is assumed to be non-zero optional required 506 deleted_files_count int Number of entries in the manifest that have status DELETED (2), when null this is assumed to be non-zero optional required 512 added_rows_count long Number of rows in all of files in the manifest that have status ADDED , when null this is assumed to be non-zero optional required 513 existing_rows_count long Number of rows in all of files in the manifest that have status EXISTING , when null this is assumed to be non-zero optional required 514 deleted_rows_count long Number of rows in all of files in the manifest that have status DELETED , when null this is assumed to be non-zero optional optional 507 partitions list<508: field_summary> (see below) A list of field summaries for each partition field in the spec. Each field in the list corresponds to a field in the manifest file\u2019s partition spec. optional optional 519 key_metadata binary Implementation-specific key metadata for encryption field_summary is a struct with the following fields: v1 v2 Field id, name Type Description required required 509 contains_null boolean Whether the manifest contains at least one partition with a null value for the field optional optional 518 contains_nan boolean Whether the manifest contains at least one partition with a NaN value for the field optional optional 510 lower_bound bytes [1] Lower bound for the non-null, non-NaN values in the partition field, or null if all values are null or NaN [2] optional optional 511 upper_bound bytes [1] Upper bound for the non-null, non-NaN values in the partition field, or null if all values are null or NaN [2] Notes: Lower and upper bounds are serialized to bytes using the single-object serialization in Appendix D. The type of used to encode the value is the type of the partition field data. If -0.0 is a value of the partition field, the lower_bound must not be +0.0, and if +0.0 is a value of the partition field, the upper_bound must not be -0.0. Scan Planning \u00b6 Scans are planned by reading the manifest files for the current snapshot. Deleted entries in data and delete manifests are not used in a scan. Manifests that contain no matching files, determined using either file counts or partition summaries, may be skipped. For each manifest, scan predicates, which filter data rows, are converted to partition predicates, which filter data and delete files. These partition predicates are used to select the data and delete files in the manifest. This conversion uses the partition spec used to write the manifest file. Scan predicates are converted to partition predicates using an inclusive projection : if a scan predicate matches a row, then the partition predicate must match that row\u2019s partition. This is called inclusive [1] because rows that do not match the scan predicate may be included in the scan by the partition predicate. For example, an events table with a timestamp column named ts that is partitioned by ts_day=day(ts) is queried by users with ranges over the timestamp column: ts > X . The inclusive projection is ts_day >= day(X) , which is used to select files that may have matching rows. Note that, in most cases, timestamps just before X will be included in the scan because the file contains rows that match the predicate and rows that do not match the predicate. Scan predicates are also used to filter data and delete files using column bounds and counts that are stored by field id in manifests. The same filter logic can be used for both data and delete files because both store metrics of the rows either inserted or deleted. If metrics show that a delete file has no rows that match a scan predicate, it may be ignored just as a data file would be ignored [2]. Data files that match the query filter must be read by the scan. Delete files that match the query filter must be applied to data files at read time, limited by the scope of the delete file using the following rules. A position delete file must be applied to a data file when all of the following are true: The data file\u2019s sequence number is less than or equal to the delete file\u2019s sequence number The data file\u2019s partition (both spec and partition values) is equal to the delete file\u2019s partition An equality delete file must be applied to a data file when all of the following are true: The data file\u2019s sequence number is strictly less than the delete\u2019s sequence number The data file\u2019s partition (both spec and partition values) is equal to the delete file\u2019s partition or the delete file\u2019s partition spec is unpartitioned In general, deletes are applied only to data files that are older and in the same partition, except for two special cases: Equality delete files stored with an unpartitioned spec are applied as global deletes. Otherwise, delete files do not apply to files in other partitions. Position delete files must be applied to data files from the same commit, when the data and delete file sequence numbers are equal. This allows deleting rows that were added in the same commit. Notes: An alternative, strict projection , creates a partition predicate that will match a file if all of the rows in the file must match the scan predicate. These projections are used to calculate the residual predicates for each file in a scan. For example, if file_a has rows with id between 1 and 10 and a delete file contains rows with id between 1 and 4, a scan for id = 9 may ignore the delete file because none of the deletes can match a row that will be selected. Table Metadata \u00b6 Table metadata is stored as JSON. Each table metadata change creates a new table metadata file that is committed by an atomic operation. This operation is used to ensure that a new version of table metadata replaces the version on which it was based. This produces a linear history of table versions and ensures that concurrent writes are not lost. The atomic operation used to commit metadata depends on how tables are tracked and is not standardized by this spec. See the sections below for examples. Table Metadata Fields \u00b6 Table metadata consists of the following fields: v1 v2 Field Description required required format-version An integer version number for the format. Currently, this can be 1 or 2 based on the spec. Implementations must throw an exception if a table\u2019s version is higher than the supported version. optional required table-uuid A UUID that identifies the table, generated when the table is created. Implementations must throw an exception if a table\u2019s UUID does not match the expected UUID after refreshing metadata. required required location The table\u2019s base location. This is used by writers to determine where to store data files, manifest files, and table metadata files. required last-sequence-number The table\u2019s highest assigned sequence number, a monotonically increasing long that tracks the order of snapshots in a table. required required last-updated-ms Timestamp in milliseconds from the unix epoch when the table was last updated. Each table metadata file should update this field just before writing. required required last-column-id An integer; the highest assigned column ID for the table. This is used to ensure columns are always assigned an unused ID when evolving schemas. required schema The table\u2019s current schema. ( Deprecated : use schemas and current-schema-id instead) optional required schemas A list of schemas, stored as objects with schema-id . optional required current-schema-id ID of the table\u2019s current schema. required partition-spec The table\u2019s current partition spec, stored as only fields. Note that this is used by writers to partition data, but is not used when reading because reads use the specs stored in manifest files. ( Deprecated : use partition-specs and default-spec-id instead) optional required partition-specs A list of partition specs, stored as full partition spec objects. optional required default-spec-id ID of the \u201ccurrent\u201d spec that writers should use by default. optional required last-partition-id An integer; the highest assigned partition field ID across all partition specs for the table. This is used to ensure partition fields are always assigned an unused ID when evolving specs. optional optional properties A string to string map of table properties. This is used to control settings that affect reading and writing and is not intended to be used for arbitrary metadata. For example, commit.retry.num-retries is used to control the number of commit retries. optional optional current-snapshot-id long ID of the current table snapshot. optional optional snapshots A list of valid snapshots. Valid snapshots are snapshots for which all data files exist in the file system. A data file must not be deleted from the file system until the last snapshot in which it was listed is garbage collected. optional optional snapshot-log A list (optional) of timestamp and snapshot ID pairs that encodes changes to the current snapshot for the table. Each time the current-snapshot-id is changed, a new entry should be added with the last-updated-ms and the new current-snapshot-id. When snapshots are expired from the list of valid snapshots, all entries before a snapshot that has expired should be removed. optional optional metadata-log A list (optional) of timestamp and metadata file location pairs that encodes changes to the previous metadata files for the table. Each time a new metadata file is created, a new entry of the previous metadata file location should be added to the list. Tables can be configured to remove oldest metadata log entries and keep a fixed-size log of the most recent entries after a commit. optional required sort-orders A list of sort orders, stored as full sort order objects. optional required default-sort-order-id Default sort order id of the table. Note that this could be used by writers, but is not used when reading because reads use the specs stored in manifest files. For serialization details, see Appendix C. Commit Conflict Resolution and Retry \u00b6 When two commits happen at the same time and are based on the same version, only one commit will succeed. In most cases, the failed commit can be applied to the new current version of table metadata and retried. Updates verify the conditions under which they can be applied to a new version and retry if those conditions are met. Append operations have no requirements and can always be applied. Replace operations must verify that the files that will be deleted are still in the table. Examples of replace operations include format changes (replace an Avro file with a Parquet file) and compactions (several files are replaced with a single file that contains the same rows). Delete operations must verify that specific files to delete are still in the table. Delete operations based on expressions can always be applied (e.g., where timestamp < X). Table schema updates and partition spec changes must validate that the schema has not changed between the base version and the current version. File System Tables \u00b6 An atomic swap can be implemented using atomic rename in file systems that support it, like HDFS or most local file systems [1]. Each version of table metadata is stored in a metadata folder under the table\u2019s base location using a file naming scheme that includes a version number, V : v<V>.metadata.json . To commit a new metadata version, V+1 , the writer performs the following steps: Read the current table metadata version V . Create new table metadata based on version V . Write the new table metadata to a unique file: <random-uuid>.metadata.json . Rename the unique file to the well-known file for version V : v<V+1>.metadata.json . If the rename succeeds, the commit succeeded and V+1 is the table\u2019s current version If the rename fails, go back to step 1. Notes: The file system table scheme is implemented in HadoopTableOperations . Metastore Tables \u00b6 The atomic swap needed to commit new versions of table metadata can be implemented by storing a pointer in a metastore or database that is updated with a check-and-put operation [1]. The check-and-put validates that the version of the table that a write is based on is still current and then makes the new metadata from the write the current version. Each version of table metadata is stored in a metadata folder under the table\u2019s base location using a naming scheme that includes a version and UUID: <V>-<uuid>.metadata.json . To commit a new metadata version, V+1 , the writer performs the following steps: Create a new table metadata file based on the current metadata. Write the new table metadata to a unique file: <V+1>-<uuid>.metadata.json . Request that the metastore swap the table\u2019s metadata pointer from the location of V to the location of V+1 . If the swap succeeds, the commit succeeded. V was still the latest metadata version and the metadata file for V+1 is now the current metadata. If the swap fails, another writer has already created V+1 . The current writer goes back to step 1. Notes: The metastore table scheme is partly implemented in BaseMetastoreTableOperations . Delete Formats \u00b6 This section details how to encode row-level deletes in Iceberg delete files. Row-level deletes are not supported in v1. Row-level delete files are valid Iceberg data files: files must use valid Iceberg formats, schemas, and column projection. It is recommended that delete files are written using the table\u2019s default file format. Row-level delete files are tracked by manifests, like data files. A separate set of manifests is used for delete files, but the manifest schemas are identical. Both position and equality deletes allow encoding deleted row values with a delete. This can be used to reconstruct a stream of changes to a table. Position Delete Files \u00b6 Position-based delete files identify deleted rows by file and position in one or more data files, and may optionally contain the deleted row. A data row is deleted if there is an entry in a position delete file for the row\u2019s file and position in the data file, starting at 0. Position-based delete files store file_position_delete , a struct with the following fields: Field id, name Type Description 2147483546 file_path string Full URI of a data file with FS scheme. This must match the file_path of the target data file in a manifest entry 2147483545 pos long Ordinal position of a deleted row in the target data file identified by file_path , starting at 0 2147483544 row required struct<...> [1] Deleted row values. Omit the column when not storing deleted rows. When present in the delete file, row is required because all delete entries must include the row values. When the deleted row column is present, its schema may be any subset of the table schema and must use field ids matching the table. To ensure the accuracy of statistics, all delete entries must include row values, or the column must be omitted (this is why the column type is required ). The rows in the delete file must be sorted by file_path then position to optimize filtering rows while scanning. Sorting by file_path allows filter pushdown by file in columnar storage formats. Sorting by position allows filtering rows while scanning, to avoid keeping deletes in memory. Equality Delete Files \u00b6 Equality delete files identify deleted rows in a collection of data files by one or more column values, and may optionally contain additional columns of the deleted row. Equality delete files store any subset of a table\u2019s columns and use the table\u2019s field ids. The delete columns are the columns of the delete file used to match data rows. Delete columns are identified by id in the delete file metadata column equality_ids . Float and double columns cannot be used as delete columns in equality delete files. A data row is deleted if its values are equal to all delete columns for any row in an equality delete file that applies to the row\u2019s data file (see Scan Planning ). Each row of the delete file produces one equality predicate that matches any row where the delete columns are equal. Multiple columns can be thought of as an AND of equality predicates. A null value in a delete column matches a row if the row\u2019s value is null , equivalent to col IS NULL . For example, a table with the following data: 1: id | 2: category | 3: name -------|-------------|--------- 1 | marsupial | Koala 2 | toy | Teddy 3 | NULL | Grizzly 4 | NULL | Polar The delete id = 3 could be written as either of the following equality delete files: equality_ids=[1] 1: id ------- 3 equality_ids=[1] 1: id | 2: category | 3: name -------|-------------|--------- 3 | NULL | Grizzly The delete id = 4 AND category IS NULL could be written as the following equality delete file: equality_ids=[1, 2] 1: id | 2: category | 3: name -------|-------------|--------- 4 | NULL | Polar If a delete column in an equality delete file is later dropped from the table, it must still be used when applying the equality deletes. If a column was added to a table and later used as a delete column in an equality delete file, the column value is read for older data files using normal projection rules (defaults to null ). Delete File Stats \u00b6 Manifests hold the same statistics for delete files and data files. For delete files, the metrics describe the values that were deleted. Appendix A: Format-specific Requirements \u00b6 Avro \u00b6 Data Type Mappings Values should be stored in Avro using the Avro types and logical type annotations in the table below. Optional fields, array elements, and map values must be wrapped in an Avro union with null . This is the only union type allowed in Iceberg data files. Optional fields must always set the Avro field default value to null. Maps with non-string keys must use an array representation with the map logical type. The array representation or Avro\u2019s map type may be used for maps with string keys. Type Avro type Notes boolean boolean int int long long float float double double decimal(P,S) { \"type\": \"fixed\", \"size\": minBytesRequired(P), \"logicalType\": \"decimal\", \"precision\": P, \"scale\": S } Stored as fixed using the minimum number of bytes for the given precision. date { \"type\": \"int\", \"logicalType\": \"date\" } Stores days from the 1970-01-01. time { \"type\": \"long\", \"logicalType\": \"time-micros\" } Stores microseconds from midnight. timestamp { \"type\": \"long\", \"logicalType\": \"timestamp-micros\", \"adjust-to-utc\": false } Stores microseconds from 1970-01-01 00:00:00.000000. timestamptz { \"type\": \"long\", \"logicalType\": \"timestamp-micros\", \"adjust-to-utc\": true } Stores microseconds from 1970-01-01 00:00:00.000000 UTC. string string uuid { \"type\": \"fixed\", \"size\": 16, \"logicalType\": \"uuid\" } fixed(L) { \"type\": \"fixed\", \"size\": L } binary bytes struct record list array map array of key-value records, or map when keys are strings (optional). Array storage must use logical type name map and must store elements that are 2-field records. The first field is a non-null key and the second field is the value. Field IDs Iceberg struct, list, and map types identify nested types by ID. When writing data to Avro files, these IDs must be stored in the Avro schema to support ID-based column pruning. IDs are stored as JSON integers in the following locations: ID Avro schema location Property Example Struct field Record field object field-id { \"type\": \"record\", ... \"fields\": [ { \"name\": \"l\", \"type\": [\"null\", \"long\"], \"default\": null, \"field-id\": 8 } ] } List element Array schema object element-id { \"type\": \"array\", \"items\": \"int\", \"element-id\": 9 } String map key Map schema object key-id { \"type\": \"map\", \"values\": \"int\", \"key-id\": 10, \"value-id\": 11 } String map value Map schema object value-id Map key, value Key, value fields in the element record. field-id { \"type\": \"array\", \"logicalType\": \"map\", \"items\": { \"type\": \"record\", \"name\": \"k12_v13\", \"fields\": [ { \"name\": \"key\", \"type\": \"int\", \"field-id\": 12 }, { \"name\": \"value\", \"type\": \"string\", \"field-id\": 13 } ] } } Note that the string map case is for maps where the key type is a string. Using Avro\u2019s map type in this case is optional. Maps with string keys may be stored as arrays. Parquet \u00b6 Data Type Mappings Values should be stored in Parquet using the types and logical type annotations in the table below. Column IDs are required. Lists must use the 3-level representation . Type Parquet physical type Logical type Notes boolean boolean int int long long float float double double decimal(P,S) P <= 9 : int32 , P <= 18 : int64 , fixed otherwise DECIMAL(P,S) Fixed must use the minimum number of bytes that can store P . date int32 DATE Stores days from the 1970-01-01. time int64 TIME_MICROS with adjustToUtc=false Stores microseconds from midnight. timestamp int64 TIMESTAMP_MICROS with adjustToUtc=false Stores microseconds from 1970-01-01 00:00:00.000000. timestamptz int64 TIMESTAMP_MICROS with adjustToUtc=true Stores microseconds from 1970-01-01 00:00:00.000000 UTC. string binary UTF8 Encoding must be UTF-8. uuid fixed_len_byte_array[16] UUID fixed(L) fixed_len_byte_array[L] binary binary struct group list 3-level list LIST See Parquet docs for 3-level representation. map 3-level map MAP See Parquet docs for 3-level representation. ORC \u00b6 Data Type Mappings Type ORC type ORC type attributes Notes boolean boolean int int ORC tinyint and smallint would also map to int . long long float float double double decimal(P,S) decimal date date time long iceberg.long-type = TIME Stores microseconds from midnight. timestamp timestamp [1] timestamptz timestamp_instant [1] string string ORC varchar and char would also map to string . uuid binary iceberg.binary-type = UUID fixed(L) binary iceberg.binary-type = FIXED & iceberg.length = L The length would not be checked by the ORC reader and should be checked by the adapter. binary binary struct struct list array map map Notes: ORC\u2019s TimestampColumnVector comprises of a time field (milliseconds since epoch) and a nanos field (nanoseconds within the second). Hence the milliseconds within the second are reported twice; once in the time field and again in the nanos field. The read adapter should only use milliseconds within the second from one of these fields. The write adapter should also report milliseconds within the second twice; once in the time field and again in the nanos field. ORC writer is expected to correctly consider millis information from one of the fields. More details at https://issues.apache.org/jira/browse/ORC-546 One of the interesting challenges with this is how to map Iceberg\u2019s schema evolution (id based) on to ORC\u2019s (name based). In theory, we could use Iceberg\u2019s column ids as the column and field names, but that would suck from a user\u2019s point of view. The column IDs must be stored in ORC type attributes using the key iceberg.id , and iceberg.required to store \"true\" if the Iceberg column is required, otherwise it will be optional. Iceberg would build the desired reader schema with their schema evolution rules and pass that down to the ORC reader, which would then use its schema evolution to map that to the writer\u2019s schema. Basically, Iceberg would need to change the names of columns and fields to get the desired mapping. Iceberg writer ORC writer Iceberg reader ORC reader struct<a (1): int, b (2): string> struct<a: int, b: string> struct<a (2): string, c (3): date> struct<b: string, c: date> struct<a (1): struct<b (2): string, c (3): date>> struct<a: struct<b:string, c:date>> struct<aa (1): struct<cc (3): date, bb (2): string>> struct<a: struct<c:date, b:string>> Appendix B: 32-bit Hash Requirements \u00b6 The 32-bit hash implementation is 32-bit Murmur3 hash, x86 variant, seeded with 0. Primitive type Hash specification Test value int hashLong(long(v)) [1] 34 \uffeb 2017239379 long hashBytes(littleEndianBytes(v)) 34L \uffeb 2017239379 decimal(P,S) hashBytes(minBigEndian(unscaled(v))) [2] 14.20 \uffeb -500754589 date hashInt(daysFromUnixEpoch(v)) 2017-11-16 \uffeb -653330422 time hashLong(microsecsFromMidnight(v)) 22:31:08 \uffeb -662762989 timestamp hashLong(microsecsFromUnixEpoch(v)) 2017-11-16T22:31:08 \uffeb -2047944441 timestamptz hashLong(microsecsFromUnixEpoch(v)) 2017-11-16T14:31:08-08:00 \uffeb -2047944441 string hashBytes(utf8Bytes(v)) iceberg \uffeb 1210000089 uuid hashBytes(uuidBytes(v)) [3] f79c3e09-677c-4bbd-a479-3f349cb785e7 \uffeb 1488055340 fixed(L) hashBytes(v) 00 01 02 03 \uffeb -188683207 binary hashBytes(v) 00 01 02 03 \uffeb -188683207 The types below are not currently valid for bucketing, and so are not hashed. However, if that changes and a hash value is needed, the following table shall apply: Primitive type Hash specification Test value boolean false: hashInt(0) , true: hashInt(1) true \uffeb 1392991556 float hashDouble(double(v)) [4] 1.0F \uffeb -142385009 double hashLong(doubleToLongBits(v)) 1.0D \uffeb -142385009 Notes: Integer and long hash results must be identical for all integer values. This ensures that schema evolution does not change bucket partition values if integer types are promoted. Decimal values are hashed using the minimum number of bytes required to hold the unscaled value as a two\u2019s complement big-endian; this representation does not include padding bytes required for storage in a fixed-length array. Hash results are not dependent on decimal scale, which is part of the type, not the data value. UUIDs are encoded using big endian. The test UUID for the example above is: f79c3e09-677c-4bbd-a479-3f349cb785e7 . This UUID encoded as a byte array is: F7 9C 3E 09 67 7C 4B BD A4 79 3F 34 9C B7 85 E7 Float hash values are the result of hashing the float cast to double to ensure that schema evolution does not change hash values if float types are promoted. Appendix C: JSON serialization \u00b6 Schemas \u00b6 Schemas are serialized as a JSON object with the same fields as a struct in the table below, and the following additional fields: v1 v2 Field JSON representation Example optional required schema-id JSON int 0 optional optional identifier-field-ids JSON list of ints [1, 2] Types are serialized according to this table: Type JSON representation Example boolean JSON string: \"boolean\" \"boolean\" int JSON string: \"int\" \"int\" long JSON string: \"long\" \"long\" float JSON string: \"float\" \"float\" double JSON string: \"double\" \"double\" date JSON string: \"date\" \"date\" time JSON string: \"time\" \"time\" timestamp without zone JSON string: \"timestamp\" \"timestamp\" timestamp with zone JSON string: \"timestamptz\" \"timestamptz\" string JSON string: \"string\" \"string\" uuid JSON string: \"uuid\" \"uuid\" fixed(L) JSON string: \"fixed[<L>]\" \"fixed[16]\" binary JSON string: \"binary\" \"binary\" decimal(P, S) JSON string: \"decimal(<P>,<S>)\" \"decimal(9,2)\" , \"decimal(9, 2)\" struct JSON object: { \"type\": \"struct\", \"fields\": [ { \"id\": <field id int>, \"name\": <name string>, \"required\": <boolean>, \"type\": <type JSON>, \"doc\": <comment string> }, ... ] } { \"type\": \"struct\", \"fields\": [ { \"id\": 1, \"name\": \"id\", \"required\": true, \"type\": \"uuid\" }, { \"id\": 2, \"name\": \"data\", \"required\": false, \"type\": { \"type\": \"list\", ... } } ] } list JSON object: { \"type\": \"list\", \"element-id\": <id int>, \"element-required\": <bool> \"element\": <type JSON> } { \"type\": \"list\", \"element-id\": 3, \"element-required\": true, \"element\": \"string\" } map JSON object: { \"type\": \"map\", \"key-id\": <key id int>, \"key\": <type JSON>, \"value-id\": <val id int>, \"value-required\": <bool> \"value\": <type JSON> } { \"type\": \"map\", \"key-id\": 4, \"key\": \"string\", \"value-id\": 5, \"value-required\": false, \"value\": \"double\" } Partition Specs \u00b6 Partition specs are serialized as a JSON object with the following fields: Field JSON representation Example spec-id JSON int 0 fields JSON list: [ <partition field JSON>, ... ] [ { \"source-id\": 4, \"field-id\": 1000, \"name\": \"ts_day\", \"transform\": \"day\" }, { \"source-id\": 1, \"field-id\": 1001, \"name\": \"id_bucket\", \"transform\": \"bucket[16]\" } ] Each partition field in the fields list is stored as an object. See the table for more detail: Transform or Field JSON representation Example identity JSON string: \"identity\" \"identity\" bucket[N] JSON string: \"bucket<N>]\" \"bucket[16]\" truncate[W] JSON string: \"truncate[<W>]\" \"truncate[20]\" year JSON string: \"year\" \"year\" month JSON string: \"month\" \"month\" day JSON string: \"day\" \"day\" hour JSON string: \"hour\" \"hour\" Partition Field JSON object: { \"source-id\": <id int>, \"field-id\": <field id int>, \"name\": <name string>, \"transform\": <transform JSON> } { \"source-id\": 1, \"field-id\": 1000, \"name\": \"id_bucket\", \"transform\": \"bucket[16]\" } In some cases partition specs are stored using only the field list instead of the object format that includes the spec ID, like the deprecated partition-spec field in table metadata. The object format should be used unless otherwise noted in this spec. The field-id property was added for each partition field in v2. In v1, the reference implementation assigned field ids sequentially in each spec starting at 1,000. See Partition Evolution for more details. Sort Orders \u00b6 Sort orders are serialized as a list of JSON object, each of which contains the following fields: Field JSON representation Example order-id JSON int 1 fields JSON list: [ <sort field JSON>, ... ] [ { \"transform\": \"identity\", \"source-id\": 2, \"direction\": \"asc\", \"null-order\": \"nulls-first\" }, { \"transform\": \"bucket[4]\", \"source-id\": 3, \"direction\": \"desc\", \"null-order\": \"nulls-last\" } ] Each sort field in the fields list is stored as an object with the following properties: Field JSON representation Example Sort Field JSON object: { \"transform\": <transform JSON>, \"source-id\": <source id int>, \"direction\": <direction string>, \"null-order\": <null-order string> } { \"transform\": \"bucket[4]\", \"source-id\": 3, \"direction\": \"desc\", \"null-order\": \"nulls-last\" } The following table describes the possible values for the some of the field within sort field: Field JSON representation Possible values direction JSON string \"asc\", \"desc\" null-order JSON string \"nulls-first\", \"nulls-last\" Table Metadata and Snapshots \u00b6 Table metadata is serialized as a JSON object according to the following table. Snapshots are not serialized separately. Instead, they are stored in the table metadata JSON. Metadata field JSON representation Example format-version JSON int 1 table-uuid JSON string \"fb072c92-a02b-11e9-ae9c-1bb7bc9eca94\" location JSON string \"s3://b/wh/data.db/table\" last-updated-ms JSON long 1515100955770 last-column-id JSON int 22 schema JSON schema (object) See above, read schemas instead schemas JSON schemas (list of objects) See above current-schema-id JSON int 0 partition-spec JSON partition fields (list) See above, read partition-specs instead partition-specs JSON partition specs (list of objects) See above default-spec-id JSON int 0 last-partition-id JSON int 1000 properties JSON object: { \"<key>\": \"<val>\", ... } { \"write.format.default\": \"avro\", \"commit.retry.num-retries\": \"4\" } current-snapshot-id JSON long 3051729675574597004 snapshots JSON list of objects: [ { \"snapshot-id\": <id>, \"timestamp-ms\": <timestamp-in-ms>, \"summary\": { \"operation\": <operation>, ... }, \"manifest-list\": \"<location>\", \"schema-id\": \"<id>\" }, ... ] [ { \"snapshot-id\": 3051729675574597004, \"timestamp-ms\": 1515100955770, \"summary\": { \"operation\": \"append\" }, \"manifest-list\": \"s3://b/wh/.../s1.avro\" \"schema-id\": 0 } ] snapshot-log JSON list of objects: [ { \"snapshot-id\": , \"timestamp-ms\": }, ... ] [ { \"snapshot-id\": 30517296..., \"timestamp-ms\": 1515100... } ] metadata-log JSON list of objects: [ { \"metadata-file\": , \"timestamp-ms\": }, ... ] [ { \"metadata-file\": \"s3://bucket/.../v1.json\", \"timestamp-ms\": 1515100... } ] sort-orders JSON sort orders (list of sort field object) See above default-sort-order-id JSON int 0 Appendix D: Single-value serialization \u00b6 This serialization scheme is for storing single values as individual binary values in the lower and upper bounds maps of manifest files. Type Binary serialization boolean 0x00 for false, non-zero byte for true int Stored as 4-byte little-endian long Stored as 8-byte little-endian float Stored as 4-byte little-endian double Stored as 8-byte little-endian date Stores days from the 1970-01-01 in an 4-byte little-endian int time Stores microseconds from midnight in an 8-byte little-endian long timestamp without zone Stores microseconds from 1970-01-01 00:00:00.000000 in an 8-byte little-endian long timestamp with zone Stores microseconds from 1970-01-01 00:00:00.000000 UTC in an 8-byte little-endian long string UTF-8 bytes (without length) uuid 16-byte big-endian value, see example in Appendix B fixed(L) Binary value binary Binary value (without length) decimal(P, S) Stores unscaled value as two\u2019s-complement big-endian binary, using the minimum number of bytes for the value struct Not supported list Not supported map Not supported Appendix E: Format version changes \u00b6 Version 2 \u00b6 Writing v1 metadata: Table metadata field last-sequence-number should not be written Snapshot field sequence-number should not be written Manifest list field sequence-number should not be written Manifest list field min-sequence-number should not be written Manifest list field content must be 0 (data) or omitted Manifest entry field sequence_number should not be written Data file field content must be 0 (data) or omitted Reading v1 metadata for v2: Table metadata field last-sequence-number must default to 0 Snapshot field sequence-number must default to 0 Manifest list field sequence-number must default to 0 Manifest list field min-sequence-number must default to 0 Manifest list field content must default to 0 (data) Manifest entry field sequence_number must default to 0 Data file field content must default to 0 (data) Writing v2 metadata: Table metadata JSON: last-sequence-number was added and is required; default to 0 when reading v1 metadata table-uuid is now required current-schema-id is now required schemas is now required partition-specs is now required default-spec-id is now required last-partition-id is now required sort-orders is now required default-sort-order-id is now required schema is no longer required and should be omitted; use schemas and current-schema-id instead partition-spec is no longer required and should be omitted; use partition-specs and default-spec-id instead Snapshot JSON: sequence-number was added and is required; default to 0 when reading v1 metadata manifest-list is now required manifests is no longer required and should be omitted; always use manifest-list instead Manifest list manifest_file : content was added and is required; 0=data, 1=deletes; default to 0 when reading v1 manifest lists sequence_number was added and is required min_sequence_number was added and is required added_files_count is now required existing_files_count is now required deleted_files_count is now required added_rows_count is now required existing_rows_count is now required deleted_rows_count is now required Manifest key-value metadata: schema-id is now required partition-spec-id is now required format-version is now required content was added and is required (must be \u201cdata\u201d or \u201cdeletes\u201d) Manifest manifest_entry : snapshot_id is now optional to support inheritance sequence_number was added and is optional, to support inheritance Manifest data_file : content was added and is required; 0=data, 1=position deletes, 2=equality deletes; default to 0 when reading v1 manifests equality_ids was added, to be used for equality deletes only block_size_in_bytes was removed (breaks v1 reader compatibility) file_ordinal was removed sort_columns was removed Note that these requirements apply when writing data to a v2 table. Tables that are upgraded from v1 may contain metadata that does not follow these requirements. Implementations should remain backward-compatible with v1 metadata requirements.","title":"Spec"},{"location":"spec/#iceberg-table-spec","text":"This is a specification for the Iceberg table format that is designed to manage a large, slow-changing collection of files in a distributed file system or key-value store as a table.","title":"Iceberg Table Spec"},{"location":"spec/#format-versioning","text":"Versions 1 and 2 of the Iceberg spec are complete and adopted by the community. The format version number is incremented when new features are added that will break forward-compatibility\u2014that is, when older readers would not read newer table features correctly. Tables may continue to be written with an older version of the spec to ensure compatibility by not using features that are not yet implemented by processing engines.","title":"Format Versioning"},{"location":"spec/#version-1-analytic-data-tables","text":"Version 1 of the Iceberg spec defines how to manage large analytic tables using immutable file formats: Parquet, Avro, and ORC. All version 1 data and metadata files are valid after upgrading a table to version 2. Appendix E documents how to default version 2 fields when reading version 1 metadata.","title":"Version 1: Analytic Data Tables"},{"location":"spec/#version-2-row-level-deletes","text":"Version 2 of the Iceberg spec adds row-level updates and deletes for analytic tables with immutable files. The primary change in version 2 adds delete files to encode that rows that are deleted in existing data files. This version can be used to delete or replace individual rows in immutable data files without rewriting the files. In addition to row-level deletes, version 2 makes some requirements stricter for writers. The full set of changes are listed in Appendix E .","title":"Version 2: Row-level Deletes"},{"location":"spec/#goals","text":"Serializable isolation \u2013 Reads will be isolated from concurrent writes and always use a committed snapshot of a table\u2019s data. Writes will support removing and adding files in a single operation and are never partially visible. Readers will not acquire locks. Speed \u2013 Operations will use O(1) remote calls to plan the files for a scan and not O(n) where n grows with the size of the table, like the number of partitions or files. Scale \u2013 Job planning will be handled primarily by clients and not bottleneck on a central metadata store. Metadata will include information needed for cost-based optimization. Evolution \u2013 Tables will support full schema and partition spec evolution. Schema evolution supports safe column add, drop, reorder and rename, including in nested structures. Dependable types \u2013 Tables will provide well-defined and dependable support for a core set of types. Storage separation \u2013 Partitioning will be table configuration. Reads will be planned using predicates on data values, not partition values. Tables will support evolving partition schemes. Formats \u2013 Underlying data file formats will support identical schema evolution rules and types. Both read- and write-optimized formats will be available.","title":"Goals"},{"location":"spec/#overview","text":"This table format tracks individual data files in a table instead of directories. This allows writers to create data files in-place and only adds files to the table in an explicit commit. Table state is maintained in metadata files. All changes to table state create a new metadata file and replace the old metadata with an atomic swap. The table metadata file tracks the table schema, partitioning config, custom properties, and snapshots of the table contents. A snapshot represents the state of a table at some time and is used to access the complete set of data files in the table. Data files in snapshots are tracked by one or more manifest files that contain a row for each data file in the table, the file\u2019s partition data, and its metrics. The data in a snapshot is the union of all files in its manifests. Manifest files are reused across snapshots to avoid rewriting metadata that is slow-changing. Manifests can track data files with any subset of a table and are not associated with partitions. The manifests that make up a snapshot are stored in a manifest list file. Each manifest list stores metadata about manifests, including partition stats and data file counts. These stats are used to avoid reading manifests that are not required for an operation.","title":"Overview"},{"location":"spec/#optimistic-concurrency","text":"An atomic swap of one table metadata file for another provides the basis for serializable isolation. Readers use the snapshot that was current when they load the table metadata and are not affected by changes until they refresh and pick up a new metadata location. Writers create table metadata files optimistically, assuming that the current version will not be changed before the writer\u2019s commit. Once a writer has created an update, it commits by swapping the table\u2019s metadata file pointer from the base version to the new version. If the snapshot on which an update is based is no longer current, the writer must retry the update based on the new current version. Some operations support retry by re-applying metadata changes and committing, under well-defined conditions. For example, a change that rewrites files can be applied to a new table snapshot if all of the rewritten files are still in the table. The conditions required by a write to successfully commit determines the isolation level. Writers can select what to validate and can make different isolation guarantees.","title":"Optimistic Concurrency"},{"location":"spec/#sequence-numbers","text":"The relative age of data and delete files relies on a sequence number that is assigned to every successful commit. When a snapshot is created for a commit, it is optimistically assigned the next sequence number, and it is written into the snapshot\u2019s metadata. If the commit fails and must be retried, the sequence number is reassigned and written into new snapshot metadata. All manifests, data files, and delete files created for a snapshot inherit the snapshot\u2019s sequence number. Manifest file metadata in the manifest list stores a manifest\u2019s sequence number. New data and metadata file entries are written with null in place of a sequence number, which is replaced with the manifest\u2019s sequence number at read time. When a data or delete file is written to a new manifest (as \u201cexisting\u201d), the inherited sequence number is written to ensure it does not change after it is first inherited. Inheriting the sequence number from manifest metadata allows writing a new manifest once and reusing it in commit retries. To change a sequence number for a retry, only the manifest list must be rewritten \u2013 which would be rewritten anyway with the latest set of manifests.","title":"Sequence Numbers"},{"location":"spec/#row-level-deletes","text":"Row-level deletes are stored in delete files. There are two ways to encode a row-level delete: Position deletes mark a row deleted by data file path and the row position in the data file Equality deletes mark a row deleted by one or more column values, like id = 5 Like data files, delete files are tracked by partition. In general, a delete file must be applied to older data files with the same partition; see Scan Planning for details. Column metrics can be used to determine whether a delete file\u2019s rows overlap the contents of a data file or a scan range.","title":"Row-level Deletes"},{"location":"spec/#file-system-operations","text":"Iceberg only requires that file systems support the following operations: In-place write \u2013 Files are not moved or altered once they are written. Seekable reads \u2013 Data file formats require seek support. Deletes \u2013 Tables delete files that are no longer used. These requirements are compatible with object stores, like S3. Tables do not require random-access writes. Once written, data and metadata files are immutable until they are deleted. Tables do not require rename, except for tables that use atomic rename to implement the commit operation for new metadata files.","title":"File System Operations"},{"location":"spec/#specification","text":"","title":"Specification"},{"location":"spec/#terms","text":"Schema \u2013 Names and types of fields in a table. Partition spec \u2013 A definition of how partition values are derived from data fields. Snapshot \u2013 The state of a table at some point in time, including the set of all data files. Manifest list \u2013 A file that lists manifest files; one per snapshot. Manifest \u2013 A file that lists data or delete files; a subset of a snapshot. Data file \u2013 A file that contains rows of a table. Delete file \u2013 A file that encodes rows of a table that are deleted by position or data values.","title":"Terms"},{"location":"spec/#writer-requirements","text":"Some tables in this spec have columns that specify requirements for v1 and v2 tables. These requirements are intended for writers when adding metadata files to a table with the given version. Requirement Write behavior (blank) The field should be omitted optional The field can be written required The field must be written Readers should be more permissive because v1 metadata files are allowed in v2 tables so that tables can be upgraded to v2 without rewriting the metadata tree. For manifest list and manifest files, this table shows the expected v2 read behavior: v1 v2 v2 read behavior optional Read the field as optional required Read the field as optional ; it may be missing in v1 files optional Ignore the field optional optional Read the field as optional optional required Read the field as optional ; it may be missing in v1 files required Ignore the field required optional Read the field as optional required required Fill in a default or throw an exception if the field is missing Readers may be more strict for metadata JSON files because the JSON files are not reused and will always match the table version. Required v2 fields that were not present in v1 or optional in v1 may be handled as required fields. For example, a v2 table that is missing last-sequence-number can throw an exception.","title":"Writer requirements"},{"location":"spec/#schemas-and-data-types","text":"A table\u2019s schema is a list of named columns. All data types are either primitives or nested types, which are maps, lists, or structs. A table schema is also a struct type. For the representations of these types in Avro, ORC, and Parquet file formats, see Appendix A.","title":"Schemas and Data Types"},{"location":"spec/#nested-types","text":"A struct is a tuple of typed values. Each field in the tuple is named and has an integer id that is unique in the table schema. Each field can be either optional or required, meaning that values can (or cannot) be null. Fields may be any type. Fields may have an optional comment or doc string. A list is a collection of values with some element type. The element field has an integer id that is unique in the table schema. Elements can be either optional or required. Element types may be any type. A map is a collection of key-value pairs with a key type and a value type. Both the key field and value field each have an integer id that is unique in the table schema. Map keys are required and map values can be either optional or required. Both map keys and map values may be any type, including nested types.","title":"Nested Types"},{"location":"spec/#primitive-types","text":"Primitive type Description Requirements boolean True or false int 32-bit signed integers Can promote to long long 64-bit signed integers float 32-bit IEEE 754 floating point Can promote to double double 64-bit IEEE 754 floating point decimal(P,S) Fixed-point decimal; precision P, scale S Scale is fixed [1], precision must be 38 or less date Calendar date without timezone or time time Time of day without date, timezone Microsecond precision [2] timestamp Timestamp without timezone Microsecond precision [2] timestamptz Timestamp with timezone Stored as UTC [2] string Arbitrary-length character sequences Encoded with UTF-8 [3] uuid Universally unique identifiers Should use 16-byte fixed fixed(L) Fixed-length byte array of length L binary Arbitrary-length byte array Notes: Decimal scale is fixed and cannot be changed by schema evolution. Precision can only be widened. All time and timestamp values are stored with microsecond precision. Timestamps with time zone represent a point in time: values are stored as UTC and do not retain a source time zone ( 2017-11-16 17:10:34 PST is stored/retrieved as 2017-11-17 01:10:34 UTC and these values are considered identical). Timestamps without time zone represent a date and time of day regardless of zone: the time value is independent of zone adjustments ( 2017-11-16 17:10:34 is always retrieved as 2017-11-16 17:10:34 ). Timestamp values are stored as a long that encodes microseconds from the unix epoch. Character strings must be stored as UTF-8 encoded byte arrays. For details on how to serialize a schema to JSON, see Appendix C.","title":"Primitive Types"},{"location":"spec/#schema-evolution","text":"Schemas may be evolved by type promotion or adding, deleting, renaming, or reordering fields in structs (both nested structs and the top-level schema\u2019s struct). Evolution applies changes to the table\u2019s current schema to produce a new schema that is identified by a unique schema ID, is added to the table\u2019s list of schemas, and is set as the table\u2019s current schema. Valid type promotions are: int to long float to double decimal(P, S) to decimal(P', S) if P' > P \u2013 widen the precision of decimal types. Any struct, including a top-level schema, can evolve through deleting fields, adding new fields, renaming existing fields, reordering existing fields, or promoting a primitive using the valid type promotions. Adding a new field assigns a new ID for that field and for any nested fields. Renaming an existing field must change the name, but not the field ID. Deleting a field removes it from the current schema. Field deletion cannot be rolled back unless the field was nullable or if the current snapshot has not changed. Grouping a subset of a struct\u2019s fields into a nested struct is not allowed, nor is moving fields from a nested struct into its immediate parent struct ( struct<a, b, c> \u2194 struct<a, struct<b, c>> ). Evolving primitive types to structs is not allowed, nor is evolving a single-field struct to a primitive ( map<string, int> \u2194 map<string, struct<int>> ).","title":"Schema Evolution"},{"location":"spec/#column-projection","text":"Columns in Iceberg data files are selected by field id. The table schema\u2019s column names and order may change after a data file is written, and projection must be done using field ids. If a field id is missing from a data file, its value for each row should be null . For example, a file may be written with schema 1: a int, 2: b string, 3: c double and read using projection schema 3: measurement, 2: name, 4: a . This must select file columns c (renamed to measurement ), b (now called name ), and a column of null values called a ; in that order.","title":"Column Projection"},{"location":"spec/#identifier-field-ids","text":"A schema can optionally track the set of primitive fields that identify rows in a table, using the property identifier-field-ids (see JSON encoding in Appendix C). Two rows are the \u201csame\u201d\u2014that is, the rows represent the same entity\u2014if the identifier fields are equal. However, uniqueness of rows by this identifier is not guaranteed or required by Iceberg and it is the responsibility of processing engines or data providers to enforce. Identifier fields may be nested in structs but cannot be nested within maps or lists. Float, double, and optional fields cannot be used as identifier fields and a nested field cannot be used as an identifier field if it is nested in an optional struct, to avoid null values in identifiers.","title":"Identifier Field IDs"},{"location":"spec/#reserved-field-ids","text":"Iceberg tables must not use field ids greater than 2147483447 ( Integer.MAX_VALUE - 200 ). This id range is reserved for metadata columns that can be used in user data schemas, like the _file column that holds the file path in which a row was stored. The set of metadata columns is: Field id, name Type Description 2147483646 _file string Path of the file in which a row is stored 2147483645 _pos long Ordinal position of a row in the source data file 2147483644 _deleted boolean Whether the row has been deleted 2147483643 _spec_id int Spec ID used to track the file containing a row 2147483642 _partition struct Partition to which a row belongs 2147483546 file_path string Path of a file, used in position-based delete files 2147483545 pos long Ordinal position of a row, used in position-based delete files 2147483544 row struct<...> Deleted row values, used in position-based delete files","title":"Reserved Field IDs"},{"location":"spec/#partitioning","text":"Data files are stored in manifests with a tuple of partition values that are used in scans to filter out files that cannot contain records that match the scan\u2019s filter predicate. Partition values for a data file must be the same for all records stored in the data file. (Manifests store data files from any partition, as long as the partition spec is the same for the data files.) Tables are configured with a partition spec that defines how to produce a tuple of partition values from a record. A partition spec has a list of fields that consist of: A source column id from the table\u2019s schema A partition field id that is used to identify a partition field and is unique within a partition spec. In v2 table metadata, it is unique across all partition specs. A transform that is applied to the source column to produce a partition value A partition name The source column, selected by id, must be a primitive type and cannot be contained in a map or list, but may be nested in a struct. For details on how to serialize a partition spec to JSON, see Appendix C. Partition specs capture the transform from table data to partition values. This is used to transform predicates to partition predicates, in addition to transforming data values. Deriving partition predicates from column predicates on the table data is used to separate the logical queries from physical storage: the partitioning can change and the correct partition filters are always derived from column predicates. This simplifies queries because users don\u2019t have to supply both logical predicates and partition predicates. For more information, see Scan Planning below.","title":"Partitioning"},{"location":"spec/#partition-transforms","text":"Transform name Description Source types Result type identity Source value, unmodified Any Source type bucket[N] Hash of value, mod N (see below) int , long , decimal , date , time , timestamp , timestamptz , string , uuid , fixed , binary int truncate[W] Value truncated to width W (see below) int , long , decimal , string Source type year Extract a date or timestamp year, as years from 1970 date , timestamp , timestamptz int month Extract a date or timestamp month, as months from 1970-01-01 date , timestamp , timestamptz int day Extract a date or timestamp day, as days from 1970-01-01 date , timestamp , timestamptz date hour Extract a timestamp hour, as hours from 1970-01-01 00:00:00 timestamp , timestamptz int void Always produces null Any Source type or int All transforms must return null for a null input value. The void transform may be used to replace the transform in an existing partition field so that the field is effectively dropped in v1 tables. See partition evolution below.","title":"Partition Transforms"},{"location":"spec/#bucket-transform-details","text":"Bucket partition transforms use a 32-bit hash of the source value. The 32-bit hash implementation is the 32-bit Murmur3 hash, x86 variant, seeded with 0. Transforms are parameterized by a number of buckets [1], N . The hash mod N must produce a positive value by first discarding the sign bit of the hash value. In pseudo-code, the function is: def bucket_N(x) = (murmur3_x86_32_hash(x) & Integer.MAX_VALUE) % N Notes: Changing the number of buckets as a table grows is possible by evolving the partition spec. For hash function details by type, see Appendix B.","title":"Bucket Transform Details"},{"location":"spec/#truncate-transform-details","text":"Type Config Truncate specification Examples int W , width v - (v % W) remainders must be positive [1] W=10 : 1 \uffeb 0 , -1 \uffeb -10 long W , width v - (v % W) remainders must be positive [1] W=10 : 1 \uffeb 0 , -1 \uffeb -10 decimal W , width (no scale) scaled_W = decimal(W, scale(v)) v - (v % scaled_W) [1, 2] W=50 , s=2 : 10.65 \uffeb 10.50 string L , length Substring of length L : v.substring(0, L) L=3 : iceberg \uffeb ice Notes: The remainder, v % W , must be positive. For languages where % can produce negative values, the correct truncate function is: v - (((v % W) + W) % W) The width, W , used to truncate decimal values is applied using the scale of the decimal column to avoid additional (and potentially conflicting) parameters.","title":"Truncate Transform Details"},{"location":"spec/#partition-evolution","text":"Table partitioning can be evolved by adding, removing, renaming, or reordering partition spec fields. Changing a partition spec produces a new spec identified by a unique spec ID that is added to the table\u2019s list of partition specs and may be set as the table\u2019s default spec. When evolving a spec, changes should not cause partition field IDs to change because the partition field IDs are used as the partition tuple field IDs in manifest files. In v2, partition field IDs must be explicitly tracked for each partition field. New IDs are assigned based on the last assigned partition ID in table metadata. In v1, partition field IDs were not tracked, but were assigned sequentially starting at 1000 in the reference implementation. This assignment caused problems when reading metadata tables based on manifest files from multiple specs because partition fields with the same ID may contain different data types. For compatibility with old versions, the following rules are recommended for partition evolution in v1 tables: Do not reorder partition fields Do not drop partition fields; instead replace the field\u2019s transform with the void transform Only add partition fields at the end of the previous partition spec","title":"Partition Evolution"},{"location":"spec/#sorting","text":"Users can sort their data within partitions by columns to gain performance. The information on how the data is sorted can be declared per data or delete file, by a sort order . A sort order is defined by an sort order id and a list of sort fields. The order of the sort fields within the list defines the order in which the sort is applied to the data. Each sort field consists of: A source column id from the table\u2019s schema A transform that is used to produce values to be sorted on from the source column. This is the same transform as described in partition transforms . A sort direction , that can only be either asc or desc A null order that describes the order of null values when sorted. Can only be either nulls-first or nulls-last Order id 0 is reserved for the unsorted order. Sorting floating-point numbers should produce the following behavior: -NaN < -Infinity < -value < -0 < 0 < value < Infinity < NaN . This aligns with the implementation of Java floating-point types comparisons. A data or delete file is associated with a sort order by the sort order\u2019s id within a manifest . Therefore, the table must declare all the sort orders for lookup. A table could also be configured with a default sort order id, indicating how the new data should be sorted by default. Writers should use this default sort order to sort the data on write, but are not required to if the default order is prohibitively expensive, as it would be for streaming writes.","title":"Sorting"},{"location":"spec/#manifests","text":"A manifest is an immutable Avro file that lists data files or delete files, along with each file\u2019s partition data tuple, metrics, and tracking information. One or more manifest files are used to store a snapshot , which tracks all of the files in a table at some point in time. Manifests are tracked by a manifest list for each table snapshot. A manifest is a valid Iceberg data file: files must use valid Iceberg formats, schemas, and column projection. A manifest may store either data files or delete files, but not both because manifests that contain delete files are scanned first during job planning. Whether a manifest is a data manifest or a delete manifest is stored in manifest metadata. A manifest stores files for a single partition spec. When a table\u2019s partition spec changes, old files remain in the older manifest and newer files are written to a new manifest. This is required because a manifest file\u2019s schema is based on its partition spec (see below). The partition spec of each manifest is also used to transform predicates on the table\u2019s data rows into predicates on partition values that are used during job planning to select files from a manifest. A manifest file must store the partition spec and other metadata as properties in the Avro file\u2019s key-value metadata: v1 v2 Key Value required required schema JSON representation of the table schema at the time the manifest was written optional required schema-id ID of the schema used to write the manifest as a string required required partition-spec JSON fields representation of the partition spec used to write the manifest optional required partition-spec-id ID of the partition spec used to write the manifest as a string optional required format-version Table format version number of the manifest as a string required content Type of content files tracked by the manifest: \u201cdata\u201d or \u201cdeletes\u201d The schema of a manifest file is a struct called manifest_entry with the following fields: v1 v2 Field id, name Type Description required required 0 status int with meaning: 0: EXISTING 1: ADDED 2: DELETED Used to track additions and deletions required optional 1 snapshot_id long Snapshot id where the file was added, or deleted if status is 2. Inherited when null. optional 3 sequence_number long Sequence number when the file was added. Inherited when null. required required 2 data_file data_file struct (see below) File path, partition tuple, metrics, \u2026 data_file is a struct with the following fields: v1 v2 Field id, name Type Description required 134 content int with meaning: 0: DATA , 1: POSITION DELETES , 2: EQUALITY DELETES Type of content stored by the data file: data, equality deletes, or position deletes (all v1 files are data files) required required 100 file_path string Full URI for the file with FS scheme required required 101 file_format string String file format name, avro, orc or parquet required required 102 partition struct<...> Partition data tuple, schema based on the partition spec output using partition field ids for the struct field ids required required 103 record_count long Number of records in this file required required 104 file_size_in_bytes long Total file size in bytes required 105 block_size_in_bytes long Deprecated. Always write a default in v1. Do not write in v2. optional 106 file_ordinal int Deprecated. Do not write. optional 107 sort_columns list<112: int> Deprecated. Do not write. optional optional 108 column_sizes map<117: int, 118: long> Map from column id to the total size on disk of all regions that store the column. Does not include bytes necessary to read other columns, like footers. Leave null for row-oriented formats (Avro) optional optional 109 value_counts map<119: int, 120: long> Map from column id to number of values in the column (including null and NaN values) optional optional 110 null_value_counts map<121: int, 122: long> Map from column id to number of null values in the column optional optional 137 nan_value_counts map<138: int, 139: long> Map from column id to number of NaN values in the column optional optional 111 distinct_counts map<123: int, 124: long> Map from column id to number of distinct values in the column; distinct counts must be derived using values in the file by counting or using sketches, but not using methods like merging existing distinct counts optional optional 125 lower_bounds map<126: int, 127: binary> Map from column id to lower bound in the column serialized as binary [1]. Each value must be less than or equal to all non-null, non-NaN values in the column for the file [2] optional optional 128 upper_bounds map<129: int, 130: binary> Map from column id to upper bound in the column serialized as binary [1]. Each value must be greater than or equal to all non-null, non-Nan values in the column for the file [2] optional optional 131 key_metadata binary Implementation-specific key metadata for encryption optional optional 132 split_offsets list<133: long> Split offsets for the data file. For example, all row group offsets in a Parquet file. Must be sorted ascending optional 135 equality_ids list<136: int> Field ids used to determine row equality in equality delete files. Required when content=2 and should be null otherwise. Fields with ids listed in this column must be present in the delete file optional optional 140 sort_order_id int ID representing sort order for this file [3]. Notes: Single-value serialization for lower and upper bounds is detailed in Appendix D. For float and double , the value -0.0 must precede +0.0 , as in the IEEE 754 totalOrder predicate. If sort order ID is missing or unknown, then the order is assumed to be unsorted. Only data files and equality delete files should be written with a non-null order id. Position deletes are required to be sorted by file and position, not a table order, and should set sort order id to null. Readers must ignore sort order id for position delete files. The partition struct stores the tuple of partition values for each file. Its type is derived from the partition fields of the partition spec used to write the manifest file. In v2, the partition struct\u2019s field ids must match the ids from the partition spec. The column metrics maps are used when filtering to select both data and delete files. For delete files, the metrics must store bounds and counts for all deleted rows, or must be omitted. Storing metrics for deleted rows ensures that the values can be used during job planning to find delete files that must be merged during a scan.","title":"Manifests"},{"location":"spec/#manifest-entry-fields","text":"The manifest entry fields are used to keep track of the snapshot in which files were added or logically deleted. The data_file struct is nested inside of the manifest entry so that it can be easily passed to job planning without the manifest entry fields. When a file is added to the dataset, it\u2019s manifest entry should store the snapshot ID in which the file was added and set status to 1 (added). When a file is replaced or deleted from the dataset, it\u2019s manifest entry fields store the snapshot ID in which the file was deleted and status 2 (deleted). The file may be deleted from the file system when the snapshot in which it was deleted is garbage collected, assuming that older snapshots have also been garbage collected [1]. Iceberg v2 adds a sequence number to the entry and makes the snapshot id optional. Both fields, sequence_number and snapshot_id , are inherited from manifest metadata when null . That is, if the field is null for an entry, then the entry must inherit its value from the manifest file\u2019s metadata, stored in the manifest list [2]. Notes: Technically, data files can be deleted when the last snapshot that contains the file as \u201clive\u201d data is garbage collected. But this is harder to detect and requires finding the diff of multiple snapshots. It is easier to track what files are deleted in a snapshot and delete them when that snapshot expires. Manifest list files are required in v2, so that the sequence_number and snapshot_id to inherit are always available.","title":"Manifest Entry Fields"},{"location":"spec/#sequence-number-inheritance","text":"Manifests track the sequence number when a data or delete file was added to the table. When adding new file, its sequence number is set to null because the snapshot\u2019s sequence number is not assigned until the snapshot is successfully committed. When reading, sequence numbers are inherited by replacing null with the manifest\u2019s sequence number from the manifest list. When writing an existing file to a new manifest, the sequence number must be non-null and set to the sequence number that was inherited. Inheriting sequence numbers through the metadata tree allows writing a new manifest without a known sequence number, so that a manifest can be written once and reused in commit retries. To change a sequence number for a retry, only the manifest list must be rewritten. When reading v1 manifests with no sequence number column, sequence numbers for all files must default to 0.","title":"Sequence Number Inheritance"},{"location":"spec/#snapshots","text":"A snapshot consists of the following fields: v1 v2 Field Description required required snapshot-id A unique long ID optional optional parent-snapshot-id The snapshot ID of the snapshot\u2019s parent. Omitted for any snapshot with no parent required sequence-number A monotonically increasing long that tracks the order of changes to a table required required timestamp-ms A timestamp when the snapshot was created, used for garbage collection and table inspection optional required manifest-list The location of a manifest list for this snapshot that tracks manifest files with additional meadata optional manifests A list of manifest file locations. Must be omitted if manifest-list is present optional required summary A string map that summarizes the snapshot changes, including operation (see below) optional optional schema-id ID of the table\u2019s current schema when the snapshot was created The snapshot summary\u2019s operation field is used by some operations, like snapshot expiration, to skip processing certain snapshots. Possible operation values are: append \u2013 Only data files were added and no files were removed. replace \u2013 Data and delete files were added and removed without changing table data; i.e., compaction, changing the data file format, or relocating data files. overwrite \u2013 Data and delete files were added and removed in a logical overwrite operation. delete \u2013 Data files were removed and their contents logically deleted and/or delete files were added to delete rows. Data and delete files for a snapshot can be stored in more than one manifest. This enables: Appends can add a new manifest to minimize the amount of data written, instead of adding new records by rewriting and appending to an existing manifest. (This is called a \u201cfast append\u201d.) Tables can use multiple partition specs. A table\u2019s partition configuration can evolve if, for example, its data volume changes. Each manifest uses a single partition spec, and queries do not need to change because partition filters are derived from data predicates. Large tables can be split across multiple manifests so that implementations can parallelize job planning or reduce the cost of rewriting a manifest. Manifests for a snapshot are tracked by a manifest list. Valid snapshots are stored as a list in table metadata. For serialization, see Appendix C.","title":"Snapshots"},{"location":"spec/#manifest-lists","text":"Snapshots are embedded in table metadata, but the list of manifests for a snapshot are stored in a separate manifest list file. A new manifest list is written for each attempt to commit a snapshot because the list of manifests always changes to produce a new snapshot. When a manifest list is written, the (optimistic) sequence number of the snapshot is written for all new manifest files tracked by the list. A manifest list includes summary metadata that can be used to avoid scanning all of the manifests in a snapshot when planning a table scan. This includes the number of added, existing, and deleted files, and a summary of values for each field of the partition spec used to write the manifest. A manifest list is a valid Iceberg data file: files must use valid Iceberg formats, schemas, and column projection. Manifest list files store manifest_file , a struct with the following fields: v1 v2 Field id, name Type Description required required 500 manifest_path string Location of the manifest file required required 501 manifest_length long Length of the manifest file required required 502 partition_spec_id int ID of a partition spec used to write the manifest; must be listed in table metadata partition-specs required 517 content int with meaning: 0: data , 1: deletes The type of files tracked by the manifest, either data or delete files; 0 for all v1 manifests required 515 sequence_number long The sequence number when the manifest was added to the table; use 0 when reading v1 manifest lists required 516 min_sequence_number long The minimum sequence number of all data or delete files in the manifest; use 0 when reading v1 manifest lists required required 503 added_snapshot_id long ID of the snapshot where the manifest file was added optional required 504 added_files_count int Number of entries in the manifest that have status ADDED (1), when null this is assumed to be non-zero optional required 505 existing_files_count int Number of entries in the manifest that have status EXISTING (0), when null this is assumed to be non-zero optional required 506 deleted_files_count int Number of entries in the manifest that have status DELETED (2), when null this is assumed to be non-zero optional required 512 added_rows_count long Number of rows in all of files in the manifest that have status ADDED , when null this is assumed to be non-zero optional required 513 existing_rows_count long Number of rows in all of files in the manifest that have status EXISTING , when null this is assumed to be non-zero optional required 514 deleted_rows_count long Number of rows in all of files in the manifest that have status DELETED , when null this is assumed to be non-zero optional optional 507 partitions list<508: field_summary> (see below) A list of field summaries for each partition field in the spec. Each field in the list corresponds to a field in the manifest file\u2019s partition spec. optional optional 519 key_metadata binary Implementation-specific key metadata for encryption field_summary is a struct with the following fields: v1 v2 Field id, name Type Description required required 509 contains_null boolean Whether the manifest contains at least one partition with a null value for the field optional optional 518 contains_nan boolean Whether the manifest contains at least one partition with a NaN value for the field optional optional 510 lower_bound bytes [1] Lower bound for the non-null, non-NaN values in the partition field, or null if all values are null or NaN [2] optional optional 511 upper_bound bytes [1] Upper bound for the non-null, non-NaN values in the partition field, or null if all values are null or NaN [2] Notes: Lower and upper bounds are serialized to bytes using the single-object serialization in Appendix D. The type of used to encode the value is the type of the partition field data. If -0.0 is a value of the partition field, the lower_bound must not be +0.0, and if +0.0 is a value of the partition field, the upper_bound must not be -0.0.","title":"Manifest Lists"},{"location":"spec/#scan-planning","text":"Scans are planned by reading the manifest files for the current snapshot. Deleted entries in data and delete manifests are not used in a scan. Manifests that contain no matching files, determined using either file counts or partition summaries, may be skipped. For each manifest, scan predicates, which filter data rows, are converted to partition predicates, which filter data and delete files. These partition predicates are used to select the data and delete files in the manifest. This conversion uses the partition spec used to write the manifest file. Scan predicates are converted to partition predicates using an inclusive projection : if a scan predicate matches a row, then the partition predicate must match that row\u2019s partition. This is called inclusive [1] because rows that do not match the scan predicate may be included in the scan by the partition predicate. For example, an events table with a timestamp column named ts that is partitioned by ts_day=day(ts) is queried by users with ranges over the timestamp column: ts > X . The inclusive projection is ts_day >= day(X) , which is used to select files that may have matching rows. Note that, in most cases, timestamps just before X will be included in the scan because the file contains rows that match the predicate and rows that do not match the predicate. Scan predicates are also used to filter data and delete files using column bounds and counts that are stored by field id in manifests. The same filter logic can be used for both data and delete files because both store metrics of the rows either inserted or deleted. If metrics show that a delete file has no rows that match a scan predicate, it may be ignored just as a data file would be ignored [2]. Data files that match the query filter must be read by the scan. Delete files that match the query filter must be applied to data files at read time, limited by the scope of the delete file using the following rules. A position delete file must be applied to a data file when all of the following are true: The data file\u2019s sequence number is less than or equal to the delete file\u2019s sequence number The data file\u2019s partition (both spec and partition values) is equal to the delete file\u2019s partition An equality delete file must be applied to a data file when all of the following are true: The data file\u2019s sequence number is strictly less than the delete\u2019s sequence number The data file\u2019s partition (both spec and partition values) is equal to the delete file\u2019s partition or the delete file\u2019s partition spec is unpartitioned In general, deletes are applied only to data files that are older and in the same partition, except for two special cases: Equality delete files stored with an unpartitioned spec are applied as global deletes. Otherwise, delete files do not apply to files in other partitions. Position delete files must be applied to data files from the same commit, when the data and delete file sequence numbers are equal. This allows deleting rows that were added in the same commit. Notes: An alternative, strict projection , creates a partition predicate that will match a file if all of the rows in the file must match the scan predicate. These projections are used to calculate the residual predicates for each file in a scan. For example, if file_a has rows with id between 1 and 10 and a delete file contains rows with id between 1 and 4, a scan for id = 9 may ignore the delete file because none of the deletes can match a row that will be selected.","title":"Scan Planning"},{"location":"spec/#table-metadata","text":"Table metadata is stored as JSON. Each table metadata change creates a new table metadata file that is committed by an atomic operation. This operation is used to ensure that a new version of table metadata replaces the version on which it was based. This produces a linear history of table versions and ensures that concurrent writes are not lost. The atomic operation used to commit metadata depends on how tables are tracked and is not standardized by this spec. See the sections below for examples.","title":"Table Metadata"},{"location":"spec/#table-metadata-fields","text":"Table metadata consists of the following fields: v1 v2 Field Description required required format-version An integer version number for the format. Currently, this can be 1 or 2 based on the spec. Implementations must throw an exception if a table\u2019s version is higher than the supported version. optional required table-uuid A UUID that identifies the table, generated when the table is created. Implementations must throw an exception if a table\u2019s UUID does not match the expected UUID after refreshing metadata. required required location The table\u2019s base location. This is used by writers to determine where to store data files, manifest files, and table metadata files. required last-sequence-number The table\u2019s highest assigned sequence number, a monotonically increasing long that tracks the order of snapshots in a table. required required last-updated-ms Timestamp in milliseconds from the unix epoch when the table was last updated. Each table metadata file should update this field just before writing. required required last-column-id An integer; the highest assigned column ID for the table. This is used to ensure columns are always assigned an unused ID when evolving schemas. required schema The table\u2019s current schema. ( Deprecated : use schemas and current-schema-id instead) optional required schemas A list of schemas, stored as objects with schema-id . optional required current-schema-id ID of the table\u2019s current schema. required partition-spec The table\u2019s current partition spec, stored as only fields. Note that this is used by writers to partition data, but is not used when reading because reads use the specs stored in manifest files. ( Deprecated : use partition-specs and default-spec-id instead) optional required partition-specs A list of partition specs, stored as full partition spec objects. optional required default-spec-id ID of the \u201ccurrent\u201d spec that writers should use by default. optional required last-partition-id An integer; the highest assigned partition field ID across all partition specs for the table. This is used to ensure partition fields are always assigned an unused ID when evolving specs. optional optional properties A string to string map of table properties. This is used to control settings that affect reading and writing and is not intended to be used for arbitrary metadata. For example, commit.retry.num-retries is used to control the number of commit retries. optional optional current-snapshot-id long ID of the current table snapshot. optional optional snapshots A list of valid snapshots. Valid snapshots are snapshots for which all data files exist in the file system. A data file must not be deleted from the file system until the last snapshot in which it was listed is garbage collected. optional optional snapshot-log A list (optional) of timestamp and snapshot ID pairs that encodes changes to the current snapshot for the table. Each time the current-snapshot-id is changed, a new entry should be added with the last-updated-ms and the new current-snapshot-id. When snapshots are expired from the list of valid snapshots, all entries before a snapshot that has expired should be removed. optional optional metadata-log A list (optional) of timestamp and metadata file location pairs that encodes changes to the previous metadata files for the table. Each time a new metadata file is created, a new entry of the previous metadata file location should be added to the list. Tables can be configured to remove oldest metadata log entries and keep a fixed-size log of the most recent entries after a commit. optional required sort-orders A list of sort orders, stored as full sort order objects. optional required default-sort-order-id Default sort order id of the table. Note that this could be used by writers, but is not used when reading because reads use the specs stored in manifest files. For serialization details, see Appendix C.","title":"Table Metadata Fields"},{"location":"spec/#commit-conflict-resolution-and-retry","text":"When two commits happen at the same time and are based on the same version, only one commit will succeed. In most cases, the failed commit can be applied to the new current version of table metadata and retried. Updates verify the conditions under which they can be applied to a new version and retry if those conditions are met. Append operations have no requirements and can always be applied. Replace operations must verify that the files that will be deleted are still in the table. Examples of replace operations include format changes (replace an Avro file with a Parquet file) and compactions (several files are replaced with a single file that contains the same rows). Delete operations must verify that specific files to delete are still in the table. Delete operations based on expressions can always be applied (e.g., where timestamp < X). Table schema updates and partition spec changes must validate that the schema has not changed between the base version and the current version.","title":"Commit Conflict Resolution and Retry"},{"location":"spec/#file-system-tables","text":"An atomic swap can be implemented using atomic rename in file systems that support it, like HDFS or most local file systems [1]. Each version of table metadata is stored in a metadata folder under the table\u2019s base location using a file naming scheme that includes a version number, V : v<V>.metadata.json . To commit a new metadata version, V+1 , the writer performs the following steps: Read the current table metadata version V . Create new table metadata based on version V . Write the new table metadata to a unique file: <random-uuid>.metadata.json . Rename the unique file to the well-known file for version V : v<V+1>.metadata.json . If the rename succeeds, the commit succeeded and V+1 is the table\u2019s current version If the rename fails, go back to step 1. Notes: The file system table scheme is implemented in HadoopTableOperations .","title":"File System Tables"},{"location":"spec/#metastore-tables","text":"The atomic swap needed to commit new versions of table metadata can be implemented by storing a pointer in a metastore or database that is updated with a check-and-put operation [1]. The check-and-put validates that the version of the table that a write is based on is still current and then makes the new metadata from the write the current version. Each version of table metadata is stored in a metadata folder under the table\u2019s base location using a naming scheme that includes a version and UUID: <V>-<uuid>.metadata.json . To commit a new metadata version, V+1 , the writer performs the following steps: Create a new table metadata file based on the current metadata. Write the new table metadata to a unique file: <V+1>-<uuid>.metadata.json . Request that the metastore swap the table\u2019s metadata pointer from the location of V to the location of V+1 . If the swap succeeds, the commit succeeded. V was still the latest metadata version and the metadata file for V+1 is now the current metadata. If the swap fails, another writer has already created V+1 . The current writer goes back to step 1. Notes: The metastore table scheme is partly implemented in BaseMetastoreTableOperations .","title":"Metastore Tables"},{"location":"spec/#delete-formats","text":"This section details how to encode row-level deletes in Iceberg delete files. Row-level deletes are not supported in v1. Row-level delete files are valid Iceberg data files: files must use valid Iceberg formats, schemas, and column projection. It is recommended that delete files are written using the table\u2019s default file format. Row-level delete files are tracked by manifests, like data files. A separate set of manifests is used for delete files, but the manifest schemas are identical. Both position and equality deletes allow encoding deleted row values with a delete. This can be used to reconstruct a stream of changes to a table.","title":"Delete Formats"},{"location":"spec/#position-delete-files","text":"Position-based delete files identify deleted rows by file and position in one or more data files, and may optionally contain the deleted row. A data row is deleted if there is an entry in a position delete file for the row\u2019s file and position in the data file, starting at 0. Position-based delete files store file_position_delete , a struct with the following fields: Field id, name Type Description 2147483546 file_path string Full URI of a data file with FS scheme. This must match the file_path of the target data file in a manifest entry 2147483545 pos long Ordinal position of a deleted row in the target data file identified by file_path , starting at 0 2147483544 row required struct<...> [1] Deleted row values. Omit the column when not storing deleted rows. When present in the delete file, row is required because all delete entries must include the row values. When the deleted row column is present, its schema may be any subset of the table schema and must use field ids matching the table. To ensure the accuracy of statistics, all delete entries must include row values, or the column must be omitted (this is why the column type is required ). The rows in the delete file must be sorted by file_path then position to optimize filtering rows while scanning. Sorting by file_path allows filter pushdown by file in columnar storage formats. Sorting by position allows filtering rows while scanning, to avoid keeping deletes in memory.","title":"Position Delete Files"},{"location":"spec/#equality-delete-files","text":"Equality delete files identify deleted rows in a collection of data files by one or more column values, and may optionally contain additional columns of the deleted row. Equality delete files store any subset of a table\u2019s columns and use the table\u2019s field ids. The delete columns are the columns of the delete file used to match data rows. Delete columns are identified by id in the delete file metadata column equality_ids . Float and double columns cannot be used as delete columns in equality delete files. A data row is deleted if its values are equal to all delete columns for any row in an equality delete file that applies to the row\u2019s data file (see Scan Planning ). Each row of the delete file produces one equality predicate that matches any row where the delete columns are equal. Multiple columns can be thought of as an AND of equality predicates. A null value in a delete column matches a row if the row\u2019s value is null , equivalent to col IS NULL . For example, a table with the following data: 1: id | 2: category | 3: name -------|-------------|--------- 1 | marsupial | Koala 2 | toy | Teddy 3 | NULL | Grizzly 4 | NULL | Polar The delete id = 3 could be written as either of the following equality delete files: equality_ids=[1] 1: id ------- 3 equality_ids=[1] 1: id | 2: category | 3: name -------|-------------|--------- 3 | NULL | Grizzly The delete id = 4 AND category IS NULL could be written as the following equality delete file: equality_ids=[1, 2] 1: id | 2: category | 3: name -------|-------------|--------- 4 | NULL | Polar If a delete column in an equality delete file is later dropped from the table, it must still be used when applying the equality deletes. If a column was added to a table and later used as a delete column in an equality delete file, the column value is read for older data files using normal projection rules (defaults to null ).","title":"Equality Delete Files"},{"location":"spec/#delete-file-stats","text":"Manifests hold the same statistics for delete files and data files. For delete files, the metrics describe the values that were deleted.","title":"Delete File Stats"},{"location":"spec/#appendix-a-format-specific-requirements","text":"","title":"Appendix A: Format-specific Requirements"},{"location":"spec/#avro","text":"Data Type Mappings Values should be stored in Avro using the Avro types and logical type annotations in the table below. Optional fields, array elements, and map values must be wrapped in an Avro union with null . This is the only union type allowed in Iceberg data files. Optional fields must always set the Avro field default value to null. Maps with non-string keys must use an array representation with the map logical type. The array representation or Avro\u2019s map type may be used for maps with string keys. Type Avro type Notes boolean boolean int int long long float float double double decimal(P,S) { \"type\": \"fixed\", \"size\": minBytesRequired(P), \"logicalType\": \"decimal\", \"precision\": P, \"scale\": S } Stored as fixed using the minimum number of bytes for the given precision. date { \"type\": \"int\", \"logicalType\": \"date\" } Stores days from the 1970-01-01. time { \"type\": \"long\", \"logicalType\": \"time-micros\" } Stores microseconds from midnight. timestamp { \"type\": \"long\", \"logicalType\": \"timestamp-micros\", \"adjust-to-utc\": false } Stores microseconds from 1970-01-01 00:00:00.000000. timestamptz { \"type\": \"long\", \"logicalType\": \"timestamp-micros\", \"adjust-to-utc\": true } Stores microseconds from 1970-01-01 00:00:00.000000 UTC. string string uuid { \"type\": \"fixed\", \"size\": 16, \"logicalType\": \"uuid\" } fixed(L) { \"type\": \"fixed\", \"size\": L } binary bytes struct record list array map array of key-value records, or map when keys are strings (optional). Array storage must use logical type name map and must store elements that are 2-field records. The first field is a non-null key and the second field is the value. Field IDs Iceberg struct, list, and map types identify nested types by ID. When writing data to Avro files, these IDs must be stored in the Avro schema to support ID-based column pruning. IDs are stored as JSON integers in the following locations: ID Avro schema location Property Example Struct field Record field object field-id { \"type\": \"record\", ... \"fields\": [ { \"name\": \"l\", \"type\": [\"null\", \"long\"], \"default\": null, \"field-id\": 8 } ] } List element Array schema object element-id { \"type\": \"array\", \"items\": \"int\", \"element-id\": 9 } String map key Map schema object key-id { \"type\": \"map\", \"values\": \"int\", \"key-id\": 10, \"value-id\": 11 } String map value Map schema object value-id Map key, value Key, value fields in the element record. field-id { \"type\": \"array\", \"logicalType\": \"map\", \"items\": { \"type\": \"record\", \"name\": \"k12_v13\", \"fields\": [ { \"name\": \"key\", \"type\": \"int\", \"field-id\": 12 }, { \"name\": \"value\", \"type\": \"string\", \"field-id\": 13 } ] } } Note that the string map case is for maps where the key type is a string. Using Avro\u2019s map type in this case is optional. Maps with string keys may be stored as arrays.","title":"Avro"},{"location":"spec/#parquet","text":"Data Type Mappings Values should be stored in Parquet using the types and logical type annotations in the table below. Column IDs are required. Lists must use the 3-level representation . Type Parquet physical type Logical type Notes boolean boolean int int long long float float double double decimal(P,S) P <= 9 : int32 , P <= 18 : int64 , fixed otherwise DECIMAL(P,S) Fixed must use the minimum number of bytes that can store P . date int32 DATE Stores days from the 1970-01-01. time int64 TIME_MICROS with adjustToUtc=false Stores microseconds from midnight. timestamp int64 TIMESTAMP_MICROS with adjustToUtc=false Stores microseconds from 1970-01-01 00:00:00.000000. timestamptz int64 TIMESTAMP_MICROS with adjustToUtc=true Stores microseconds from 1970-01-01 00:00:00.000000 UTC. string binary UTF8 Encoding must be UTF-8. uuid fixed_len_byte_array[16] UUID fixed(L) fixed_len_byte_array[L] binary binary struct group list 3-level list LIST See Parquet docs for 3-level representation. map 3-level map MAP See Parquet docs for 3-level representation.","title":"Parquet"},{"location":"spec/#orc","text":"Data Type Mappings Type ORC type ORC type attributes Notes boolean boolean int int ORC tinyint and smallint would also map to int . long long float float double double decimal(P,S) decimal date date time long iceberg.long-type = TIME Stores microseconds from midnight. timestamp timestamp [1] timestamptz timestamp_instant [1] string string ORC varchar and char would also map to string . uuid binary iceberg.binary-type = UUID fixed(L) binary iceberg.binary-type = FIXED & iceberg.length = L The length would not be checked by the ORC reader and should be checked by the adapter. binary binary struct struct list array map map Notes: ORC\u2019s TimestampColumnVector comprises of a time field (milliseconds since epoch) and a nanos field (nanoseconds within the second). Hence the milliseconds within the second are reported twice; once in the time field and again in the nanos field. The read adapter should only use milliseconds within the second from one of these fields. The write adapter should also report milliseconds within the second twice; once in the time field and again in the nanos field. ORC writer is expected to correctly consider millis information from one of the fields. More details at https://issues.apache.org/jira/browse/ORC-546 One of the interesting challenges with this is how to map Iceberg\u2019s schema evolution (id based) on to ORC\u2019s (name based). In theory, we could use Iceberg\u2019s column ids as the column and field names, but that would suck from a user\u2019s point of view. The column IDs must be stored in ORC type attributes using the key iceberg.id , and iceberg.required to store \"true\" if the Iceberg column is required, otherwise it will be optional. Iceberg would build the desired reader schema with their schema evolution rules and pass that down to the ORC reader, which would then use its schema evolution to map that to the writer\u2019s schema. Basically, Iceberg would need to change the names of columns and fields to get the desired mapping. Iceberg writer ORC writer Iceberg reader ORC reader struct<a (1): int, b (2): string> struct<a: int, b: string> struct<a (2): string, c (3): date> struct<b: string, c: date> struct<a (1): struct<b (2): string, c (3): date>> struct<a: struct<b:string, c:date>> struct<aa (1): struct<cc (3): date, bb (2): string>> struct<a: struct<c:date, b:string>>","title":"ORC"},{"location":"spec/#appendix-b-32-bit-hash-requirements","text":"The 32-bit hash implementation is 32-bit Murmur3 hash, x86 variant, seeded with 0. Primitive type Hash specification Test value int hashLong(long(v)) [1] 34 \uffeb 2017239379 long hashBytes(littleEndianBytes(v)) 34L \uffeb 2017239379 decimal(P,S) hashBytes(minBigEndian(unscaled(v))) [2] 14.20 \uffeb -500754589 date hashInt(daysFromUnixEpoch(v)) 2017-11-16 \uffeb -653330422 time hashLong(microsecsFromMidnight(v)) 22:31:08 \uffeb -662762989 timestamp hashLong(microsecsFromUnixEpoch(v)) 2017-11-16T22:31:08 \uffeb -2047944441 timestamptz hashLong(microsecsFromUnixEpoch(v)) 2017-11-16T14:31:08-08:00 \uffeb -2047944441 string hashBytes(utf8Bytes(v)) iceberg \uffeb 1210000089 uuid hashBytes(uuidBytes(v)) [3] f79c3e09-677c-4bbd-a479-3f349cb785e7 \uffeb 1488055340 fixed(L) hashBytes(v) 00 01 02 03 \uffeb -188683207 binary hashBytes(v) 00 01 02 03 \uffeb -188683207 The types below are not currently valid for bucketing, and so are not hashed. However, if that changes and a hash value is needed, the following table shall apply: Primitive type Hash specification Test value boolean false: hashInt(0) , true: hashInt(1) true \uffeb 1392991556 float hashDouble(double(v)) [4] 1.0F \uffeb -142385009 double hashLong(doubleToLongBits(v)) 1.0D \uffeb -142385009 Notes: Integer and long hash results must be identical for all integer values. This ensures that schema evolution does not change bucket partition values if integer types are promoted. Decimal values are hashed using the minimum number of bytes required to hold the unscaled value as a two\u2019s complement big-endian; this representation does not include padding bytes required for storage in a fixed-length array. Hash results are not dependent on decimal scale, which is part of the type, not the data value. UUIDs are encoded using big endian. The test UUID for the example above is: f79c3e09-677c-4bbd-a479-3f349cb785e7 . This UUID encoded as a byte array is: F7 9C 3E 09 67 7C 4B BD A4 79 3F 34 9C B7 85 E7 Float hash values are the result of hashing the float cast to double to ensure that schema evolution does not change hash values if float types are promoted.","title":"Appendix B: 32-bit Hash Requirements"},{"location":"spec/#appendix-c-json-serialization","text":"","title":"Appendix C: JSON serialization"},{"location":"spec/#schemas","text":"Schemas are serialized as a JSON object with the same fields as a struct in the table below, and the following additional fields: v1 v2 Field JSON representation Example optional required schema-id JSON int 0 optional optional identifier-field-ids JSON list of ints [1, 2] Types are serialized according to this table: Type JSON representation Example boolean JSON string: \"boolean\" \"boolean\" int JSON string: \"int\" \"int\" long JSON string: \"long\" \"long\" float JSON string: \"float\" \"float\" double JSON string: \"double\" \"double\" date JSON string: \"date\" \"date\" time JSON string: \"time\" \"time\" timestamp without zone JSON string: \"timestamp\" \"timestamp\" timestamp with zone JSON string: \"timestamptz\" \"timestamptz\" string JSON string: \"string\" \"string\" uuid JSON string: \"uuid\" \"uuid\" fixed(L) JSON string: \"fixed[<L>]\" \"fixed[16]\" binary JSON string: \"binary\" \"binary\" decimal(P, S) JSON string: \"decimal(<P>,<S>)\" \"decimal(9,2)\" , \"decimal(9, 2)\" struct JSON object: { \"type\": \"struct\", \"fields\": [ { \"id\": <field id int>, \"name\": <name string>, \"required\": <boolean>, \"type\": <type JSON>, \"doc\": <comment string> }, ... ] } { \"type\": \"struct\", \"fields\": [ { \"id\": 1, \"name\": \"id\", \"required\": true, \"type\": \"uuid\" }, { \"id\": 2, \"name\": \"data\", \"required\": false, \"type\": { \"type\": \"list\", ... } } ] } list JSON object: { \"type\": \"list\", \"element-id\": <id int>, \"element-required\": <bool> \"element\": <type JSON> } { \"type\": \"list\", \"element-id\": 3, \"element-required\": true, \"element\": \"string\" } map JSON object: { \"type\": \"map\", \"key-id\": <key id int>, \"key\": <type JSON>, \"value-id\": <val id int>, \"value-required\": <bool> \"value\": <type JSON> } { \"type\": \"map\", \"key-id\": 4, \"key\": \"string\", \"value-id\": 5, \"value-required\": false, \"value\": \"double\" }","title":"Schemas"},{"location":"spec/#partition-specs","text":"Partition specs are serialized as a JSON object with the following fields: Field JSON representation Example spec-id JSON int 0 fields JSON list: [ <partition field JSON>, ... ] [ { \"source-id\": 4, \"field-id\": 1000, \"name\": \"ts_day\", \"transform\": \"day\" }, { \"source-id\": 1, \"field-id\": 1001, \"name\": \"id_bucket\", \"transform\": \"bucket[16]\" } ] Each partition field in the fields list is stored as an object. See the table for more detail: Transform or Field JSON representation Example identity JSON string: \"identity\" \"identity\" bucket[N] JSON string: \"bucket<N>]\" \"bucket[16]\" truncate[W] JSON string: \"truncate[<W>]\" \"truncate[20]\" year JSON string: \"year\" \"year\" month JSON string: \"month\" \"month\" day JSON string: \"day\" \"day\" hour JSON string: \"hour\" \"hour\" Partition Field JSON object: { \"source-id\": <id int>, \"field-id\": <field id int>, \"name\": <name string>, \"transform\": <transform JSON> } { \"source-id\": 1, \"field-id\": 1000, \"name\": \"id_bucket\", \"transform\": \"bucket[16]\" } In some cases partition specs are stored using only the field list instead of the object format that includes the spec ID, like the deprecated partition-spec field in table metadata. The object format should be used unless otherwise noted in this spec. The field-id property was added for each partition field in v2. In v1, the reference implementation assigned field ids sequentially in each spec starting at 1,000. See Partition Evolution for more details.","title":"Partition Specs"},{"location":"spec/#sort-orders","text":"Sort orders are serialized as a list of JSON object, each of which contains the following fields: Field JSON representation Example order-id JSON int 1 fields JSON list: [ <sort field JSON>, ... ] [ { \"transform\": \"identity\", \"source-id\": 2, \"direction\": \"asc\", \"null-order\": \"nulls-first\" }, { \"transform\": \"bucket[4]\", \"source-id\": 3, \"direction\": \"desc\", \"null-order\": \"nulls-last\" } ] Each sort field in the fields list is stored as an object with the following properties: Field JSON representation Example Sort Field JSON object: { \"transform\": <transform JSON>, \"source-id\": <source id int>, \"direction\": <direction string>, \"null-order\": <null-order string> } { \"transform\": \"bucket[4]\", \"source-id\": 3, \"direction\": \"desc\", \"null-order\": \"nulls-last\" } The following table describes the possible values for the some of the field within sort field: Field JSON representation Possible values direction JSON string \"asc\", \"desc\" null-order JSON string \"nulls-first\", \"nulls-last\"","title":"Sort Orders"},{"location":"spec/#table-metadata-and-snapshots","text":"Table metadata is serialized as a JSON object according to the following table. Snapshots are not serialized separately. Instead, they are stored in the table metadata JSON. Metadata field JSON representation Example format-version JSON int 1 table-uuid JSON string \"fb072c92-a02b-11e9-ae9c-1bb7bc9eca94\" location JSON string \"s3://b/wh/data.db/table\" last-updated-ms JSON long 1515100955770 last-column-id JSON int 22 schema JSON schema (object) See above, read schemas instead schemas JSON schemas (list of objects) See above current-schema-id JSON int 0 partition-spec JSON partition fields (list) See above, read partition-specs instead partition-specs JSON partition specs (list of objects) See above default-spec-id JSON int 0 last-partition-id JSON int 1000 properties JSON object: { \"<key>\": \"<val>\", ... } { \"write.format.default\": \"avro\", \"commit.retry.num-retries\": \"4\" } current-snapshot-id JSON long 3051729675574597004 snapshots JSON list of objects: [ { \"snapshot-id\": <id>, \"timestamp-ms\": <timestamp-in-ms>, \"summary\": { \"operation\": <operation>, ... }, \"manifest-list\": \"<location>\", \"schema-id\": \"<id>\" }, ... ] [ { \"snapshot-id\": 3051729675574597004, \"timestamp-ms\": 1515100955770, \"summary\": { \"operation\": \"append\" }, \"manifest-list\": \"s3://b/wh/.../s1.avro\" \"schema-id\": 0 } ] snapshot-log JSON list of objects: [ { \"snapshot-id\": , \"timestamp-ms\": }, ... ] [ { \"snapshot-id\": 30517296..., \"timestamp-ms\": 1515100... } ] metadata-log JSON list of objects: [ { \"metadata-file\": , \"timestamp-ms\": }, ... ] [ { \"metadata-file\": \"s3://bucket/.../v1.json\", \"timestamp-ms\": 1515100... } ] sort-orders JSON sort orders (list of sort field object) See above default-sort-order-id JSON int 0","title":"Table Metadata and Snapshots"},{"location":"spec/#appendix-d-single-value-serialization","text":"This serialization scheme is for storing single values as individual binary values in the lower and upper bounds maps of manifest files. Type Binary serialization boolean 0x00 for false, non-zero byte for true int Stored as 4-byte little-endian long Stored as 8-byte little-endian float Stored as 4-byte little-endian double Stored as 8-byte little-endian date Stores days from the 1970-01-01 in an 4-byte little-endian int time Stores microseconds from midnight in an 8-byte little-endian long timestamp without zone Stores microseconds from 1970-01-01 00:00:00.000000 in an 8-byte little-endian long timestamp with zone Stores microseconds from 1970-01-01 00:00:00.000000 UTC in an 8-byte little-endian long string UTF-8 bytes (without length) uuid 16-byte big-endian value, see example in Appendix B fixed(L) Binary value binary Binary value (without length) decimal(P, S) Stores unscaled value as two\u2019s-complement big-endian binary, using the minimum number of bytes for the value struct Not supported list Not supported map Not supported","title":"Appendix D: Single-value serialization"},{"location":"spec/#appendix-e-format-version-changes","text":"","title":"Appendix E: Format version changes"},{"location":"spec/#version-2","text":"Writing v1 metadata: Table metadata field last-sequence-number should not be written Snapshot field sequence-number should not be written Manifest list field sequence-number should not be written Manifest list field min-sequence-number should not be written Manifest list field content must be 0 (data) or omitted Manifest entry field sequence_number should not be written Data file field content must be 0 (data) or omitted Reading v1 metadata for v2: Table metadata field last-sequence-number must default to 0 Snapshot field sequence-number must default to 0 Manifest list field sequence-number must default to 0 Manifest list field min-sequence-number must default to 0 Manifest list field content must default to 0 (data) Manifest entry field sequence_number must default to 0 Data file field content must default to 0 (data) Writing v2 metadata: Table metadata JSON: last-sequence-number was added and is required; default to 0 when reading v1 metadata table-uuid is now required current-schema-id is now required schemas is now required partition-specs is now required default-spec-id is now required last-partition-id is now required sort-orders is now required default-sort-order-id is now required schema is no longer required and should be omitted; use schemas and current-schema-id instead partition-spec is no longer required and should be omitted; use partition-specs and default-spec-id instead Snapshot JSON: sequence-number was added and is required; default to 0 when reading v1 metadata manifest-list is now required manifests is no longer required and should be omitted; always use manifest-list instead Manifest list manifest_file : content was added and is required; 0=data, 1=deletes; default to 0 when reading v1 manifest lists sequence_number was added and is required min_sequence_number was added and is required added_files_count is now required existing_files_count is now required deleted_files_count is now required added_rows_count is now required existing_rows_count is now required deleted_rows_count is now required Manifest key-value metadata: schema-id is now required partition-spec-id is now required format-version is now required content was added and is required (must be \u201cdata\u201d or \u201cdeletes\u201d) Manifest manifest_entry : snapshot_id is now optional to support inheritance sequence_number was added and is optional, to support inheritance Manifest data_file : content was added and is required; 0=data, 1=position deletes, 2=equality deletes; default to 0 when reading v1 manifests equality_ids was added, to be used for equality deletes only block_size_in_bytes was removed (breaks v1 reader compatibility) file_ordinal was removed sort_columns was removed Note that these requirements apply when writing data to a v2 table. Tables that are upgraded from v1 may contain metadata that does not follow these requirements. Implementations should remain backward-compatible with v1 metadata requirements.","title":"Version 2"},{"location":"terms/","text":"Terms \u00b6 Snapshot \u00b6 A snapshot is the state of a table at some time. Each snapshot lists all of the data files that make up the table\u2019s contents at the time of the snapshot. Data files are stored across multiple manifest files, and the manifests for a snapshot are listed in a single manifest list file. Manifest list \u00b6 A manifest list is a metadata file that lists the manifests that make up a table snapshot. Each manifest file in the manifest list is stored with information about its contents, like partition value ranges, used to speed up metadata operations. Manifest file \u00b6 A manifest file is a metadata file that lists a subset of data files that make up a snapshot. Each data file in a manifest is stored with a partition tuple , column-level stats, and summary information used to prune splits during scan planning . Partition spec \u00b6 A partition spec is a description of how to partition data in a table. A spec consists of a list of source columns and transforms. A transform produces a partition value from a source value. For example, date(ts) produces the date associated with a timestamp column named ts . Partition tuple \u00b6 A partition tuple is a tuple or struct of partition data stored with each data file. All values in a partition tuple are the same for all rows stored in a data file. Partition tuples are produced by transforming values from row data using a partition spec. Iceberg stores partition values unmodified, unlike Hive tables that convert values to and from strings in file system paths and keys. Snapshot log (history table) \u00b6 The snapshot log is a metadata log of how the table\u2019s current snapshot has changed over time. The log is a list of timestamp and ID pairs: when the current snapshot changed and the snapshot ID the current snapshot was changed to. The snapshot log is stored in table metadata as snapshot-log .","title":"Definitions"},{"location":"terms/#terms","text":"","title":"Terms"},{"location":"terms/#snapshot","text":"A snapshot is the state of a table at some time. Each snapshot lists all of the data files that make up the table\u2019s contents at the time of the snapshot. Data files are stored across multiple manifest files, and the manifests for a snapshot are listed in a single manifest list file.","title":"Snapshot"},{"location":"terms/#manifest-list","text":"A manifest list is a metadata file that lists the manifests that make up a table snapshot. Each manifest file in the manifest list is stored with information about its contents, like partition value ranges, used to speed up metadata operations.","title":"Manifest list"},{"location":"terms/#manifest-file","text":"A manifest file is a metadata file that lists a subset of data files that make up a snapshot. Each data file in a manifest is stored with a partition tuple , column-level stats, and summary information used to prune splits during scan planning .","title":"Manifest file"},{"location":"terms/#partition-spec","text":"A partition spec is a description of how to partition data in a table. A spec consists of a list of source columns and transforms. A transform produces a partition value from a source value. For example, date(ts) produces the date associated with a timestamp column named ts .","title":"Partition spec"},{"location":"terms/#partition-tuple","text":"A partition tuple is a tuple or struct of partition data stored with each data file. All values in a partition tuple are the same for all rows stored in a data file. Partition tuples are produced by transforming values from row data using a partition spec. Iceberg stores partition values unmodified, unlike Hive tables that convert values to and from strings in file system paths and keys.","title":"Partition tuple"},{"location":"terms/#snapshot-log-history-table","text":"The snapshot log is a metadata log of how the table\u2019s current snapshot has changed over time. The log is a list of timestamp and ID pairs: when the current snapshot changed and the snapshot ID the current snapshot was changed to. The snapshot log is stored in table metadata as snapshot-log .","title":"Snapshot log (history table)"},{"location":"trademarks/","text":"Trademarks \u00b6 Apache Iceberg, Iceberg, Apache, the Apache feather logo, and the Apache Iceberg project logo are either registered trademarks or trademarks of The Apache Software Foundation in the United States and other countries.","title":"Trademarks"},{"location":"trademarks/#trademarks","text":"Apache Iceberg, Iceberg, Apache, the Apache feather logo, and the Apache Iceberg project logo are either registered trademarks or trademarks of The Apache Software Foundation in the United States and other countries.","title":"Trademarks"},{"location":"trino-prestodb/","text":"Trino (formerly Presto SQL) \u00b6 The Iceberg connector is available with PrestoSQL, which is rebranded as Trino since January 2021. This page is kept for backwards compatibility only. The documentation for Iceberg\u2019s Trino connector has moved to the Trino website. Please consult the Iceberg section of the Trino docs for more information. PrestoDB \u00b6 The Presto trademark is now exclusively reserved for PrestoDB. The documentation for Iceberg\u2019s PrestoDB connector is in the PrestoDB website. Please consult the Iceberg section of the PrestoDB docs for more information.","title":"Trino prestodb"},{"location":"trino-prestodb/#trino-formerly-presto-sql","text":"The Iceberg connector is available with PrestoSQL, which is rebranded as Trino since January 2021. This page is kept for backwards compatibility only. The documentation for Iceberg\u2019s Trino connector has moved to the Trino website. Please consult the Iceberg section of the Trino docs for more information.","title":"Trino (formerly Presto SQL)"},{"location":"trino-prestodb/#prestodb","text":"The Presto trademark is now exclusively reserved for PrestoDB. The documentation for Iceberg\u2019s PrestoDB connector is in the PrestoDB website. Please consult the Iceberg section of the PrestoDB docs for more information.","title":"PrestoDB"},{"location":"why-iceberg/","text":"Why Iceberg? \u00b6 Iceberg was created because no other table format for provides a complete solution for SQL tables. Some improve atomicity guarantees, but no other format delivers both reliable operations and behavior that data engineers need. Iceberg tracks individual data files in a table instead of directories. This allows writers to create data files in-place and only adds files to the table in an explicit commit. Table state is maintained in metadata files. All changes to table state create a new metadata file and replace the old metadata with an atomic operation. The table metadata file tracks the table schema, partitioning config, other properties, and snapshots of the table contents. The atomic transitions from one table metadata file to the next provide snapshot isolation. Readers use the latest table state (snapshot) that was current when they load the table metadata and are not affected by changes until they refresh and pick up a new metadata location. A snapshot is a complete set of data files in the table at some point in time. Snapshots are listed in the metadata file, but the files in a snapshot are stored across a separate set of manifest files. Data files in snapshots are stored in one or more manifest files that contain a row for each data file in the table, its partition data, and its metrics. A snapshot is the union of all files in its manifests. Manifest files can be shared between snapshots to avoid rewriting metadata that is slow-changing. Goals \u00b6 This also provides improved guarantees and performance: Snapshot isolation : Readers always use a consistent snapshot of the table, without needing to hold a lock. All table updates are atomic. O(1) RPCs to plan : Instead of listing O(n) directories in a table to plan a job, reading a snapshot requires O(1) RPC calls. Distributed planning : File pruning and predicate push-down is distributed to jobs, removing the metastore as a bottleneck. Version history and rollback : Table snapshots are kept as history and tables can roll back if a job produces bad data. Finer granularity partitioning : Distributed planning and O(1) RPC calls remove the current barriers to finer-grained partitioning. Safe file-level operations . By supporting atomic changes, Iceberg enables new use cases, like safely compacting small files and safely appending late data to tables. Why a new table format? \u00b6 The central metastore can be a scale bottleneck and the file system doesn\u2019t\u2014and shouldn\u2019t\u2014provide transactions to isolate concurrent reads and writes. There are several problems with the current format: There is no specification : Implementations don\u2019t handle all cases consistently. For example, bucketing in Hive and Spark use different hash functions and are not compatible. Hive uses a locking scheme to make cross-partition changes safe, but no other implementations use it. Table data is tracked in two stores : a central metastore, for partitions, and the file system, for files. This makes atomic changes to a table\u2019s contents impossible. It also requires job planning to make listing calls that are O(n) with the number of partitions. These listing calls are expensive, especially when using a remote object store. Operations depend on file rename : Most output committers depend on rename operations to implement guarantees and reduce the amount of time tables only have partial data from a write. But rename is not a metadata-only operation in S3 and will copy data. The new S3 committers that use multipart upload make this better, but can\u2019t entirely solve the problem and put a lot of load on the file system during job commit. The current format\u2019s dependence on listing and rename cannot be changed, so a new format is needed. Other design goals \u00b6 In addition to changes in how table contents are tracked, Iceberg\u2019s design improves a few other areas: Reliable types : Iceberg provides a core set of types, tested to work consistently across all of the supported data formats. Types include date, timestamp, and decimal, as well as nested combinations of map, list, and struct. Schema evolution : Columns are tracked by ID to fully support add, drop, and rename across all columns and nested fields. Hidden partitioning : Partitioning is built into Iceberg as table configuration; it can plan efficient queries without extra partition predicates. Stats-based split filtering : Stats for the columns in each data file are used to eliminate splits before creating tasks, boosting performance for highly selective queries. Metrics : The format includes cost-based optimization metrics stored with data files for better job planning. Unmodified partition data : The Hive layout stores partition data escaped in strings. Iceberg stores partition data without modification. Portable spec : Tables are not tied to Java. Iceberg has a clear specification for other implementations.","title":"Why iceberg"},{"location":"why-iceberg/#why-iceberg","text":"Iceberg was created because no other table format for provides a complete solution for SQL tables. Some improve atomicity guarantees, but no other format delivers both reliable operations and behavior that data engineers need. Iceberg tracks individual data files in a table instead of directories. This allows writers to create data files in-place and only adds files to the table in an explicit commit. Table state is maintained in metadata files. All changes to table state create a new metadata file and replace the old metadata with an atomic operation. The table metadata file tracks the table schema, partitioning config, other properties, and snapshots of the table contents. The atomic transitions from one table metadata file to the next provide snapshot isolation. Readers use the latest table state (snapshot) that was current when they load the table metadata and are not affected by changes until they refresh and pick up a new metadata location. A snapshot is a complete set of data files in the table at some point in time. Snapshots are listed in the metadata file, but the files in a snapshot are stored across a separate set of manifest files. Data files in snapshots are stored in one or more manifest files that contain a row for each data file in the table, its partition data, and its metrics. A snapshot is the union of all files in its manifests. Manifest files can be shared between snapshots to avoid rewriting metadata that is slow-changing.","title":"Why Iceberg?"},{"location":"why-iceberg/#goals","text":"This also provides improved guarantees and performance: Snapshot isolation : Readers always use a consistent snapshot of the table, without needing to hold a lock. All table updates are atomic. O(1) RPCs to plan : Instead of listing O(n) directories in a table to plan a job, reading a snapshot requires O(1) RPC calls. Distributed planning : File pruning and predicate push-down is distributed to jobs, removing the metastore as a bottleneck. Version history and rollback : Table snapshots are kept as history and tables can roll back if a job produces bad data. Finer granularity partitioning : Distributed planning and O(1) RPC calls remove the current barriers to finer-grained partitioning. Safe file-level operations . By supporting atomic changes, Iceberg enables new use cases, like safely compacting small files and safely appending late data to tables.","title":"Goals"},{"location":"why-iceberg/#why-a-new-table-format","text":"The central metastore can be a scale bottleneck and the file system doesn\u2019t\u2014and shouldn\u2019t\u2014provide transactions to isolate concurrent reads and writes. There are several problems with the current format: There is no specification : Implementations don\u2019t handle all cases consistently. For example, bucketing in Hive and Spark use different hash functions and are not compatible. Hive uses a locking scheme to make cross-partition changes safe, but no other implementations use it. Table data is tracked in two stores : a central metastore, for partitions, and the file system, for files. This makes atomic changes to a table\u2019s contents impossible. It also requires job planning to make listing calls that are O(n) with the number of partitions. These listing calls are expensive, especially when using a remote object store. Operations depend on file rename : Most output committers depend on rename operations to implement guarantees and reduce the amount of time tables only have partial data from a write. But rename is not a metadata-only operation in S3 and will copy data. The new S3 committers that use multipart upload make this better, but can\u2019t entirely solve the problem and put a lot of load on the file system during job commit. The current format\u2019s dependence on listing and rename cannot be changed, so a new format is needed.","title":"Why a new table format?"},{"location":"why-iceberg/#other-design-goals","text":"In addition to changes in how table contents are tracked, Iceberg\u2019s design improves a few other areas: Reliable types : Iceberg provides a core set of types, tested to work consistently across all of the supported data formats. Types include date, timestamp, and decimal, as well as nested combinations of map, list, and struct. Schema evolution : Columns are tracked by ID to fully support add, drop, and rename across all columns and nested fields. Hidden partitioning : Partitioning is built into Iceberg as table configuration; it can plan efficient queries without extra partition predicates. Stats-based split filtering : Stats for the columns in each data file are used to eliminate splits before creating tasks, boosting performance for highly selective queries. Metrics : The format includes cost-based optimization metrics stored with data files for better job planning. Unmodified partition data : The Hive layout stores partition data escaped in strings. Iceberg stores partition data without modification. Portable spec : Tables are not tied to Java. Iceberg has a clear specification for other implementations.","title":"Other design goals"}]}