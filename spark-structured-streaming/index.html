<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    <link rel="canonical" href="https://iceberg.apache.org/spark-structured-streaming/">
    <link rel="shortcut icon" href="../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Structured Streaming - Apache Iceberg</title>
    <link href="../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../css/highlight.css">
    <link href="../css/extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../js/jquery-3.2.1.min.js"></script>
    <script src="../js/bootstrap-3.3.7.min.js"></script>
    <script src="../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Spark Structured Streaming", url: "#_top", children: [
              {title: "Streaming Writes", url: "#streaming-writes" },
              {title: "Maintenance for streaming tables", url: "#maintenance-for-streaming-tables" },
          ]},
        ];

    </script>
    <script src="../js/base.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    <!--
 - Licensed to the Apache Software Foundation (ASF) under one or more
 - contributor license agreements.  See the NOTICE file distributed with
 - this work for additional information regarding copyright ownership.
 - The ASF licenses this file to You under the Apache License, Version 2.0
 - (the "License"); you may not use this file except in compliance with
 - the License.  You may obtain a copy of the License at
 -
 -   http://www.apache.org/licenses/LICENSE-2.0
 -
 - Unless required by applicable law or agreed to in writing, software
 - distributed under the License is distributed on an "AS IS" BASIS,
 - WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 - See the License for the specific language governing permissions and
 - limitations under the License.
 -->

<h1 id="spark-structured-streaming">Spark Structured Streaming<a class="headerlink" href="#spark-structured-streaming" title="Permanent link">&para;</a></h1>
<p>Iceberg uses Apache Spark&rsquo;s DataSourceV2 API for data source and catalog implementations. Spark DSv2 is an evolving API
with different levels of support in Spark versions.</p>
<p>As of Spark 3.0, DataFrame reads and writes are supported.</p>
<table>
<thead>
<tr>
<th>Feature support</th>
<th>Spark 3.0</th>
<th>Spark 2.4</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="#writing-with-streaming-query">DataFrame write</a></td>
<td>✔</td>
<td>✔</td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="streaming-writes">Streaming Writes<a class="headerlink" href="#streaming-writes" title="Permanent link">&para;</a></h2>
<p>To write values from streaming query to Iceberg table, use <code>DataStreamWriter</code>:</p>
<pre><code class="language-scala">val tableIdentifier: String = ...
data.writeStream
    .format(&quot;iceberg&quot;)
    .outputMode(&quot;append&quot;)
    .trigger(Trigger.ProcessingTime(1, TimeUnit.MINUTES))
    .option(&quot;path&quot;, tableIdentifier)
    .option(&quot;checkpointLocation&quot;, checkpointPath)
    .start()
</code></pre>
<p>The <code>tableIdentifier</code> can be:</p>
<ul>
<li>The fully-qualified path to a HDFS table, like <code>hdfs://nn:8020/path/to/table</code></li>
<li>A table name if the table is tracked by a catalog, like <code>database.table_name</code></li>
</ul>
<p>Iceberg doesn&rsquo;t support &ldquo;continuous processing&rdquo;, as it doesn&rsquo;t provide the interface to &ldquo;commit&rdquo; the output.</p>
<p>Iceberg supports <code>append</code> and <code>complete</code> output modes:</p>
<ul>
<li><code>append</code>: appends the rows of every micro-batch to the table</li>
<li><code>complete</code>: replaces the table contents every micro-batch</li>
</ul>
<p>The table should be created in prior to start the streaming query. Refer <a href="/spark-ddl/#create-table">SQL create table</a>
on Spark page to see how to create the Iceberg table.</p>
<h3 id="writing-against-partitioned-table">Writing against partitioned table<a class="headerlink" href="#writing-against-partitioned-table" title="Permanent link">&para;</a></h3>
<p>Iceberg requires the data to be sorted according to the partition spec per task (Spark partition) in prior to write
against partitioned table. For batch queries you&rsquo;re encouraged to do explicit sort to fulfill the requirement
(see <a href="/spark-writes/#writing-to-partitioned-tables">here</a>), but the approach would bring additional latency as
repartition and sort are considered as heavy operations for streaming workload. To avoid additional latency, you can
enable fanout writer to eliminate the requirement.</p>
<pre><code class="language-scala">val tableIdentifier: String = ...
data.writeStream
    .format(&quot;iceberg&quot;)
    .outputMode(&quot;append&quot;)
    .trigger(Trigger.ProcessingTime(1, TimeUnit.MINUTES))
    .option(&quot;path&quot;, tableIdentifier)
    .option(&quot;fanout-enabled&quot;, &quot;true&quot;)
    .option(&quot;checkpointLocation&quot;, checkpointPath)
    .start()
</code></pre>
<p>Fanout writer opens the files per partition value and doesn&rsquo;t close these files till write task is finished.
This functionality is discouraged for batch query, as explicit sort against output rows isn&rsquo;t expensive for batch workload.</p>
<h2 id="maintenance-for-streaming-tables">Maintenance for streaming tables<a class="headerlink" href="#maintenance-for-streaming-tables" title="Permanent link">&para;</a></h2>
<p>Streaming queries can create new table versions quickly, which creates lots of table metadata to track those versions.
Maintaining metadata by tuning the rate of commits, expiring old snapshots, and automatically cleaning up metadata files
is highly recommended.</p>
<h3 id="tune-the-rate-of-commits">Tune the rate of commits<a class="headerlink" href="#tune-the-rate-of-commits" title="Permanent link">&para;</a></h3>
<p>Having high rate of commits would produce lots of data files, manifests, and snapshots which leads the table hard
to maintain. We encourage having trigger interval 1 minute at minimum, and increase the interval if needed.</p>
<p>The triggers section in <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers">Structured Streaming Programming Guide</a>
documents how to configure the interval.</p>
<h3 id="expire-old-snapshots">Expire old snapshots<a class="headerlink" href="#expire-old-snapshots" title="Permanent link">&para;</a></h3>
<p>Each micro-batch written to a table produces a new snapshot, which are tracked in table metadata until they are expired to remove the metadata and any data files that are no longer needed. Snapshots accumulate quickly with frequent commits, so it is highly recommended that tables written by streaming queries are <a href="../maintenance/#expire-snapshots">regularly maintained</a>.</p>
<h3 id="compacting-data-files">Compacting data files<a class="headerlink" href="#compacting-data-files" title="Permanent link">&para;</a></h3>
<p>The amount of data written in a micro batch is typically small, which can cause the table metadata to track lots of small files. <a href="../maintenance/#compact-data-files">Compacting small files into larger files</a> reduces the metadata needed by the table, and increases query efficiency.</p>
<h3 id="rewrite-manifests">Rewrite manifests<a class="headerlink" href="#rewrite-manifests" title="Permanent link">&para;</a></h3>
<p>To optimize write latency on streaming workload, Iceberg may write the new snapshot with a &ldquo;fast&rdquo; append that does not automatically compact manifests.
This could lead lots of small manifest files. Manifests can be <a href="../maintenance/#rewrite-manifests">rewritten to optimize queries and to compact</a>.</p>

  <br>
</div>

<footer class="container-fluid wm-page-content"><p>Copyright 2018-2021 <a href='https://www.apache.org/'>The Apache Software Foundation</a><br />Apache Iceberg, Iceberg, Apache, the Apache feather logo, and the Apache Iceberg project logo are either registered<br />trademarks or trademarks of The Apache Software Foundation in the United States and other countries.</p>
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>