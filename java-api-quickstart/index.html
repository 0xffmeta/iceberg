<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    <link rel="canonical" href="https://iceberg.apache.org/java-api-quickstart/">
    <link rel="shortcut icon" href="../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Java Quickstart - Apache Iceberg</title>
    <link href="../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../css/highlight.css">
    <link href="../css/extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../js/jquery-3.2.1.min.js"></script>
    <script src="../js/bootstrap-3.3.7.min.js"></script>
    <script src="../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Java API Quickstart", url: "#_top", children: [
              {title: "Create a table", url: "#create-a-table" },
              {title: "Schemas", url: "#schemas" },
              {title: "Partitioning", url: "#partitioning" },
          ]},
        ];

    </script>
    <script src="../js/base.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    <!--
 - Licensed to the Apache Software Foundation (ASF) under one or more
 - contributor license agreements.  See the NOTICE file distributed with
 - this work for additional information regarding copyright ownership.
 - The ASF licenses this file to You under the Apache License, Version 2.0
 - (the "License"); you may not use this file except in compliance with
 - the License.  You may obtain a copy of the License at
 -
 -   http://www.apache.org/licenses/LICENSE-2.0
 -
 - Unless required by applicable law or agreed to in writing, software
 - distributed under the License is distributed on an "AS IS" BASIS,
 - WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 - See the License for the specific language governing permissions and
 - limitations under the License.
 -->

<h1 id="java-api-quickstart">Java API Quickstart<a class="headerlink" href="#java-api-quickstart" title="Permanent link">&para;</a></h1>
<h2 id="create-a-table">Create a table<a class="headerlink" href="#create-a-table" title="Permanent link">&para;</a></h2>
<p>Tables are created using either a <a href="../javadoc/master/?org/apache/iceberg/catalog/Catalog.html"><code>Catalog</code></a> or an implementation of the <a href="../javadoc/master/?org/apache/iceberg/Tables.html"><code>Tables</code></a> interface.</p>
<h3 id="using-a-hive-catalog">Using a Hive catalog<a class="headerlink" href="#using-a-hive-catalog" title="Permanent link">&para;</a></h3>
<p>The Hive catalog connects to a Hive metastore to keep track of Iceberg tables.
You can initialize a Hive catalog with a name and some properties.
(see: <a href="https://iceberg.apache.org/configuration/#catalog-properties">Catalog properties</a>)</p>
<p><strong>Note:</strong> Currently, <code>setConf</code> is always required for hive catalogs, but this will change in the future.</p>
<pre><code class="language-java">import org.apache.iceberg.hive.HiveCatalog;

Catalog catalog = new HiveCatalog();
catalog.setConf(spark.sparkContext().hadoopConfiguration());  // Configure using Spark's Hadoop configuration

Map &lt;String, String&gt; properties = new HashMap&lt;String, String&gt;();
properties.put(&quot;warehouse&quot;, &quot;...&quot;);
properties.put(&quot;uri&quot;, &quot;...&quot;);

catalog.initialize(&quot;hive&quot;, properties);
</code></pre>
<p>The <code>Catalog</code> interface defines methods for working with tables, like <code>createTable</code>, <code>loadTable</code>, <code>renameTable</code>, and <code>dropTable</code>.</p>
<p>To create a table, pass an <code>Identifier</code> and a <code>Schema</code> along with other initial metadata:</p>
<pre><code class="language-java">import org.apache.iceberg.Table;
import org.apache.iceberg.catalog.TableIdentifier;

TableIdentifier name = TableIdentifier.of(&quot;logging&quot;, &quot;logs&quot;);
Table table = catalog.createTable(name, schema, spec);

// or to load an existing table, use the following line
// Table table = catalog.loadTable(name);
</code></pre>
<p>The logs <a href="#create-a-schema">schema</a> and <a href="#create-a-partition-spec">partition spec</a> are created below.</p>
<h3 id="using-a-hadoop-catalog">Using a Hadoop catalog<a class="headerlink" href="#using-a-hadoop-catalog" title="Permanent link">&para;</a></h3>
<p>A Hadoop catalog doesn&rsquo;t need to connect to a Hive MetaStore, but can only be used with HDFS or similar file systems that support atomic rename. Concurrent writes with a Hadoop catalog are not safe with a local FS or S3. To create a Hadoop catalog:</p>
<pre><code class="language-java">import org.apache.hadoop.conf.Configuration;
import org.apache.iceberg.hadoop.HadoopCatalog;

Configuration conf = new Configuration();
String warehousePath = &quot;hdfs://host:8020/warehouse_path&quot;;
HadoopCatalog catalog = new HadoopCatalog(conf, warehousePath);
</code></pre>
<p>Like the Hive catalog, <code>HadoopCatalog</code> implements <code>Catalog</code>, so it also has methods for working with tables, like <code>createTable</code>, <code>loadTable</code>, and <code>dropTable</code>.</p>
<p>This example creates a table with Hadoop catalog:</p>
<pre><code class="language-java">import org.apache.iceberg.Table;
import org.apache.iceberg.catalog.TableIdentifier;

TableIdentifier name = TableIdentifier.of(&quot;logging&quot;, &quot;logs&quot;);
Table table = catalog.createTable(name, schema, spec);

// or to load an existing table, use the following line
// Table table = catalog.loadTable(name);
</code></pre>
<p>The logs <a href="#create-a-schema">schema</a> and <a href="#create-a-partition-spec">partition spec</a> are created below.</p>
<h3 id="using-hadoop-tables">Using Hadoop tables<a class="headerlink" href="#using-hadoop-tables" title="Permanent link">&para;</a></h3>
<p>Iceberg also supports tables that are stored in a directory in HDFS. Concurrent writes with a Hadoop tables are not safe when stored in the local FS or S3. Directory tables don&rsquo;t support all catalog operations, like rename, so they use the <code>Tables</code> interface instead of <code>Catalog</code>.</p>
<p>To create a table in HDFS, use <code>HadoopTables</code>:</p>
<pre><code class="language-java">import org.apache.hadoop.conf.Configuration;
import org.apache.iceberg.hadoop.HadoopTables;
import org.apache.iceberg.Table;

Configuration conf = new Configuration();
HadoopTables tables = new HadoopTables(conf);
Table table = tables.create(schema, spec, table_location);

// or to load an existing table, use the following line
// Table table = tables.load(table_location);
</code></pre>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Hadoop tables shouldn&rsquo;t be used with file systems that do not support atomic rename. Iceberg relies on rename to synchronize concurrent commits for directory tables.</p>
</div>
<h3 id="tables-in-spark">Tables in Spark<a class="headerlink" href="#tables-in-spark" title="Permanent link">&para;</a></h3>
<p>Spark uses both <code>HiveCatalog</code> and <code>HadoopTables</code> to load tables. Hive is used when the identifier passed to <code>load</code> or <code>save</code> is not a path, otherwise Spark assumes it is a path-based table.</p>
<p>To read and write to tables from Spark see:</p>
<ul>
<li><a href="../spark-queries/#querying-with-sql">SQL queries in Spark</a></li>
<li><a href="../spark-writes/#insert-into"><code>INSERT INTO</code> in Spark</a></li>
<li><a href="../spark-writes/#merge-into"><code>MERGE INTO</code> in Spark</a></li>
</ul>
<h2 id="schemas">Schemas<a class="headerlink" href="#schemas" title="Permanent link">&para;</a></h2>
<h3 id="create-a-schema">Create a schema<a class="headerlink" href="#create-a-schema" title="Permanent link">&para;</a></h3>
<p>This example creates a schema for a <code>logs</code> table:</p>
<pre><code class="language-java">import org.apache.iceberg.Schema;
import org.apache.iceberg.types.Types;

Schema schema = new Schema(
      Types.NestedField.required(1, &quot;level&quot;, Types.StringType.get()),
      Types.NestedField.required(2, &quot;event_time&quot;, Types.TimestampType.withZone()),
      Types.NestedField.required(3, &quot;message&quot;, Types.StringType.get()),
      Types.NestedField.optional(4, &quot;call_stack&quot;, Types.ListType.ofRequired(5, Types.StringType.get()))
    );
</code></pre>
<p>When using the Iceberg API directly, type IDs are required. Conversions from other schema formats, like Spark, Avro, and Parquet will automatically assign new IDs.</p>
<p>When a table is created, all IDs in the schema are re-assigned to ensure uniqueness.</p>
<h3 id="convert-a-schema-from-avro">Convert a schema from Avro<a class="headerlink" href="#convert-a-schema-from-avro" title="Permanent link">&para;</a></h3>
<p>To create an Iceberg schema from an existing Avro schema, use converters in <code>AvroSchemaUtil</code>:</p>
<pre><code class="language-java">import org.apache.avro.Schema;
import org.apache.avro.Schema.Parser;
import org.apache.iceberg.avro.AvroSchemaUtil;

Schema avroSchema = new Parser().parse(&quot;{\&quot;type\&quot;: \&quot;record\&quot; , ... }&quot;);
Schema icebergSchema = AvroSchemaUtil.toIceberg(avroSchema);
</code></pre>
<h3 id="convert-a-schema-from-spark">Convert a schema from Spark<a class="headerlink" href="#convert-a-schema-from-spark" title="Permanent link">&para;</a></h3>
<p>To create an Iceberg schema from an existing table, use converters in <code>SparkSchemaUtil</code>:</p>
<pre><code class="language-java">import org.apache.iceberg.spark.SparkSchemaUtil;

Schema schema = SparkSchemaUtil.schemaForTable(sparkSession, table_name);
</code></pre>
<h2 id="partitioning">Partitioning<a class="headerlink" href="#partitioning" title="Permanent link">&para;</a></h2>
<h3 id="create-a-partition-spec">Create a partition spec<a class="headerlink" href="#create-a-partition-spec" title="Permanent link">&para;</a></h3>
<p>Partition specs describe how Iceberg should group records into data files. Partition specs are created for a table&rsquo;s schema using a builder.</p>
<p>This example creates a partition spec for the <code>logs</code> table that partitions records by the hour of the log event&rsquo;s timestamp and by log level:</p>
<pre><code class="language-java">import org.apache.iceberg.PartitionSpec;

PartitionSpec spec = PartitionSpec.builderFor(schema)
      .hour(&quot;event_time&quot;)
      .identity(&quot;level&quot;)
      .build();
</code></pre>
<p>For more information on the different partition transforms that Iceberg offers, visit <a href="../spec/#partitioning">this page</a>.</p>

  <br>
</div>

<footer class="container-fluid wm-page-content"><p>Copyright 2018-2021 <a href='https://www.apache.org/'>The Apache Software Foundation</a><br />Apache Iceberg, Iceberg, Apache, the Apache feather logo, and the Apache Iceberg project logo are either registered<br />trademarks or trademarks of The Apache Software Foundation in the United States and other countries.</p>
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>