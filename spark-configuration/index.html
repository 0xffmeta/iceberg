<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    <link rel="canonical" href="https://iceberg.apache.org/spark-configuration/">
    <link rel="shortcut icon" href="../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Configuration - Apache Iceberg</title>
    <link href="../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../css/highlight.css">
    <link href="../css/extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../js/jquery-3.2.1.min.js"></script>
    <script src="../js/bootstrap-3.3.7.min.js"></script>
    <script src="../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Spark Configuration", url: "#_top", children: [
              {title: "Catalogs", url: "#catalogs" },
              {title: "SQL Extensions", url: "#sql-extensions" },
              {title: "Runtime configuration", url: "#runtime-configuration" },
          ]},
        ];

    </script>
    <script src="../js/base.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    <!--
 - Licensed to the Apache Software Foundation (ASF) under one or more
 - contributor license agreements.  See the NOTICE file distributed with
 - this work for additional information regarding copyright ownership.
 - The ASF licenses this file to You under the Apache License, Version 2.0
 - (the "License"); you may not use this file except in compliance with
 - the License.  You may obtain a copy of the License at
 -
 -   http://www.apache.org/licenses/LICENSE-2.0
 -
 - Unless required by applicable law or agreed to in writing, software
 - distributed under the License is distributed on an "AS IS" BASIS,
 - WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 - See the License for the specific language governing permissions and
 - limitations under the License.
 -->

<h1 id="spark-configuration">Spark Configuration<a class="headerlink" href="#spark-configuration" title="Permanent link">&para;</a></h1>
<h2 id="catalogs">Catalogs<a class="headerlink" href="#catalogs" title="Permanent link">&para;</a></h2>
<p>Spark 3.0 adds an API to plug in table catalogs that are used to load, create, and manage Iceberg tables. Spark catalogs are configured by setting Spark properties under <code>spark.sql.catalog</code>.</p>
<p>This creates an Iceberg catalog named <code>hive_prod</code> that loads tables from a Hive metastore:</p>
<pre><code class="language-plain">spark.sql.catalog.hive_prod = org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.hive_prod.type = hive
spark.sql.catalog.hive_prod.uri = thrift://metastore-host:port
# omit uri to use the same URI as Spark: hive.metastore.uris in hive-site.xml
</code></pre>
<p>Iceberg also supports a directory-based catalog in HDFS that can be configured using <code>type=hadoop</code>:</p>
<pre><code class="language-plain">spark.sql.catalog.hadoop_prod = org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.hadoop_prod.type = hadoop
spark.sql.catalog.hadoop_prod.warehouse = hdfs://nn:8020/warehouse/path
</code></pre>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Hive-based catalog only loads Iceberg tables. To load non-Iceberg tables in the same Hive metastore, use a <a href="#replacing-the-session-catalog">session catalog</a>.</p>
</div>
<h3 id="catalog-configuration">Catalog configuration<a class="headerlink" href="#catalog-configuration" title="Permanent link">&para;</a></h3>
<p>A catalog is created and named by adding a property <code>spark.sql.catalog.(catalog-name)</code> with an implementation class for its value.</p>
<p>Iceberg supplies two implementations:</p>
<ul>
<li><code>org.apache.iceberg.spark.SparkCatalog</code> supports a Hive Metastore or a Hadoop warehouse as a catalog</li>
<li><code>org.apache.iceberg.spark.SparkSessionCatalog</code> adds support for Iceberg tables to Spark&rsquo;s built-in catalog, and delegates to the built-in catalog for non-Iceberg tables</li>
</ul>
<p>Both catalogs are configured using properties nested under the catalog name. Common configuration properties for Hive and Hadoop are:</p>
<table>
<thead>
<tr>
<th>Property</th>
<th>Values</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.sql.catalog.<em>catalog-name</em>.type</td>
<td><code>hive</code> or <code>hadoop</code></td>
<td>The underlying Iceberg catalog implementation, <code>HiveCatalog</code>, <code>HadoopCatalog</code> or left unset if using a custom catalog</td>
</tr>
<tr>
<td>spark.sql.catalog.<em>catalog-name</em>.catalog-impl</td>
<td></td>
<td>The underlying Iceberg catalog implementation.</td>
</tr>
<tr>
<td>spark.sql.catalog.<em>catalog-name</em>.default-namespace</td>
<td>default</td>
<td>The default current namespace for the catalog</td>
</tr>
<tr>
<td>spark.sql.catalog.<em>catalog-name</em>.uri</td>
<td>thrift://host:port</td>
<td>Metastore connect URI; default from <code>hive-site.xml</code></td>
</tr>
<tr>
<td>spark.sql.catalog.<em>catalog-name</em>.warehouse</td>
<td>hdfs://nn:8020/warehouse/path</td>
<td>Base path for the warehouse directory</td>
</tr>
<tr>
<td>spark.sql.catalog.<em>catalog-name</em>.cache-enabled</td>
<td><code>true</code> or <code>false</code></td>
<td>Whether to enable catalog cache, default value is <code>true</code></td>
</tr>
</tbody>
</table>
<p>Additional properties can be found in common <a href="../configuration/#catalog-properties">catalog configuration</a>.</p>
<h3 id="using-catalogs">Using catalogs<a class="headerlink" href="#using-catalogs" title="Permanent link">&para;</a></h3>
<p>Catalog names are used in SQL queries to identify a table. In the examples above, <code>hive_prod</code> and <code>hadoop_prod</code> can be used to prefix database and table names that will be loaded from those catalogs.</p>
<pre><code class="language-sql">SELECT * FROM hive_prod.db.table -- load db.table from catalog hive_prod
</code></pre>
<p>Spark 3 keeps track of the current catalog and namespace, which can be omitted from table names.</p>
<pre><code class="language-sql">USE hive_prod.db;
SELECT * FROM table -- load db.table from catalog hive_prod
</code></pre>
<p>To see the current catalog and namespace, run <code>SHOW CURRENT NAMESPACE</code>.</p>
<h3 id="replacing-the-session-catalog">Replacing the session catalog<a class="headerlink" href="#replacing-the-session-catalog" title="Permanent link">&para;</a></h3>
<p>To add Iceberg table support to Spark&rsquo;s built-in catalog, configure <code>spark_catalog</code> to use Iceberg&rsquo;s <code>SparkSessionCatalog</code>.</p>
<pre><code class="language-plain">spark.sql.catalog.spark_catalog = org.apache.iceberg.spark.SparkSessionCatalog
spark.sql.catalog.spark_catalog.type = hive
</code></pre>
<p>Spark&rsquo;s built-in catalog supports existing v1 and v2 tables tracked in a Hive Metastore. This configures Spark to use Iceberg&rsquo;s <code>SparkSessionCatalog</code> as a wrapper around that session catalog. When a table is not an Iceberg table, the built-in catalog will be used to load it instead.</p>
<p>This configuration can use same Hive Metastore for both Iceberg and non-Iceberg tables.</p>
<h3 id="using-catalog-specific-hadoop-configuration-values">Using catalog specific Hadoop configuration values<a class="headerlink" href="#using-catalog-specific-hadoop-configuration-values" title="Permanent link">&para;</a></h3>
<p>Similar to configuring Hadoop properties by using <code>spark.hadoop.*</code>, it&rsquo;s possible to set per-catalog Hadoop configuration values when using Spark by adding the property for the catalog with the prefix <code>spark.sql.catalog.(catalog-name).hadoop.*</code>. These properties will take precedence over values configured globally using <code>spark.hadoop.*</code> and will only affect Iceberg tables.</p>
<pre><code class="language-plain">spark.sql.catalog.hadoop_prod.hadoop.fs.s3a.endpoint = http://aws-local:9000
</code></pre>
<h3 id="loading-a-custom-catalog">Loading a custom catalog<a class="headerlink" href="#loading-a-custom-catalog" title="Permanent link">&para;</a></h3>
<p>Spark supports loading a custom Iceberg <code>Catalog</code> implementation by specifying the <code>catalog-impl</code> property. Here is an example:</p>
<pre><code class="language-plain">spark.sql.catalog.custom_prod = org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.custom_prod.catalog-impl = com.my.custom.CatalogImpl
spark.sql.catalog.custom_prod.my-additional-catalog-config = my-value
</code></pre>
<h3 id="catalogs-in-spark-24">Catalogs in Spark 2.4<a class="headerlink" href="#catalogs-in-spark-24" title="Permanent link">&para;</a></h3>
<p>When using Iceberg 0.11.0 and later, Spark 2.4 can load tables from multiple Iceberg catalogs or from table locations.</p>
<p>Catalogs in 2.4 are configured just like catalogs in 3.0, but only Iceberg catalogs are supported.</p>
<h2 id="sql-extensions">SQL Extensions<a class="headerlink" href="#sql-extensions" title="Permanent link">&para;</a></h2>
<p>Iceberg 0.11.0 and later add an extension module to Spark to add new SQL commands, like <code>CALL</code> for stored procedures or <code>ALTER TABLE ... WRITE ORDERED BY</code>.</p>
<p>Using those SQL commands requires adding Iceberg extensions to your Spark environment using the following Spark property:</p>
<table>
<thead>
<tr>
<th>Spark extensions property</th>
<th>Iceberg extensions implementation</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>spark.sql.extensions</code></td>
<td><code>org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions</code></td>
</tr>
</tbody>
</table>
<p>SQL extensions are not available for Spark 2.4.</p>
<h2 id="runtime-configuration">Runtime configuration<a class="headerlink" href="#runtime-configuration" title="Permanent link">&para;</a></h2>
<h3 id="read-options">Read options<a class="headerlink" href="#read-options" title="Permanent link">&para;</a></h3>
<p>Spark read options are passed when configuring the DataFrameReader, like this:</p>
<pre><code class="language-scala">// time travel
spark.read
    .option(&quot;snapshot-id&quot;, 10963874102873L)
    .table(&quot;catalog.db.table&quot;)
</code></pre>
<table>
<thead>
<tr>
<th>Spark option</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>snapshot-id</td>
<td>(latest)</td>
<td>Snapshot ID of the table snapshot to read</td>
</tr>
<tr>
<td>as-of-timestamp</td>
<td>(latest)</td>
<td>A timestamp in milliseconds; the snapshot used will be the snapshot current at this time.</td>
</tr>
<tr>
<td>split-size</td>
<td>As per table property</td>
<td>Overrides this table&rsquo;s read.split.target-size and read.split.metadata-target-size</td>
</tr>
<tr>
<td>lookback</td>
<td>As per table property</td>
<td>Overrides this table&rsquo;s read.split.planning-lookback</td>
</tr>
<tr>
<td>file-open-cost</td>
<td>As per table property</td>
<td>Overrides this table&rsquo;s read.split.open-file-cost</td>
</tr>
<tr>
<td>vectorization-enabled</td>
<td>As per table property</td>
<td>Overrides this table&rsquo;s read.parquet.vectorization.enabled</td>
</tr>
<tr>
<td>batch-size</td>
<td>As per table property</td>
<td>Overrides this table&rsquo;s read.parquet.vectorization.batch-size</td>
</tr>
</tbody>
</table>
<h3 id="write-options">Write options<a class="headerlink" href="#write-options" title="Permanent link">&para;</a></h3>
<p>Spark write options are passed when configuring the DataFrameWriter, like this:</p>
<pre><code class="language-scala">// write with Avro instead of Parquet
df.write
    .option(&quot;write-format&quot;, &quot;avro&quot;)
    .option(&quot;snapshot-property.key&quot;, &quot;value&quot;)
    .insertInto(&quot;catalog.db.table&quot;)
</code></pre>
<table>
<thead>
<tr>
<th>Spark option</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>write-format</td>
<td>Table write.format.default</td>
<td>File format to use for this write operation; parquet, avro, or orc</td>
</tr>
<tr>
<td>target-file-size-bytes</td>
<td>As per table property</td>
<td>Overrides this table&rsquo;s write.target-file-size-bytes</td>
</tr>
<tr>
<td>check-nullability</td>
<td>true</td>
<td>Sets the nullable check on fields</td>
</tr>
<tr>
<td>snapshot-property.<em>custom-key</em></td>
<td>null</td>
<td>Adds an entry with custom-key and corresponding value in the snapshot summary</td>
</tr>
<tr>
<td>fanout-enabled</td>
<td>false</td>
<td>Overrides this table&rsquo;s write.spark.fanout.enabled</td>
</tr>
<tr>
<td>check-ordering</td>
<td>true</td>
<td>Checks if input schema and table schema are same</td>
</tr>
</tbody>
</table>

  <br>
</div>

<footer class="container-fluid wm-page-content"><p>Copyright 2018-2021 <a href='https://www.apache.org/'>The Apache Software Foundation</a><br />Apache Iceberg, Iceberg, Apache, the Apache feather logo, and the Apache Iceberg project logo are either registered<br />trademarks or trademarks of The Apache Software Foundation in the United States and other countries.</p>
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>